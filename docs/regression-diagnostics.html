<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Regression diagnostics | notes</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Regression diagnostics | notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Regression diagnostics | notes" />
  
  
  

<meta name="author" content="Dr Pete Philipson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="inference-for-the-multiple-linear-regression-model.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>MAS3928: Statistical Modelling</a></li>
<li class="chapter" data-level="" data-path="module-information.html"><a href="module-information.html"><i class="fa fa-check"></i>Module information</a>
<ul>
<li class="chapter" data-level="" data-path="module-information.html"><a href="module-information.html#lecturer-information"><i class="fa fa-check"></i>Lecturer information</a></li>
<li class="chapter" data-level="" data-path="module-information.html"><a href="module-information.html#module-schedule"><i class="fa fa-check"></i>Module schedule</a></li>
<li class="chapter" data-level="" data-path="module-information.html"><a href="module-information.html#course-materials"><i class="fa fa-check"></i>Course materials</a></li>
<li class="chapter" data-level="" data-path="module-information.html"><a href="module-information.html#assessment"><i class="fa fa-check"></i>Assessment</a></li>
<li class="chapter" data-level="" data-path="module-information.html"><a href="module-information.html#relevant-texts"><i class="fa fa-check"></i>Relevant texts</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#multiple-linear-regression"><i class="fa fa-check"></i><b>1.1</b> Multiple linear regression</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#matrix-form-of-the-model"><i class="fa fa-check"></i><b>1.2</b> Matrix form of the model</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#example---matrix-form-for-pre-diabetes-data"><i class="fa fa-check"></i>Example - Matrix form for pre-diabetes data</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#parameter-estimation"><i class="fa fa-check"></i><b>1.3</b> Parameter estimation</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#estimation-of-underlinebeta"><i class="fa fa-check"></i><b>1.3.1</b> Estimation of <span class="math inline">\(\underline{\beta}\)</span></a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#estimation-of-sigma_epsilon2"><i class="fa fa-check"></i><b>1.3.2</b> Estimation of <span class="math inline">\(\sigma_{\epsilon}^2\)</span></a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#sec:resfithat"><i class="fa fa-check"></i><b>1.3.3</b> Residuals, fitted values and the ‘hat matrix’</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#properties-of-the-hat-matrix"><i class="fa fa-check"></i><b>1.3.4</b> Properties of the hat matrix</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#example-multiple-linear-regresion-analysis-of-bodyweight-data"><i class="fa fa-check"></i>Example: Multiple linear regresion analysis of bodyweight data</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#expectations-variances-and-inference"><i class="fa fa-check"></i><b>1.4</b> Expectations, variances and inference</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#expectation-of-underlinehatbeta"><i class="fa fa-check"></i><b>1.4.1</b> Expectation of <span class="math inline">\(\underline{\hat{\beta}}\)</span></a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction.html"><a href="introduction.html#variance-of-underlinehatbeta"><i class="fa fa-check"></i><b>1.4.2</b> Variance of <span class="math inline">\(\underline{\hat{\beta}}\)</span></a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction.html"><a href="introduction.html#sec:inferforbetahat"><i class="fa fa-check"></i><b>1.4.3</b> Inference for <span class="math inline">\(\underline{\hat{\beta}}\)</span></a></li>
<li class="chapter" data-level="1.4.4" data-path="introduction.html"><a href="introduction.html#sec:expvaryhat"><i class="fa fa-check"></i><b>1.4.4</b> Expectation and variance of the fitted values</a></li>
<li class="chapter" data-level="1.4.5" data-path="introduction.html"><a href="introduction.html#expectation-and-variance-of-the-residuals"><i class="fa fa-check"></i><b>1.4.5</b> Expectation and variance of the residuals</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#sec:mlrinr"><i class="fa fa-check"></i><b>1.5</b> Multiple linear regression in <code>R</code></a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#using-data-in-r"><i class="fa fa-check"></i><b>1.5.1</b> Using data in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#example-analysis-of-bodyweight-data-using-r"><i class="fa fa-check"></i>Example: Analysis of bodyweight data using <code>R</code></a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#the-role-of-the-intercept"><i class="fa fa-check"></i><b>1.6</b> The role of the intercept</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#example-analysis-of-bodyweight-data-without-an-intercept-term"><i class="fa fa-check"></i>Example: Analysis of bodyweight data without an intercept term</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#example-analysis-of-mens-premier-league-football-data---the-role-of-the-intercept"><i class="fa fa-check"></i>Example: Analysis of men’s Premier League football data - the role of the intercept</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="introduction.html"><a href="introduction.html#interpretability-of-the-intercept-and-extrapolation"><i class="fa fa-check"></i><b>1.6.1</b> Interpretability of the intercept and extrapolation</a></li>
<li class="chapter" data-level="1.6.2" data-path="introduction.html"><a href="introduction.html#mean-centering-of-covariates"><i class="fa fa-check"></i><b>1.6.2</b> Mean-centering of covariates</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#example-mean-centering-mens-premier-league-football-data"><i class="fa fa-check"></i>Example: Mean-centering (men’s Premier League football data)</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#sec:multicol"><i class="fa fa-check"></i><b>1.7</b> Properties of <span class="math inline">\((\mathrm{X}^T\mathrm{X})^{-1}\)</span>: multicollinearity</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#example-multicollinearity-in-mens-premier-league-football-data"><i class="fa fa-check"></i>Example: Multicollinearity in men’s Premier League football data</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html"><i class="fa fa-check"></i><b>2</b> Regression diagnostics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#standardised-residuals"><i class="fa fa-check"></i><b>2.1</b> Standardised residuals</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#residual-plots"><i class="fa fa-check"></i><b>2.1.1</b> Residual plots</a></li>
<li class="chapter" data-level="" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#correlation-between-residuals-and-fitted-values"><i class="fa fa-check"></i>Correlation between residuals and fitted values</a></li>
<li class="chapter" data-level="2.1.2" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#outliers"><i class="fa fa-check"></i><b>2.1.2</b> Outliers</a></li>
<li class="chapter" data-level="" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#example-residual-analysis-for-pre-diabetes-data"><i class="fa fa-check"></i>Example: Residual analysis for pre-diabetes data</a></li>
<li class="chapter" data-level="2.1.3" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#normality-of-the-residuals"><i class="fa fa-check"></i><b>2.1.3</b> Normality of the residuals</a></li>
<li class="chapter" data-level="2.1.4" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#anderson-darling-test"><i class="fa fa-check"></i><b>2.1.4</b> Anderson-Darling test</a></li>
<li class="chapter" data-level="" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#a-cautionary-note"><i class="fa fa-check"></i>A cautionary note</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#regression-diagnostics-1"><i class="fa fa-check"></i><b>2.2</b> Regression diagnostics</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#leverage-values"><i class="fa fa-check"></i><b>2.2.1</b> Leverage values</a></li>
<li class="chapter" data-level="" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#example-leverage-values-for-the-pre-diabetes-data"><i class="fa fa-check"></i>Example: Leverage values for the pre-diabetes data</a></li>
<li class="chapter" data-level="2.2.2" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#influential-observations"><i class="fa fa-check"></i><b>2.2.2</b> Influential observations</a></li>
<li class="chapter" data-level="2.2.3" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#dealing-with-unusual-observations"><i class="fa fa-check"></i><b>2.2.3</b> Dealing with unusual observations</a></li>
<li class="chapter" data-level="" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html#example-model-checking-for-the-premier-league-data"><i class="fa fa-check"></i>Example: Model checking for the Premier League data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html"><i class="fa fa-check"></i><b>3</b> Inference for the multiple linear regression model</a>
<ul>
<li class="chapter" data-level="3.1" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#assessing-the-fit"><i class="fa fa-check"></i><b>3.1</b> Assessing the fit</a></li>
<li class="chapter" data-level="3.2" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#the-basic-anova-table"><i class="fa fa-check"></i><b>3.2</b> The basic anova table</a>
<ul>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#cochrans-theorem"><i class="fa fa-check"></i>Cochran’s Theorem</a></li>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The coefficient of determination</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#example-cheddar-cheese-study"><i class="fa fa-check"></i>Example: Cheddar cheese study</a>
<ul>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#solution"><i class="fa fa-check"></i>Solution</a></li>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#anova-in-r"><i class="fa fa-check"></i>Anova in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#the-extra-sum-of-squares-method"><i class="fa fa-check"></i><b>3.3</b> The extra sum of squares method</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#the-extended-anova-table"><i class="fa fa-check"></i><b>3.3.1</b> The extended anova table</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#example-extra-sum-of-squares-for-cheese-data"><i class="fa fa-check"></i>Example: Extra sum of squares for cheese data</a></li>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#example-warfarin-study"><i class="fa fa-check"></i>Example: Warfarin study</a></li>
<li class="chapter" data-level="3.4" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#the-general-extra-sum-of-squares-method"><i class="fa fa-check"></i><b>3.4</b> The general extra sum of squares method</a></li>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#example-extended-extra-sum-of-squares-for-cheese-data"><i class="fa fa-check"></i>Example: Extended extra sum of squares for cheese data</a>
<ul>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#summary-extra-sum-of-squares-method"><i class="fa fa-check"></i>Summary: extra sum of squares method</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#example-anova-for-crime-data-based-on-summary-information"><i class="fa fa-check"></i>Example: Anova for crime data based on summary information</a></li>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#solution-1"><i class="fa fa-check"></i>Solution</a></li>
<li class="chapter" data-level="3.5" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#sec:infer"><i class="fa fa-check"></i><b>3.5</b> Inference on individual parameters</a></li>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#example-inference-on-individual-parameters---warfarin-example"><i class="fa fa-check"></i>Example: Inference on individual parameters - warfarin example</a></li>
<li class="chapter" data-level="3.6" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#confidence-and-prediction-intervals-for-the-fitted-values"><i class="fa fa-check"></i><b>3.6</b> Confidence and prediction intervals for the fitted values</a></li>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#example-confidence-and-prediction-intervals-for-the-cheese-data"><i class="fa fa-check"></i>Example: Confidence and prediction intervals for the cheese data</a></li>
<li class="chapter" data-level="3.7" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#sec:polymodels"><i class="fa fa-check"></i><b>3.7</b> Polynomial models</a></li>
<li class="chapter" data-level="" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#example-polynomial-model"><i class="fa fa-check"></i>Example: Polynomial model</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="inference-for-the-multiple-linear-regression-model.html"><a href="inference-for-the-multiple-linear-regression-model.html#choosing-the-order-of-a-polynomial-model"><i class="fa fa-check"></i><b>3.7.1</b> Choosing the order of a polynomial model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html"><i class="fa fa-check"></i><b>4</b> Analysis of designed experiments</a>
<ul>
<li class="chapter" data-level="4.1" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#completely-randomised-design"><i class="fa fa-check"></i><b>4.1</b> Completely randomised design</a></li>
<li class="chapter" data-level="" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#example-one-way-anova"><i class="fa fa-check"></i>Example: One-way anova</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#analysis-of-completely-randomised-design-data-in-r"><i class="fa fa-check"></i><b>4.1.1</b> Analysis of completely randomised design data in <code>R</code></a></li>
<li class="chapter" data-level="4.1.2" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#interpretation-of-results-multiple-comparisons"><i class="fa fa-check"></i><b>4.1.2</b> Interpretation of results: multiple comparisons</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#example-multiple-comparisons"><i class="fa fa-check"></i>Example: Multiple comparisons</a>
<ul>
<li class="chapter" data-level="4.1.3" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#model-checking"><i class="fa fa-check"></i><b>4.1.3</b> Model checking</a></li>
<li class="chapter" data-level="4.1.4" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#completely-randomised-design-dealing-with-quantitative-variables"><i class="fa fa-check"></i><b>4.1.4</b> Completely randomised design: dealing with quantitative variables</a></li>
<li class="chapter" data-level="" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#example-randomised-design-with-a-quantitative-variable"><i class="fa fa-check"></i>Example: Randomised design with a quantitative variable</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#randomised-block-design"><i class="fa fa-check"></i><b>4.2</b> Randomised block design</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#two-way-analysis-of-variance"><i class="fa fa-check"></i><b>4.2.1</b> Two-way analysis of variance</a></li>
<li class="chapter" data-level="4.2.2" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#orthogonality-and-testing-of-blocks-in-a-two-way-analysis-of-variance-model"><i class="fa fa-check"></i><b>4.2.2</b> Orthogonality and testing of blocks in a two-way analysis of variance model</a></li>
<li class="chapter" data-level="" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#example-two-way-anova-on-nitrate-data"><i class="fa fa-check"></i>Example: Two-way anova on nitrate data</a></li>
<li class="chapter" data-level="" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#example-chicken-egg-production"><i class="fa fa-check"></i>Example: Chicken egg production</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#factorial-experiments"><i class="fa fa-check"></i><b>4.3</b> Factorial experiments</a>
<ul>
<li class="chapter" data-level="" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#example-two-way-anova-with-interactions-for-the-yeast-data"><i class="fa fa-check"></i>Example: Two-way anova with interactions for the yeast data</a></li>
<li class="chapter" data-level="4.3.1" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#exploratory-plots-for-interactions"><i class="fa fa-check"></i><b>4.3.1</b> Exploratory plots for interactions</a></li>
<li class="chapter" data-level="" data-path="analysis-of-designed-experiments.html"><a href="analysis-of-designed-experiments.html#example-transforming-the-response"><i class="fa fa-check"></i>Example: Transforming the response</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="general-linear-models.html"><a href="general-linear-models.html"><i class="fa fa-check"></i><b>5</b> General linear models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="general-linear-models.html"><a href="general-linear-models.html#indicator-and-dummy-variables"><i class="fa fa-check"></i><b>5.1</b> Indicator and dummy variables</a></li>
<li class="chapter" data-level="" data-path="general-linear-models.html"><a href="general-linear-models.html#example-gasoline-data"><i class="fa fa-check"></i>Example: Gasoline data</a>
<ul>
<li class="chapter" data-level="" data-path="general-linear-models.html"><a href="general-linear-models.html#model-interpretation"><i class="fa fa-check"></i>Model interpretation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="general-linear-models.html"><a href="general-linear-models.html#model-selection-criteria"><i class="fa fa-check"></i><b>5.2</b> Model selection criteria</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="general-linear-models.html"><a href="general-linear-models.html#model-selection-criteria-adjusted-r2"><i class="fa fa-check"></i><b>5.2.1</b> Model selection criteria: adjusted <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-diagnostics" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Regression diagnostics<a href="regression-diagnostics.html#regression-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Chapter 1 introduced the multiple linear regression model, how to estimate the parameters and some properties of the estimators, amongst other things. We now consider the situation where, having fitted a model (or several candidate models), we wish to assess the validity of the model(s). Recall that when defining the multiple linear regression model we made a series of assumptions, which, in turn, allowed us to use maximum likelihood to estimate the parameters. In this chapter we describe a suite of methods that can be used to check if our model conforms to the assumptions. Namely, we can consider whether</p>
<ol type="i">
<li>
the relationship between <span class="math inline">\(Y\)</span> and the <span class="math inline">\(x\)</span>-variables is linear;
</li>
<li>
the errors are normally distributed;
</li>
<li>
the errors are uncorrelated;
</li>
<li>
the error term, <span class="math inline">\(\epsilon_i\)</span>, has constant variance <span class="math inline">\(\sigma_{\epsilon}^2\)</span>;
</li>
</ol>
Additionally, we will investigate whether any points
<ol type="i" start="5">
<li>
are unusual (outliers)
</li>
<li>
have a large effect on the regression coefficients (via their leverage).
</li>
<li>
are unduly influential (via their Cook’s distance);
</li>
</ol>
<p>You may have encountered some of these before. In the previous chapter, we informally assessed the assumption of a linear relationship through a scatterplot, or a series of pairwise scatterplots when we have more than one covariate. However, when there are many covariates (and particularly if some of these are categorical or ordinal) this can become an unwieldy approach.</p>
<p>Moreover, the underlying linear (or otherwise) relationship can be masked by the relationship of the response with the other variables; the residuals, however, are adjusted for this. As such, it is recommended to use residual checks to verify the functional form of the model, i.e. are the relationships linear? Happily, the residuals also allow us to check the remaining assumptions too, since they are our estimate of the true (but unknown) errors.</p>
<div id="standardised-residuals" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Standardised residuals<a href="regression-diagnostics.html#standardised-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall, from section <a href="introduction.html#sec:resfithat">1.3.3</a>, that the residual is the observed value minus the fitted value. Its definition and variance are given by
<span class="math display">\[\begin{align*}
\color{red}{\underline{\hat{\epsilon}}} &amp;\color{red}{= \underline{y} - \underline{\hat{y}}} \\
&amp;\color{red}{= (\mathrm{I} - \mathrm{H})\underline{y}}
\end{align*}\]</span>
and
<span class="math display">\[
\color{red}{\mathrm{Var}\left(\underline{\hat{\epsilon}}\right) = (\mathrm{I} - \mathrm{H})\sigma_{\epsilon}^2.}
\]</span></p>
<p>We might also like to think about the residuals and fitted values at the observation level, particularly if we are looking at observations that are unusual. Based on the above we have
<span class="math display">\[
\color{red}{\hat{\epsilon}_i = y_i - \hat{y}_i}
\]</span>
and
<span class="math display">\[
\color{red}{\mathrm{Var}(\hat{\epsilon}_i) = (1 - h_{ii})\sigma_{\epsilon}^2}
\]</span>
respectively, for <span class="math inline">\(i = 1, \ldots, n\)</span>.</p>
<p>Note that this informs us that the variance is not the same for each residual, since the <span class="math inline">\(h_{ii}\)</span> values will (typically) be different for each <span class="math inline">\(i\)</span>. Furthermore, the raw residuals are not scale invariant, so changing the unit of measurement would drastically affect the residuals, and the threshold for a large residual depends on the context at hand.</p>
<p>For instance, on data with populations measured in millions we would expect large residuals even in a well-behaved model, whereas in water treatment, or air quality, measurements are often made which amount to microscopic quantities (typically measured in parts per million) that would lead to miniscule residuals, even if the underlying model is poor.</p>
<p>To get around these issues, we work with the <em>standardised</em> residuals
<span class="math display">\[
\color{red}{\hat{e}_i = \frac{\hat{\epsilon}_i}{\sqrt{(1 - h_{ii})s^2}}}
\]</span></p>
<p>where <span class="math inline">\(s^2\)</span> is our estimate of <span class="math inline">\(\sigma_{\epsilon}^2\)</span> (see Chapter 1). The standardised residuals have a mean of zero (as do the raw residuals) since their sum is constrained to be zero - which induces dependence - and a variance of (approximately) one. Placing the residuals on a common scale also allows us to look for outliers or unusual observations more easily (see later). We can calculate the standardised residuals in <code>R</code> using the following command, where <code>fit2</code> is our second fitted model from chapter 1:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="regression-diagnostics.html#cb25-1" tabindex="-1"></a><span class="fu">rstandard</span>(fit2)</span></code></pre></div>
<pre><code>##           1           2           3           4           5           6 
## -0.20643768 -0.74827738 -0.80142866  0.54358256  0.85837659 -0.98421544 
##           7           8           9          10          11          12 
##  1.14666224 -0.09647632 -0.04022545 -0.22714152 -0.33189908 -0.46936793 
##          13          14          15          16          17          18 
##  0.09836234  1.13448521  1.56936911 -1.43201467  1.05443091 -0.82758724 
##          19          20          21          22          23          24 
## -1.03068909 -0.53747090 -1.26308890  0.19268170 -0.58264176  2.77358349</code></pre>
<p>We use the terminology of standardised residuals in this module since this is what is adopted in <code>R</code>. You may sometimes see the phrase ‘(internally) studentised’ residuals elsewhere (in MAS2902 for instance) for this same concept. There is also, as you might expect, an externally studentised residual, but this is not explored further here.</p>
<div id="residual-plots" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Residual plots<a href="regression-diagnostics.html#residual-plots" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We typically use visual inspection (i.e. plots) to check the model assumptions since the raw values themselves are hard to interpret. We primarily plot <span class="math inline">\(\hat{e}_i\)</span> against the fitted values <span class="math inline">\(\hat{y_i}\)</span>, and they can also be plotted against each of the explanatory variables in order to verify the functional form. If the model assumptions are satisfied, then, for each plot, the standardised residuals should be randomly scattered within a horizontal band:</p>
<p><img src="diagnostics_files/figure-html/blank1-1.png" width="65%" style="display: block; margin: auto;" /></p>
<p>Otherwise, we could get signs of non-constant variance, i.e. increasing, double bow, decreasing, or signs of non-linearity (see practical 2). In practice, nonlinearity is hard to distinguish from correlation in the standardised residuals. Some examples of residuals not conforming to model assumptions are given above.</p>
</div>
<div id="correlation-between-residuals-and-fitted-values" class="section level3 unnumbered hasAnchor">
<h3>Correlation between residuals and fitted values<a href="regression-diagnostics.html#correlation-between-residuals-and-fitted-values" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The reason we plot residuals against fitted values, rather than the observations themselves is because the residuals and fitted values are <em>uncorrelated</em>. This allows us to detect any unusual observations without worrying that these may be due to an underlying dependence between the quantities being plotted.</p>
<div id="proof-1" class="section level4 unnumbered hasAnchor">
<h4>Proof<a href="regression-diagnostics.html#proof-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[\begin{align*}
\color{red}{\mathrm{Cov}(\underline{e}, \underline{\hat{Y}})} &amp;\color{red}{= \mathrm{Cov}(\underline{Y} - \underline{\hat{Y}}, \underline{\hat{Y}})} \\
&amp;\color{red}{= \mathrm{Cov}(\left\{\mathrm{I} - \mathrm{H}\right\}\underline{Y},  \underline{\hat{Y}})} \\
&amp;\color{red}{=  \mathrm{Cov}(\left\{\mathrm{I} - \mathrm{H}\right\}\underline{Y},  \mathrm{H}\underline{Y})} \\
&amp;\color{red}{= (\mathrm{I} - \mathrm{H})\mathrm{Cov}(\underline{Y}, \underline{Y})\mathrm{H}^T} \\
&amp;\color{red}{= (\mathrm{I} - \mathrm{H})\mathrm{Var}(\underline{Y})\mathrm{H}^T} \\
&amp;\color{red}{= (\mathrm{I} - \mathrm{H})\sigma_{\epsilon}^2\mathrm{I}\mathrm{H}^T} \\
&amp;\color{red}{= \sigma_{\epsilon}^2 (\mathrm{I} - \mathrm{H}) \mathrm{H}^T} \\
&amp;\color{red}{=  \sigma_{\epsilon}^2 (\mathrm{I} - \mathrm{H})\mathrm{H}} \\
&amp;\color{red}{= \sigma_{\epsilon}^2(\mathrm{H} - \mathrm{H}^2)} \\
&amp;\color{red}{= \sigma_{\epsilon}^2 (\mathrm{H} - \mathrm{H})} \\
&amp;\color{red}{= \underline{0}}
\end{align*}\]</span></p>
<p>This result also hold for the standardised residuals and the fitted values, but the proof is more convoluted; the residuals and the observed values, however, are not independent.</p>
</div>
</div>
<div id="outliers" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Outliers<a href="regression-diagnostics.html#outliers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Outliers</em> are points which appear separated in some way from the remainder of the data. Part of the purpose of our residual plots is to check for unusual observations. The standardised residuals give a good initial indicator of outliers. If we assume the (raw) residuals are normally distributed (we will check this assumption shortly), then the standardised residuals are also normal, albeit with a different variance, which is close to one (see above).</p>
<p>Hence, the standardised residuals are assumed to be approximately <em>standard</em> normal, and this suggests that values outside the range <span class="math inline">\(\pm 2\)</span> are indicative of outliers. However, by normal distribution theory, we expect around <span class="math inline">\(5\%\)</span> of observations to be outside this range by chance - recall that <span class="math inline">\(\pm 1.96\)</span> cuts off <span class="math inline">\(2.5\%\)</span> of a standard normal distribution, i.e. <span class="math inline">\(Pr(Z &gt; 1.96) = 0.025\)</span>, where <span class="math inline">\(Z \sim N(0, 1)\)</span>. This value is often rounded to <span class="math inline">\(\pm 2\)</span> to act as a simple rule-of-thumb for residual plots. Clearly, the larger the absolute value of the standardised residual the more likely it is to be an outlier.</p>
<p>Note that it is not reasonable to remove outliers just because we don’t like the look of them! It is, of course, worth checking the original source of the data - if the outliers are a result of recording error, then this should be corrected. If the outliers appear genuine, we should see how discrepant they are. We can carry out an analysis with and without the suspect observations to investigate the sensitivity of the results.</p>
</div>
<div id="example-residual-analysis-for-pre-diabetes-data" class="section level3 unnumbered hasAnchor">
<h3>Example: Residual analysis for pre-diabetes data<a href="regression-diagnostics.html#example-residual-analysis-for-pre-diabetes-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Residual plots for the pre-diabetes data can be generated in <code>R</code> using the following commands:</p>
<div id="standardised-residuals-against-covariates" class="section level4 unnumbered hasAnchor">
<h4>Standardised residuals against covariates<a href="regression-diagnostics.html#standardised-residuals-against-covariates" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="regression-diagnostics.html#cb27-1" tabindex="-1"></a><span class="fu">plot</span>(bodyweight<span class="sc">$</span>Consumption, <span class="fu">rstandard</span>(fit2), </span>
<span id="cb27-2"><a href="regression-diagnostics.html#cb27-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Consumption&quot;</span>, <span class="at">ylab =</span><span class="st">&quot;Standardised residuals&quot;</span>)</span>
<span id="cb27-3"><a href="regression-diagnostics.html#cb27-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>), <span class="at">lty =</span> <span class="dv">2</span>) </span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="diagnostics_files/figure-html/unnamed-chunk-4-1.png" alt="Scatterplot of standardised residuals against (average) food consumption for the pre-diabetes data." width="65%" />
<p class="caption">
Figure 2.1: Scatterplot of standardised residuals against (average) food consumption for the pre-diabetes data.
</p>
</div>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="regression-diagnostics.html#cb28-1" tabindex="-1"></a><span class="fu">plot</span>(bodyweight<span class="sc">$</span>Exercise, <span class="fu">rstandard</span>(fit2), </span>
<span id="cb28-2"><a href="regression-diagnostics.html#cb28-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Exercise&quot;</span>, <span class="at">ylab =</span><span class="st">&quot;Standardised residuals&quot;</span>)</span>
<span id="cb28-3"><a href="regression-diagnostics.html#cb28-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>), <span class="at">lty =</span> <span class="dv">2</span>) </span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="diagnostics_files/figure-html/unnamed-chunk-6-1.png" alt="Scatterplot of standardised residuals against (average) food consumption for the pre-diabetes data." width="65%" />
<p class="caption">
Figure 2.2: Scatterplot of standardised residuals against (average) food consumption for the pre-diabetes data.
</p>
</div>
</div>
<div id="standardised-residuals-against-fitted-values" class="section level4 unnumbered hasAnchor">
<h4>Standardised residuals against fitted values<a href="regression-diagnostics.html#standardised-residuals-against-fitted-values" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="regression-diagnostics.html#cb29-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">fitted.values</span>(fit2), <span class="fu">rstandard</span>(fit2), </span>
<span id="cb29-2"><a href="regression-diagnostics.html#cb29-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Fitted values&quot;</span>, <span class="at">ylab =</span><span class="st">&quot;Standardised residuals&quot;</span>)</span>
<span id="cb29-3"><a href="regression-diagnostics.html#cb29-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>), <span class="at">lty =</span> <span class="dv">2</span>) </span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="diagnostics_files/figure-html/unnamed-chunk-8-1.png" alt="Scatterplot of standardised residuals against fitted values for the multiple linear regession model with consumption and exercise." width="65%" />
<p class="caption">
Figure 2.3: Scatterplot of standardised residuals against fitted values for the multiple linear regession model with consumption and exercise.
</p>
</div>
<p>In each of the above plots, we have added a dashed horizontal line at zero to help look for patterns, we would expect around half of the standardised residuals to lie both above and below the line in each case. Further dashed lines at <span class="math inline">\(\pm 2\)</span> allow us to identify how many points lie outside these bounds (recall, we expect <span class="math inline">\(\approx 5\%\)</span> in a well-behaved model).</p>
<p>To summarise these plots we can say:</p>
<p><span style="color: red;">- there is no clear pattern in the residuals although there is a (mild) suggestion of curvature in the plot against exercise.</span></p>
<p><span style="color: red;"> - the variance does not appear to change in a systematic way.</span></p>
<p><span style="color: red;">- there is one point with a standardised residual greater than 2 in modulus. Possible outlier? We expect about <span class="math inline">\(5\%\)</span> to be in this range, i.e. 1 in a sample size of 24.</span></p>
<p><span style="color: red;">Thus there is no clear evidence of a departure from our assumptions.</span></p>
<p>Note that when solely discrete (or ordinal) variable(s) are included in the model, the residual plot appears as strips of points at the observed values for the covariate. This is to be expected! We can still assess variability by checking whether the length of the strip is roughly the same and that it is centred around zero.</p>
</div>
</div>
<div id="normality-of-the-residuals" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Normality of the residuals<a href="regression-diagnostics.html#normality-of-the-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As mentioned earlier, analysis of the multiple linear regression model hinges on assumptions of normality (and independence) of the errors; you will consider models that deviate from this assumption in semester two. Hence, we need to check this assumption after fitting a model as the consequences of non-normality of the residuals are:</p>
<ul>
<li><p>the least squares estimates, <span class="math inline">\(\underline{\hat{\beta}}\)</span>, may not be optimal;</p></li>
<li><p>the associated tests and confidence intervals are inaccurate.</p></li>
</ul>
<p>However, it has been shown that only really long-tailed distributions cause a major problem and that mild non-normality can safely be ignored. Furthermore, the larger the sample size the more the the non-normality is the more the consequences are mitigated. To get the normality plot for the pre-diabetes data, we use the following commands:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="regression-diagnostics.html#cb30-1" tabindex="-1"></a><span class="fu">qqnorm</span>(<span class="fu">rstandard</span>(fit2), <span class="at">ylab =</span> <span class="st">&quot;Standardised residuals&quot;</span>)</span>
<span id="cb30-2"><a href="regression-diagnostics.html#cb30-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="diagnostics_files/figure-html/unnamed-chunk-10-1.png" alt="Normality plot of standardised residuals." width="65%" />
<p class="caption">
Figure 2.4: Normality plot of standardised residuals.
</p>
</div>
<p>As before, for the <code>R</code> code to work we must have previously defined the object <code>fit2</code>.</p>
<p>Comment: The standardised residuals fit fairly well to a straight line but are being partially distorted by the potential outlier. Successive observations in the plot are not independent and so `ripples’ often occur by chance.</p>
</div>
<div id="anderson-darling-test" class="section level3 hasAnchor" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Anderson-Darling test<a href="regression-diagnostics.html#anderson-darling-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Visual inspection of residual plots is a useful way to see where any departures from our assumptions may lie. However, it may also be good to have a summary measure via a formal statistical test to assess where there is a significant departure from normality. The most commonly used statistic is the <em>Anderson-Darling</em> (AD) statistic - <code>R</code> outputs the test statistic and a p-value when carrying out an AD test. The null hypotheses is that the residuals can reasonably be assumed to come from a normal distribution, with the alternative hypothesis stating the converse. As such, large p-values imply the normality assumption is fine, small p-values imply a significant departure from normality.</p>
<p>Comment: Note that there are some issues with the AD test for large sample sizes as it is very sensitive to outliers. From above, we know that our model is reasonably robust to outliers so we should be careful not to over-interpret an AD test if the plots pass a visual check.</p>
<div id="anderson-darling-test-in-r" class="section level5 unnumbered hasAnchor">
<h5>Anderson-Darling test in <code>R</code><a href="regression-diagnostics.html#anderson-darling-test-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>To carry out an AD test in <code>R</code> we first need to load the library <code>nortest</code>. For the pre-diabetes example:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="regression-diagnostics.html#cb31-1" tabindex="-1"></a><span class="fu">library</span>(nortest)</span>
<span id="cb31-2"><a href="regression-diagnostics.html#cb31-2" tabindex="-1"></a><span class="fu">ad.test</span>(<span class="fu">rstandard</span>(fit2))</span></code></pre></div>
<pre><code>## 
##  Anderson-Darling normality test
## 
## data:  rstandard(fit2)
## A = 0.53928, p-value = 0.1494</code></pre>
<p>As the p-value is fairly large in this case, we do not have a significant departure from normality and conclude that the assumption is not disputed for this model.</p>
</div>
</div>
<div id="a-cautionary-note" class="section level3 unnumbered hasAnchor">
<h3>A cautionary note<a href="regression-diagnostics.html#a-cautionary-note" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the assumptions of the model is that the true (but unknown) errors, <span class="math inline">\(\underline{\epsilon}\)</span>, follow a normal distribution. We assess this using the estimated errors, i.e. the residuals. The assumed normality of the errors also induces normality on the response, but this is conditional on the values of the covariates, namely <span class="math inline">\(Y_i \mid \underline{x}_i, \underline{\beta}, \sigma_{\epsilon}^2 \sim N(\underline{x}_i^T \underline{\beta}, \sigma_{\epsilon}^2)\)</span>. As such we cannot judge the normality assumption on plots of the response variable alone - even though it may be tempting or even feel intuitive to do so - without taking into account the values of the covariates. This can be awkward to construct whereas a residual check is straightforward (and will show the same thing).</p>
<p>Consider a multiple linear regression model with one continuous covariate (<span class="math inline">\(x_1\)</span>) and one binary covariate (<span class="math inline">\(x_2\)</span>), such as a treatment arm in a trial. If there are considerable differences between the two treatment groups then this will induce a bimodal distribution on the (unconditional) response variable, <span class="math inline">\(\underline{Y}\)</span>:</p>
<p><img src="Graphics/cache/y_bimodal.png" width="65%" style="display: block; margin: auto;" /></p>
<p>However, if we then fit a multiple linear regression model and look at the (standardised) residuals we see an approximate normal distribution</p>
<p><img src="Graphics/cache/e_caution.png" width="65%" style="display: block; margin: auto;" /></p>
<p>Similarly, we can inspect quantile-quantile plots of both the response (below left) and the (standardised) residuals (below right).</p>
<p><img src="Graphics/cache/y_bimodal_norm.png" width="65%" style="display: block; margin: auto;" /><img src="Graphics/cache/e_caution_norm.png" width="65%" style="display: block; margin: auto;" /></p>
<p>We observe that the upper plot does not conform to a straight line relationship whereas the right-hand one does. Note in passing that an AD test gives <span class="math inline">\(p &lt; 0.001\)</span> (reject <span class="math inline">\(H_0\)</span>) for the response variable and <span class="math inline">\(p &gt; 0.10\)</span> (retain <span class="math inline">\(H_0\)</span>) for the residuals.</p>
</div>
</div>
<div id="regression-diagnostics-1" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Regression diagnostics<a href="regression-diagnostics.html#regression-diagnostics-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As well as a raft of residual checks, two other metrics are commonly used to check the fit of a regression model - leverage and influence. We first consider leverage.</p>
<div id="leverage-values" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Leverage values<a href="regression-diagnostics.html#leverage-values" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An observation with an extreme value in the <span class="math inline">\(x\)</span>-space (explanatory variables) is called a point with high leverage. Leverage is a measure of how far the explanatory variables deviate from their mean. Recall that the hat matrix, <span class="math inline">\(\mathrm{H}\)</span> is defined as</p>
<p><span class="math display">\[
\color{red}{\mathrm{H} = \mathrm{X}(\mathrm{X}^T\mathrm{X})^{-1}\mathrm{X}^T}
\]</span></p>
<p>High leverage points can have an unusually large effect on the estimates of regression coefficients. To detect high leverage points we look for large values of <span class="math inline">\(h_{ii}\)</span>, the diagonal elements of <span class="math inline">\(\mathrm{H}\)</span>, which are known as the <em>leverages</em>. Note that the leverage values depend on the covariates alone, and not on the response variable.</p>
<p>If the value of <span class="math inline">\(h_{ii}\)</span> is large, then <span class="math inline">\(\mathrm{Var}[\hat{\epsilon}_i]\)</span> will be small, i.e. the fit will be close to <span class="math inline">\(Y_i\)</span>, since <span class="math inline">\(\mathrm{Var}[\hat{\epsilon}_i] = (1 - h_{ii})\sigma_{\epsilon}^2\)</span> and this will approach zero as the hat values get close to unity. This means that the regression line is `forced’ to fit well to points with a large leverage value so these points have the potential to severely alter the gradient of the regression line. A consequence of this is that the variance of <span class="math inline">\(\hat{Y_i}\)</span>, which is given by <span class="math inline">\(h_{ii}\sigma_{\epsilon}^2\)</span>, will be at its largest for points of high leverage. The sketches below demonstrate this.</p>
<p><img src="diagnostics_files/figure-html/blank2-1.png" width="65%" style="display: block; margin: auto;" /></p>
<div id="properties-of-the-leverage-values" class="section level4 unnumbered hasAnchor">
<h4>Properties of the leverage values<a href="regression-diagnostics.html#properties-of-the-leverage-values" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li>The leverages are bounded between 0 and 1, i.e. <span class="math inline">\(0 \leq h_{ii} \leq 1\)</span></li>
<li>The sum of the leverages is equal to the number of parameters in the model (including the intercept), i.e. <span class="math inline">\(\Sigma_{i=1}^n h_{ii} = p+1\)</span></li>
</ol>
<div id="derivation" class="section level5 unnumbered hasAnchor">
<h5>Derivation<a href="regression-diagnostics.html#derivation" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span style="color: red;">We will focus on the second property only. Recall that the trace of a (symmetric) matrix is the sum of the diagonal elements. Now,</span></p>
<p><span class="math display">\[\begin{align*}
\color{red}{\Sigma_{i=1}^n h_{ii}} &amp; \color{red}{= \mathrm{tr}(\mathrm{H})} \\
&amp;\color{red}{= \mathrm{tr}\left(\mathrm{X}(\mathrm{X^T X})^{-1}\mathrm{X}^T\right)} \\
&amp;\color{red}{= \mathrm{tr}\left(\mathrm{X}^T\mathrm{X}(\mathrm{X^T X})^{-1}\right)} \\
&amp;\color{red}{= \mathrm{tr}(\mathrm{I}_{p+1})} \\
&amp;\color{red}{= p + 1.}
\end{align*}\]</span></p>
<p>Usually, a value of <span class="math inline">\(h_{ii} &gt; 2(p + 1)/n\)</span> is regarded as indicating a point of high leverage, where <span class="math inline">\(p\)</span> is the number of explanatory variables in the model. Note that the leverage values depend on the <span class="math inline">\(x\)</span>-variables alone, and not on the response variable.</p>
</div>
</div>
</div>
<div id="example-leverage-values-for-the-pre-diabetes-data" class="section level3 unnumbered hasAnchor">
<h3>Example: Leverage values for the pre-diabetes data<a href="regression-diagnostics.html#example-leverage-values-for-the-pre-diabetes-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As for the residual checks, we typically plot the leverage values to look for large values. This can be achieved in <code>R</code> via the following commands:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="regression-diagnostics.html#cb33-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">hatvalues</span>(fit2), <span class="at">ylab =</span><span class="st">&quot;Leverages&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb33-2"><a href="regression-diagnostics.html#cb33-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">2</span><span class="sc">*</span><span class="dv">3</span><span class="sc">/</span><span class="dv">24</span> , <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:levsplot1"></span>
<img src="diagnostics_files/figure-html/levsplot1-1.png" alt="Leverage values from mutliple linear regression model for pre-diabetes data." width="65%" />
<p class="caption">
Figure 2.5: Leverage values from mutliple linear regression model for pre-diabetes data.
</p>
</div>
<p>Note that <span class="math inline">\(p = 2\)</span> here (and <span class="math inline">\(n = 24\)</span>) since we have two explanatory variables: consumption and exercise. From the plot, we can identify that there are two points of high leverage here. For large datasets, it may be more prudent to use <code>R</code> to find out how many points exceed the threshold</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="regression-diagnostics.html#cb34-1" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">hatvalues</span>(fit2) <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span><span class="dv">3</span><span class="sc">/</span><span class="dv">24</span>)</span></code></pre></div>
<pre><code>## 
## FALSE  TRUE 
##    22     2</code></pre>
<p>If any points are flagged we can identify them using</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="regression-diagnostics.html#cb36-1" tabindex="-1"></a>levs <span class="ot">&lt;-</span> <span class="fu">hatvalues</span>(fit2)</span>
<span id="cb36-2"><a href="regression-diagnostics.html#cb36-2" tabindex="-1"></a>levs[levs <span class="sc">&gt;</span> <span class="dv">2</span><span class="sc">*</span><span class="dv">3</span><span class="sc">/</span><span class="dv">24</span>]</span></code></pre></div>
<pre><code>##         1         3 
## 0.2942944 0.4251701</code></pre>
<p>This tells us that it is points 1 and 3 in this case.</p>
</div>
<div id="influential-observations" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Influential observations<a href="regression-diagnostics.html#influential-observations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Some points have more influence on the regression analysis than others. These are not necessarily clear outliers or points of high leverage, but are typically fairly large in both regards. One approach for detecting such observations is using Cook’s distance, which is defined by:</p>
<p><span class="math display">\[
\color{red}{D_i = \frac{1}{p+1}(\hat{e}_i)^2 \frac{h_{ii}}{1 - h_{ii}}}
\]</span></p>
<p>for <span class="math inline">\(i = 1, \ldots, n\)</span>, and <span class="math inline">\(\hat{e}_i\)</span> and <span class="math inline">\(h_{ii}\)</span> are the standardised residual and leverage values respectively defined earlier. Although not clear from the formula, Cook’s distance is a measure of how much the fitted values in the model would change if the <span class="math inline">\(i^{th}\)</span> data point was deleted.</p>
<p>Large values of <span class="math inline">\(D_i\)</span> indicate that the point has a large influence on the model. We can calculate and plot the Cook’s distances in <code>R</code> using:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="regression-diagnostics.html#cb38-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">cooks.distance</span>(fit2), <span class="at">ylab =</span> <span class="st">&quot;Cooks distance&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cooksplot1"></span>
<img src="diagnostics_files/figure-html/cooksplot1-1.png" alt="Cook distances from mutliple linear regression model for pre-diabetes data." width="65%" />
<p class="caption">
Figure 2.6: Cook distances from mutliple linear regression model for pre-diabetes data.
</p>
</div>
<p>There is no agreed threshold to identify influential observations, unlike for points of high leverage. However, a value of 1 has been suggested but this is typically conservative. A pragmatic approach is to investigate any (groups of) observations that appear to have larger values than the others. In our example, taking a threshold of 0.10 appears sensible.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="regression-diagnostics.html#cb39-1" tabindex="-1"></a>cooks <span class="ot">&lt;-</span> <span class="fu">cooks.distance</span>(fit2)</span>
<span id="cb39-2"><a href="regression-diagnostics.html#cb39-2" tabindex="-1"></a>cooks[cooks <span class="sc">&gt;</span> <span class="fl">0.10</span>]</span></code></pre></div>
<pre><code>##         3        15        21        24 
## 0.1583550 0.2177828 0.1302879 0.1374646</code></pre>
<p>We see that point 15 has the largest Cook’s distance, then point 3, and then point 24. Point 24 was the point with the largest absolute value of the standardised residuals; 3 was the point with the highest leverage. Observation 15 has the second largest absolute value of the standardised residuals, and the third highest leverage.</p>
</div>
<div id="dealing-with-unusual-observations" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Dealing with unusual observations<a href="regression-diagnostics.html#dealing-with-unusual-observations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What should we do with the influential and/or high leverage points? As a first pass, we should check that they have been entered correctly, either by ourselves or by a data clerk or external source, but this may not be possible. Visually, we can plot the data with them highlighted and check whether they stand out. Can they be explained? The following <code>R</code> code produces such a plot for our example, where we have highlighted points 3 (highest leverage), 15 (largest Cook’s distance) and 24 (large outlier) as X, Y and Z respectively. We also colour-code points by exercise (1 is black, 2 is red, 3 is green):</p>
<div class="figure" style="text-align: center">
<img src="diagnostics_files/figure-html/label_inf_plot1-1.png" alt="Scatterplot for pre-diabetes data with unusual points identified." width="65%" />
<p class="caption">
(#fig:label_inf_plot1)Scatterplot for pre-diabetes data with unusual points identified.
</p>
</div>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="regression-diagnostics.html#cb41-1" tabindex="-1"></a><span class="co"># Or in one command</span></span>
<span id="cb41-2"><a href="regression-diagnostics.html#cb41-2" tabindex="-1"></a><span class="fu">points</span>(bodyweight[<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">15</span>, <span class="dv">24</span>), <span class="dv">2</span><span class="sc">:</span><span class="dv">1</span>], <span class="at">pch  =</span> <span class="fu">c</span>(<span class="st">&quot;X&quot;</span>, <span class="st">&quot;Y&quot;</span>, <span class="st">&quot;Z&quot;</span>))</span></code></pre></div>
<p>We see that the point with the highest leverage (point 3, X) has an unusually large value for consumption. Point 24 (Z) - the possible outlier - does not look that unusual, being central in the consumption range, albeit with a higher value for weight than we might expect. Point 15 (Y), with the largest Cook’s distance, has the lowest consumption and a high weight value for that group (the green points). We could remove the most influential point and re-fit the model, but it does not change the results much here (this is left as an exercise).</p>
</div>
<div id="example-model-checking-for-the-premier-league-data" class="section level3 unnumbered hasAnchor">
<h3>Example: Model checking for the Premier League data<a href="regression-diagnostics.html#example-model-checking-for-the-premier-league-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now return to Example 1.10 from chapter 1 and carry out the full suite of checks (i.e. outliers, points of high leverage/influence and a normality test) on the model for points scored using clean sheets as the single (mean-centered) covariate.</p>
<p>To recap, the call to fit the model (after mean-centering the variable) was</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="regression-diagnostics.html#cb42-1" tabindex="-1"></a>CleanSheetsScaled <span class="ot">&lt;-</span> <span class="fu">scale</span>(prem<span class="sc">$</span>CleanSheets, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb42-2"><a href="regression-diagnostics.html#cb42-2" tabindex="-1"></a>fit_mean_centre <span class="ot">&lt;-</span> <span class="fu">lm</span>(Points <span class="sc">~</span> CleanSheetsScaled, <span class="at">data =</span> prem)</span></code></pre></div>
<p>A scatterplot of the original data, with the line of best fit superimposed, is included below, with each point labelled by final league position, from 1 (top) to 20 (bottom):</p>
<p><img src="diagnostics_files/figure-html/prem_label-1.png" width="65%" style="display: block; margin: auto;" /></p>
<ol type="a">
<li>
Interpret the plot of the (standardised) residuals.
</li>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-22"></span>
<img src="diagnostics_files/figure-html/unnamed-chunk-22-1.png" alt="Plot of standardised residuals against fitted values." width="65%" />
<p class="caption">
Figure 2.7: Plot of standardised residuals against fitted values.
</p>
</div>
<p><span style="color: red;">- All but one of the residuals lies in <span class="math inline">\((-2, 2)\)</span>, and we would expect one by chance in this dataset of twenty observations.</span></p>
<p><span style="color: red;">- There is no discernible pattern to the (standardised) residuals with random scatter within our horizontal band, with no evidence of nonlinearity or non constant variance.</span></p>
<li>
Assess the normality assumption of the residuals and interpret the output of the Anderson-Darling test.
</li>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-23"></span>
<img src="diagnostics_files/figure-html/unnamed-chunk-23-1.png" alt="Normality plot of standardised residuals." width="65%" />
<p class="caption">
Figure 2.8: Normality plot of standardised residuals.
</p>
</div>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="regression-diagnostics.html#cb43-1" tabindex="-1"></a><span class="fu">ad.test</span>(<span class="fu">rstandard</span>(fit_mean_centre)) <span class="co"># See Chapter 1</span></span></code></pre></div>
<pre><code>## 
##  Anderson-Darling normality test
## 
## data:  rstandard(fit_mean_centre)
## A = 0.33253, p-value = 0.4807</code></pre>
<p><span style="color: red;">- From the plot, the points lie close to the nominal 45 degree line.</span></p>
<p><span style="color: red;">- Due to the correlation between (standardised) residuals we expect to see <code>bumps' or</code>ripples’ and this does not invalidate the normality assumption.</span></p>
<p><span style="color: red;">- For the AD test, the p-value is not significant at any conventional level so we retain the null hypothesis that the (standardised) residuals can be assumed to be normally distributed.</span></p>
<li>
Use regression diagnostics to identify any points of high leverage, or of high influence.
</li>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-25"></span>
<img src="diagnostics_files/figure-html/unnamed-chunk-25-1.png" alt="Plot of leverages (left) and Cook distances (right) indexed by final league position." width="48%" /><img src="diagnostics_files/figure-html/unnamed-chunk-25-2.png" alt="Plot of leverages (left) and Cook distances (right) indexed by final league position." width="48%" />
<p class="caption">
Figure 2.9: Plot of leverages (left) and Cook distances (right) indexed by final league position.
</p>
</div>
<p><span style="color: red;">The leverage values are plotted on the left, and the Cook’s distances on the right. We can see one point above the threshold (dashed line) for the leverages (which is <span class="math inline">\(2\times 2/20 = 0.20\)</span> here). This corresponds to observation 2. On inspection of the original data, we see that this is the team (Manchester City) with the most clean sheets, so is large in <span class="math inline">\(x\)</span> space, thus having the capacity to alter the regression line.</span></p>
<p><img src="diagnostics_files/figure-html/unnamed-chunk-26-1.png" width="65%" style="display: block; margin: auto;" /></p>
<p><span style="color: red;">For the influential points, we see that the largest is for observation 1. This point lies a considerable distance from the regression line, our model would expect fewer points based on this covariate value. Other covariates may be needed in the model. The next largest Cook’s distances (points 5 and 7) are not that much larger than the body of the points so we do not consider them further, apart from observing that they deviate the most from the fitted line (for point 5 it is above the line of best fit, for point 7 it is below).</span></p>
</ol>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inference-for-the-multiple-linear-regression-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/diagnostics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["notes.pdf", "notes.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
