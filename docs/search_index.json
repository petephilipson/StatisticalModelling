[["index.html", "MAS3928: Statistical Modelling", " MAS3928: Statistical Modelling "],["module-information.html", "Module information Lecturer information Module schedule Course materials Assessment Relevant texts", " Module information Lecturer information This is a ten credit module, taught during semester one by Dr. Pete Philipson. My contact details are: Dr. Pete Philipson Herschel Building, Room 3.14 peter.philipson1@ncl.ac.uk I have reserved Tuesdays 1pm - 2pm and Thursdays 11am - 12pm (both from week 3 onwards) as “student support hours” for this module. Beyond this, I operate an open-door policy so you are welcome to drop by at your own convenience - there will be a timetable outside my door to help find a suitable time. Module schedule Two lectures per week: Tuesdays 11am - 12pm (LT2) &amp; Wednesdays 10am - 11am (LT3) Computer practical sessions in weeks 3, 5, 7, 9 &amp; 11 (Thursday 3pm - 4pm) of the semester in the Herschel Cluster Note that the term has a reading/self-study week following week 6 so there are no timetabled sessions for a week at this juncture. Course materials All course materials will be available from Canvas. Announcements will also be made on Canvas and to your University email address. Assessment The module will be assessed by two assignments - one summative assessment worth 20% of the credit for the module, and one formative assessment (worth 0% of the credit) - and a written exam in January 2026, which will be worth the remaining \\(80\\%\\) of the module credit. The provisional in-course-assessment schedule can be found below. Submission will be electronic, via Canvas, at 4pm on the specified day. Table 0.1: Provisional in-course-assessment schedule. Type Weight (%) Release date Submission date Formative 0 Wednesday 15th October Wednesday 22nd October Summative 20 Wednesday 22nd October Wednesday 12th November Relevant texts Fox (1997); Weisberg (2005); Harrell Jr (2015); Sheather (2009); Faraway (2014). "],["introduction.html", "1 Introduction 1.1 Multiple linear regression 1.2 Matrix form of the model Example - Matrix form for pre-diabetes data 1.3 Parameter estimation Example: Multiple linear regresion analysis of bodyweight data 1.4 Expectations, variances and inference 1.5 Multiple linear regression in R Example: Analysis of bodyweight data using R 1.6 The role of the intercept Example: Analysis of bodyweight data without an intercept term Example: Analysis of men’s Premier League football data - the role of the intercept Example: Mean-centering (men’s Premier League football data) 1.7 Properties of \\((\\mathrm{X}^T\\mathrm{X})^{-1}\\): multicollinearity Example: Multicollinearity in men’s Premier League football data", " 1 Introduction Regression is one of the fundamental topics in statistical modelling, providing a mechanism for expressing the potential dependence of some response of interest on a number of covariates. Its uses are widespread, permeating almost every scientific field, providing the bedrock for analysis of data arising from medicine, population health, demography, agriculture, sports and many more. In this module we will focus on the particular modelling case of when the response variable is continuous. We will begin with a recap of the multiple linear regression model in chapter 1, extending some of the ideas you have seen in MAS2902. Subsequently, we will consider checking our core assumptions and using regression diagnostics to identify any unusual observations. The method of analysis of variance (anova) will then be introduced in chapter 3, allowing us to remove predictors from a model using the extra sum of squares technique. Focus then shifts in chapter 4 to looking at models to handle factors in the context of designed experiments, giving rise to classic one-way and two-way anova models. The fifth and final chapter will introduce indicator variables to handle binary explanatory variables and consider model selection techniques. 1.1 Multiple linear regression As you have seen in MAS2902, there are often several possible explanatory variables to consider in a regression model. For example, in modelling someone’s weight we may want to include height and age (as well as a few other things) as covariates. We can extend the simple linear regression model to the multiple linear regression model \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots \\beta_p x_{ip} + \\epsilon_i \\tag{1.1} \\] for \\(i = 1, \\ldots n\\). As in simple linear regression, we make assumptions about the (unobserved) error terms, namely that they have zero mean and are independently normally distributed, i.e. \\(\\epsilon_i \\sim N(0, \\sigma_{\\epsilon}^2)\\) and \\(\\textrm{Cov}(\\epsilon_i, \\epsilon_j) = 0\\). Note our model in (1.1) is called a multiple linear regression model because it is linear in the parameters, \\(\\underline{\\beta} = \\left(\\beta_0, \\ldots, \\beta_p\\right)^T\\) - it does not have to be linear in the explanatory variables. For example, the following are multiple linear regression models \\[\\begin{eqnarray*} Y_i &amp;=&amp; \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\\\ Y_i &amp;=&amp; \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}^2 + \\beta_3 \\ln x_{i3} + \\epsilon_i \\end{eqnarray*}\\] for \\(i = 1, \\ldots n\\). To see this, we can substitute \\(w_i\\), say, for \\(x_i^2\\) in the first model. However, the below example is not a multiple linear regression model: \\[ Y_i = \\beta_0 + e^{\\beta_1 x_{i1} + \\beta_2 x_{i2}} + \\epsilon_i \\;\\; \\textrm{for}\\;\\; i = 1, \\ldots n. \\] Sometimes seemingly nonlinear models can be transformed to linear models, but this will have a knock-on effect on the assumptions. 1.2 Matrix form of the model Suppose we have \\(n\\) observations then equation (1.1) gives us the following system of equations: \\[\\begin{eqnarray*} Y_1 &amp;=&amp; \\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{12} + \\ldots \\beta_p x_{1p} + \\epsilon_1 \\\\ Y_2 &amp;=&amp; \\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22} + \\ldots \\beta_p x_{2p} + \\epsilon_2 \\\\ &amp;\\vdots&amp; \\\\ Y_n &amp;=&amp; \\beta_0 + \\beta_1 x_{n1} + \\beta_2 x_{n2} + \\ldots \\beta_p x_{np} + \\epsilon_n. \\end{eqnarray*}\\] Note that the sample size is usually far greater than the number of covariates (\\(n&gt;&gt;p\\)), i.e. the problem is . This set-up is more conveniently represented in matrix format by placing This leads to \\[\\begin{align*} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\\\ \\end{pmatrix} = \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12}&amp; \\ldots&amp; x_{1p} \\\\ 1 &amp; x_{21} &amp; x_{22}&amp; \\ldots&amp; x_{2p} \\\\ \\vdots&amp; \\vdots&amp; \\vdots&amp; \\vdots&amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2}&amp; \\ldots&amp; x_{np} \\\\ \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}. \\end{align*}\\] This can be compactly written as \\[\\begin{equation} \\underline{Y} = \\mathrm{X}\\underline{\\beta} + \\underline{\\epsilon}. \\end{equation}\\] The associated dimensions are. (i) \\([n \\times 1]\\) for the response vector, \\(\\underline{Y}\\), (ii) \\([n \\times (p+1)]\\) for the design matrix, \\(\\mathrm{X}\\), (iii) \\([(p+1) \\times 1]\\) for the parameter vector, \\(\\underline{\\beta}\\), (iv) \\([n \\times 1]\\) for the error vector, \\(\\underline{\\epsilon}\\). The column of ‘1’s in the design matrix is necessary to give the constant term for each observation, i.e. the intercept. We will discuss the role of the intercept in greater detail later. We can collect the covariate information for each item or individual into a vector alongside the ’1’ for the intercept: \\[ \\underline{x}_i = \\begin{pmatrix} 1 \\\\ x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\ x_{ip} \\end{pmatrix}, \\] for \\(i = 1, \\ldots, n\\). This allows us to express the design matrix as \\[ \\mathrm{X} = \\begin{pmatrix} \\underline{x}_1^T \\\\ \\underline{x}_2^T \\\\ \\vdots \\\\ \\underline{x}_n^T \\end{pmatrix}. \\] It is sometimes convenient to set \\(\\underline{x}_i^* = (x_{i1}, x_{i2}, \\ldots x_{ip})\\) for \\(i = 1, \\ldots, n\\). This is the vector of explanatory variables alone (i.e. no intercept term) for the \\(i^{th}\\) observation, which we will make use of later. We will now illustrate the multivariate regression model with a few examples. Example - Matrix form for pre-diabetes data We now introduce a specific example, which we shall return to throughout the chapter. In a study on body weight in 24 patients over 50, who have been diagnosed with pre-diabetes. Data were collected on their weight (in kilograms), food consumption (in calories), and a(n) (ordinal) measure of how much exercise each patient takes, on average per week: 0 = no exercise, 1 = some exercise, 2 = moderate exercise, 3 = heavy exercise. The food consumption was calculated by averaging over a week’s consumption and we will treat the ordinal covariate (exercise) as if it was continuous. The full data are given in Table 1.1 Table 1.1: Weight, average food consumption and exercise score for twenty-four pre-diabetes patients in body weight study. Weight (kg) Consumption (cal) Exercise Weight (kg) Consumption (cal) Exercise 60.4 2680 3 84.4 3160 0 81.1 3280 1 93.0 3330 0 94.9 3890 2 61.3 2360 2 86.4 3170 0 74.9 3030 0 90.3 3390 1 94.3 3390 0 60.4 2670 2 61.8 2700 2 77.8 2770 0 78.1 3090 0 85.0 3330 1 74.8 3020 1 71.6 2710 0 59.0 2410 0 64.6 2600 1 69.2 2830 2 75.1 2880 0 67.1 2620 0 89.6 3430 0 82.4 2820 1 We can produce scatterplots of weight against consumption and exercise: Figure 1.1: Scatterplot of weight against (average) food consumption (left) and exercise (right) for the pre-diabetes data Comments: We see that body weight is approximately linearly related to food consumption with a positive slope. There may be one unusual observation (top-right)? Could this be influential? Could others? We will consider these questions in more detail in the next chapter. There is some evidence that body weight declines with increasing exercise but the effect is possibly being masked by the variability in food consumption. Note that we observe vertical strips when dealing with ordinal variables so trends are perhaps harder to see. We wish to fit the model: \\[ \\color{red}{Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i} \\] for \\(i = 1, \\ldots, n\\), where \\(x_{i1}\\) is food consumption and \\(x_{i2}\\) is exercise for the \\(i^{th}\\) patient. Equivalently, using our matrix formulation, we can express the model as \\[ \\color{red}{\\underline{Y} = \\mathrm{X}\\underline{\\beta} + \\underline{\\epsilon}} \\] We can define the relevant quantities using the available data for the multiple linear regression model as follows: \\[\\begin{align*} \\color{red}{ \\underline{Y} = \\begin{pmatrix} 60.4 \\\\ 81.1 \\\\ \\vdots \\\\ 82.4 \\\\ \\end{pmatrix}, \\mathrm{X} = \\begin{pmatrix} 1 &amp; 2680&amp; 3 \\\\ 1 &amp; 3280&amp; 1 \\\\ \\vdots&amp; \\vdots&amp; \\vdots \\\\ 1 &amp; 2820&amp; 1 \\\\ \\end{pmatrix}, \\underline{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\end{pmatrix}, \\underline{\\epsilon} = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{24} \\\\ \\end{pmatrix}.} \\end{align*}\\] Note that both \\(\\underline{Y}\\) and \\(\\underline{\\epsilon}\\) are unchanged by the addition of variables since these are fixed data in the case of the former, and, as yet, unknown errors in the case of the latter - note that since there is a single error term, \\(\\epsilon_i\\), for each observation, \\(Y_i\\), the errors will always form a vector of length \\(n\\). The residuals (which estimate the errors) can also be collected in a vector of length \\(n\\), but we only know the values of the residuals after model fitting and their values will be different under each model. Both \\(\\mathrm{X}\\) and \\(\\underline{\\beta}\\) do change, with the addition of a column and row respectively for each additional covariate. 1.3 Parameter estimation Having set up the model, how do we obtain estimates for the unknown parameters? One approach is to use maximum likelihood. In order to set the scene, recall that our key assumptions are normality, zero mean, common variance and independence of the errors, i.e. \\(\\epsilon_i \\sim N(0, \\sigma_{\\epsilon}^2)\\) and \\(\\textrm{Cov}(\\epsilon_i, \\epsilon_j) = 0, i \\neq j\\). Since \\(Y_i = \\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip} + \\epsilon_i\\), then the normality assumption on \\(\\epsilon_i\\) induces a normal distribution on each \\(Y_i\\) (and, in turn, the vector \\(\\underline{Y}\\)). The values for the covariates are fixed, as are the true (usually unknown) values of \\(\\underline{\\beta}\\) so these just serve as constants when thinking about the distribution of \\(Y_i\\). Furthermore, the assumption of independence also carries through and the variance is unaltered (since we are simply adding scalars to a random variable). This tells us that \\(Y_i \\mid \\underline{x}_i, \\underline{\\beta}, \\sigma_{\\epsilon}^2 \\sim N(\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}, \\sigma_{\\epsilon}^2)\\) for \\(i = 1, \\ldots, n\\). This can be written more compactly as \\(Y_i \\mid \\underline{x}_i, \\underline{\\beta}, \\sigma_{\\epsilon}^2 \\sim N(\\underline{x}_i^T\\underline{\\beta}, \\sigma_{\\epsilon}^2)\\). Before going further, recall that the probability density function for a univariate normal random variable, \\(Y \\sim N(\\mu, \\sigma^2)\\), is: \\[ \\color{red}{f(y \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right\\}} \\] The likelihood principle instructs us to pick values of the parameters that maximise the likelihood. If observations are independent (as we have just shown they are here), then the likelihood function for all observations is a product of the individual normal densities for each observation of the form: \\[ \\color{red}{L(\\mu, \\sigma^2 \\mid y_1, \\ldots, y_n) = \\prod_{i=1}^n f(y_i \\mid \\mu, \\sigma^2).} \\] Now, for our multiple linear regression model, we have \\[\\begin{align*} \\color{red}{L(\\underline{\\beta}, \\sigma_{\\epsilon}^2 \\mid y_1, \\ldots, y_n, \\mathrm{X})} &amp;\\color{red}{= \\prod_{i=1}^n f(y_i \\mid \\underline{x}_i, \\underline{\\beta}, \\sigma_{\\epsilon}^2)} \\\\ &amp;\\color{red}{= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma_{\\epsilon}^2}} \\exp\\left\\{-\\frac{(y_i - \\underline{x}_i^T\\underline{\\beta})^2}{2\\sigma_{\\epsilon}^2}\\right\\}} \\\\ &amp;\\color{red}{= (2\\pi\\sigma_{\\epsilon}^2)^{-n/2} \\exp\\left\\{-\\frac{1}{2\\sigma_{\\epsilon}^2}\\sum_{i=1}^n (y_i - \\underline{x}_i^T\\underline{\\beta})^2 \\right\\}} \\\\ &amp;\\color{red}{= (2\\pi)^{-n/2} (\\sigma_{\\epsilon}^2)^{-n/2} \\exp\\left\\{-\\frac{1}{2\\sigma_{\\epsilon}^2}\\sum_{i=1}^n (y_i - \\underline{x}_i^T\\underline{\\beta})^2 \\right\\}} \\\\ \\end{align*}\\] 1.3.1 Estimation of \\(\\underline{\\beta}\\) To estimate the parameter vector, \\(\\underline{\\beta}\\) we consider the log-likelihood since this is typically easier to work with, being additive as opposed to multiplicative. The log-likelihood is given by \\[\\begin{equation*} \\color{red}{\\ell(\\underline{\\beta}, \\sigma_{\\epsilon}^2 \\mid y_1, \\ldots, y_n, \\mathrm{X}) = -\\frac{n}{2} \\ln(2\\pi) -\\frac{n}{2} \\ln (\\sigma_{\\epsilon}^2) - \\frac{1}{2\\sigma_{\\epsilon}^2} \\sum_{i=1}^n (y_i - \\underline{x}_i^T\\underline{\\beta})^2} \\end{equation*}\\] We could (partially) differentiate with respect to each element of \\(\\underline{\\beta}\\) and this would lead to a system of equations (with a lot of structure) known as the normal equations. However, it is easier to consider the matrix formulation of the model by noting that \\[ \\sum_{i=1}^n (y_i - \\underline{x}_i^T\\underline{\\beta})^2 = (\\underline{y} - \\mathrm{X}\\underline{\\beta})^T (\\underline{y} - \\mathrm{X}\\underline{\\beta}) \\] This is, in fact, the exact quantity that is minimised using the method of least squares, which can also be used to estimate the parameters for this model (and simple linear regression). Now, ignoring the constant terms (by treating \\(\\sigma_{\\epsilon}^2\\) as fixed at this stage), we wish to minimise \\[\\begin{align*} \\color{red}{(\\underline{y} - \\mathrm{X}\\underline{\\beta})^T (\\underline{y} - \\mathrm{X}\\underline{\\beta})} &amp;\\color{red}{= (\\underline{y}^T - \\underline{\\beta}^T\\mathrm{X}^T)(\\underline{y} - \\mathrm{X}\\underline{\\beta})} \\\\ &amp;\\color{red}{= \\underline{y}^T\\underline{y} - \\underline{y}^T\\mathrm{X}\\underline{\\beta} - \\underline{\\beta}^T\\mathrm{X}^T\\underline{y} + \\underline{\\beta}^T\\mathrm{X}^T \\mathrm{X}\\underline{\\beta}} \\end{align*}\\] By noting that \\(\\underline{y}^T\\mathrm{X}\\underline{\\beta}\\) is a scalar we can rewrite this as its transpose, i.e. \\(\\underline{y}^T\\mathrm{X}\\underline{\\beta} = \\underline{\\beta}^T\\mathrm{X}^T\\underline{y}\\). Hence \\[ \\color{red}{(\\underline{y} - \\mathrm{X}\\underline{\\beta})^T (\\underline{y} - \\mathrm{X}\\underline{\\beta}) = \\underline{y}^T\\underline{y} - 2\\underline{\\beta}^T\\mathrm{X}^T\\underline{y} + \\underline{\\beta}^T\\mathrm{X}^T \\mathrm{X}\\underline{\\beta}} \\] We can now differentiate to obtain \\[ \\color{red}{\\frac{\\partial \\ell}{\\partial \\underline{\\beta}} = - 2\\mathrm{X}^T\\underline{y} + 2\\mathrm{X}^T \\mathrm{X}\\underline{\\beta}} \\] Setting equal to zero and solving for \\(\\underline{\\beta}\\) leads to the solution \\[\\begin{equation} \\color{red}{\\underline{\\hat{\\beta}} = (\\mathrm{X}^T\\mathrm{X})^{-1}\\mathrm{X}^T\\underline{y}} \\end{equation}\\] This estimate exists as long as the inverse exists, i.e. no column of \\(\\mathrm{X}\\) is a linear combination of other columns, i.e. there is no multicollinearity. We have to be careful that no columns of \\(\\mathrm{X}\\) are linearly related as this can be harder to detect and leads to serious issues; we will discuss multicollinearity in more detail in section 1.7. 1.3.2 Estimation of \\(\\sigma_{\\epsilon}^2\\) We can estimate \\(\\sigma_{\\epsilon}^2\\) in a similar fashion (now treating \\(\\underline{\\beta}\\) as fixed), using maximum likelihood once more. Recall that \\[\\begin{equation*} \\ell(\\underline{\\beta}, \\sigma_{\\epsilon}^2 \\mid y_1, \\ldots, y_n, \\mathrm{X}) = -\\frac{n}{2} \\ln(2\\pi) -\\frac{n}{2} \\ln (\\sigma_{\\epsilon}^2) - \\frac{1}{2\\sigma_{\\epsilon}^2} \\sum_{i=1}^n (y_i - \\underline{x}_i^T\\underline{\\beta})^2. \\end{equation*}\\] For ease of calculation we let \\(\\tau = \\sigma_{\\epsilon}^2\\) and then differentiating with respect to \\(\\tau\\) we obtain \\[\\begin{equation*} \\color{red}{\\frac{\\partial \\ell}{\\partial\\tau} = -\\frac{n}{2\\tau} + \\frac{\\sum (y_i - \\underline{x}_i^T\\underline{\\beta})^2}{2\\tau^2}} \\end{equation*}\\] Setting the above equal to zero and solving for \\(\\tau\\) we obtain \\[\\begin{equation} \\color{red}{\\hat{\\tau} = \\frac{\\sum (y_i - \\underline{x}_i^T\\underline{\\beta})^2}{n} = \\frac{\\sum (y_i - \\hat{y_i})^2}{n}.} \\end{equation}\\] However, this is a biased estimate (akin to the sample variance bias problem), so we adjust for the fact that we have estimated the \\(p\\)-vector \\(\\underline{\\beta}\\) by using \\[ \\color{red}{\\hat{\\sigma}_{\\epsilon}^2 = s^2 = \\frac{\\sum (y_i - \\hat{y_i})^2}{n - p - 1} } \\tag{1.2} \\] where \\(p\\) is the number of explanatory variables in the model. Note that \\(\\underline{\\beta}\\) has length \\(k = p+1\\) typically, with the additional intercept term. 1.3.3 Residuals, fitted values and the ‘hat matrix’ The vector of residuals (which estimate the errors) can be obtained by subtraction after model fitting, namely as ‘observed - fitted’ \\[\\begin{align*} \\color{red}{\\underline{\\hat{\\epsilon}}} &amp;\\color{red}{= \\underline{y} - \\underline{\\hat{y}}} \\\\ &amp;\\color{red}{= \\underline{y} - \\mathrm{X}\\underline{\\hat{\\beta}},} \\end{align*}\\] where the fitted values are found as \\(\\underline{\\hat{y}} = \\mathrm{X}\\underline{\\hat{\\beta}}\\). The hat matrix We can rewrite the estimate for the errors by substituting in \\(\\underline{\\hat{\\beta}} = (\\mathrm{X}^T\\mathrm{X})^{-1}\\mathrm{X}^T\\underline{y}\\) to obtain \\[\\begin{align*} \\color{red}{\\underline{\\hat{\\epsilon}}} &amp;\\color{red}{= \\underline{y} - \\mathrm{X}(\\mathrm{X}^T\\mathrm{X})^{-1}\\mathrm{X}^T\\underline{y}} \\\\ &amp;\\color{red}{= (\\mathrm{I} - \\mathrm{X}(\\mathrm{X}^T\\mathrm{X})^{-1}\\mathrm{X}^T) \\underline{y}} \\\\ &amp;\\color{red}{= (\\mathrm{I} - \\mathrm{H})\\underline{y},} \\end{align*}\\] where \\(\\mathrm{H} = \\mathrm{X}(\\mathrm{X}^T\\mathrm{X})^{-1}\\mathrm{X}^T\\). \\(\\mathrm{H}\\) is known as the ‘hat’ matrix since \\[\\begin{align*} \\color{red}{\\underline{\\hat{y}}} &amp;\\color{red}{= \\mathrm{X}\\underline{\\hat{\\beta}}} \\\\ &amp;\\color{red}{= \\mathrm{X}(\\mathrm{X}^T\\mathrm{X})^{-1}\\mathrm{X}^T\\underline{y}} \\\\ &amp;\\color{red}{= \\mathrm{H}\\underline{y}.} \\end{align*}\\] Hence, multiplying by \\(\\mathrm{H}\\) converts \\(\\underline{y}\\) to \\(\\underline{\\hat{y}}\\), i.e. it is the matrix that puts a hat on \\(\\underline{y}\\). The hat matrix is an \\(n \\times n\\) matrix with elements \\[\\begin{align*} \\color{red}{\\mathrm{H} = \\begin{pmatrix} h_{11}&amp; h_{12}&amp; \\ldots&amp; h_{1n} \\\\ h_{21}&amp; h_{22}&amp; \\ldots&amp; h_{2n} \\\\ \\vdots&amp; \\vdots&amp; &amp; \\vdots \\\\ h_{n1}&amp; h_{n2}&amp; \\ldots&amp; h_{nn} \\\\ \\end{pmatrix}} \\;\\;\\;\\; \\end{align*}\\] The diagonal values of \\(\\mathrm{H}\\) (i.e. the \\(h_{ii}\\) values for \\(i = 1, \\ldots, n\\)) are called the leverages (see chapter 2). 1.3.4 Properties of the hat matrix It turns out that the hat matrix, \\(\\mathrm{H}\\), has some useful properties, which will prove to be handy later. Namely, \\(\\mathrm{H}\\) is symmetric, whereby \\(\\mathrm{H}^T = \\mathrm{H}\\), \\(\\mathrm{H}\\) is idempotent, i.e \\(\\mathrm{H}^2 = \\mathrm{H}\\mathrm{H} = \\mathrm{H}\\). Proof Now \\[\\begin{align*} \\color{red}{\\mathrm{H}^T} &amp;\\color{red}{=\\Bigl(\\mathrm{X} (\\mathrm{X}^T\\mathrm{X})^{-1} \\mathrm{X}^T\\Bigr)^T} \\\\ &amp;\\color{red}{= \\mathrm{X}\\left(\\mathrm{X}^T\\mathrm{X})^{-1}\\right)^T \\mathrm{X}^T} \\\\ &amp;\\color{red}{= \\mathrm{X}\\left(\\mathrm{X}^T\\mathrm{X})^T\\right)^{-1} \\mathrm{X}^T} \\\\ &amp;\\color{red}{= \\mathrm{X} (\\mathrm{X}^T\\mathrm{X})^{-1} \\mathrm{X}^T} \\\\ &amp;\\color{red}{= \\mathrm{H}.} \\\\ \\end{align*}\\] We now have \\[\\begin{align*} \\color{red}{\\mathrm{H}\\mathrm{H}} &amp;\\color{red}{= \\Bigl(\\mathrm{X} (\\mathrm{X}^T\\mathrm{X})^{-1} \\mathrm{X}^T\\Bigr) \\Bigl(\\mathrm{X} (\\mathrm{X}^T\\mathrm{X})^{-1} \\mathrm{X}^T\\Bigr)} \\\\ &amp;\\color{red}{= \\mathrm{X} (\\mathrm{X}^T\\mathrm{X})^{-1} \\mathrm{X}^T\\mathrm{X} (\\mathrm{X}^T\\mathrm{X})^{-1} \\mathrm{X}^T} \\\\ &amp;\\color{red}{= \\mathrm{X} (\\mathrm{X}^T\\mathrm{X})^{-1} \\mathrm{X}^T} \\\\ &amp;\\color{red}{= \\mathrm{H}.} \\\\ \\end{align*}\\] Example: Multiple linear regresion analysis of bodyweight data We are now in a position to estimate the parameters for the data on pre-diabetes patients introduced earlier. First we recall that for the multiple linear regression model for the bodyweight data we have the design matrix given by: \\[\\begin{align*} \\mathrm{X} = \\begin{pmatrix} 1 &amp; 2680&amp; 3 \\\\ 1 &amp; 3280&amp; 1 \\\\ \\vdots&amp; \\vdots&amp; \\vdots \\\\ 1 &amp; 2820&amp; 1 \\\\ \\end{pmatrix} \\end{align*}\\] Using matrix algebra we can calculate \\[\\begin{align*} \\mathrm{X}^T\\mathrm{X} &amp;= \\begin{pmatrix} 24 &amp; 71560 &amp; 19 \\\\ 71560 &amp; 216577400 &amp; 55380\\\\ 19 &amp; 55380 &amp; 35\\\\ \\end{pmatrix} \\\\ \\mathrm{X}^T\\underline{y} &amp;= \\begin{pmatrix} 1837.50 \\\\ 5570449 \\\\ 1354.60 \\end{pmatrix}. \\end{align*}\\] Taking the (3 by 3) matrix inverse we get \\[\\begin{align*} (\\mathrm{X}^T\\mathrm{X})^{-1} = \\begin{pmatrix} 3.02 &amp; -9.69 \\times 10^{-4} &amp; -0.10 \\\\ -9.69 \\times 10^{-4} &amp; 3.20 \\times 10^{-7} &amp; 2.04 \\times 10^{-5} \\\\ -0.10 &amp; 2.04 \\times 10^{-5} &amp; 0.05 \\\\ \\end{pmatrix} \\end{align*}\\] Hence, the parameter estimates can be found as \\[\\begin{eqnarray*} \\color{red}{ \\underline{\\beta} = (\\mathrm{X}^T\\mathrm{X})^{-1}\\mathrm{X}^T\\underline{y} = (-2.104, 0.027, -3.278)^T.} \\end{eqnarray*}\\] Note that we can also calculate \\(\\underline{\\hat{y}} = \\mathrm{X}\\underline{\\hat{\\beta}}\\) from the above, and subsequently \\(\\underline{\\hat{\\varepsilon}} = \\underline{y} - \\underline{\\hat{y}}\\). The fitted line for the multiple linear regression model is \\[ \\color{red}{\\textrm{Weight} = -2.014 + 0.027\\times \\textrm{Consumption} -3.278\\times\\textrm{Exercise}.} \\] This can be interpreted in a similar way to simple linear regression, but with a few caveats: body weight goes up by 0.027 kg for every additional calorie consumed, . body weight decreases by around 3.3 kg as individuals move up an exercise category, . be careful not to interpret the above as exercise being ‘more important’ than consumption due to having a larger coefficient - the scales of the variables are different and we also have no idea (yet!) whether these values are significant. It may be more meaningful to express the change due to consumption in different units. Also, note that the values of the parameter estimates change with the introduction (or removal) of variables into (from) the model - this is always the case (unless the covariates are independent), no matter how significant (or not) they are. This is important when building a regression model. We have seen that fitting a multiple linear regression model with two covariates can be achieved `by hand’. However, it is clear that as we look to build more complex models then it may be advantageous to use software - we will see how to do this in section 1.5. 1.4 Expectations, variances and inference We now consider the properties of the estimators \\(\\underline{\\hat{\\beta}}\\) and \\(\\underline{\\hat{\\epsilon}}\\), i.e. is \\(\\underline{\\hat{\\beta}}\\) unbiased? This will allow us, among other things, to assess the significance (or otherwise) of the parameter estimates. We begin by considering the expectation and variance of \\(\\underline{\\hat{\\beta}}\\). 1.4.1 Expectation of \\(\\underline{\\hat{\\beta}}\\) Now, \\[\\begin{align*} \\color{red}{\\mathrm{E}\\left[\\underline{\\hat{\\beta}}\\right]} &amp;\\color{red}{= \\mathrm{E}\\left[\\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}\\mathrm{X}^T\\underline{Y}\\right]} \\\\ &amp;\\color{red}{= \\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}\\mathrm{X}^T \\mathrm{E}[\\underline{Y}]} \\\\ &amp;\\color{red}{= \\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}\\mathrm{X}^T \\mathrm{X}\\underline{\\beta}} \\\\ &amp;\\color{red}{= \\underline{\\beta}} \\end{align*}\\] Hence, \\(\\underline{\\hat{\\beta}}\\) is an unbiased estimator of \\(\\underline{\\beta}\\). 1.4.2 Variance of \\(\\underline{\\hat{\\beta}}\\) Before looking at the variance in detail we note \\[ \\left\\{\\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}\\mathrm{X}^T\\right\\}^T = \\mathrm{X}\\left\\{\\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}\\right\\}^T = \\mathrm{X}\\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1} \\] since \\((\\mathrm{X}^T\\mathrm{X})^{-1}\\) is a symmetric matrix. We are now in a position to look at the variance \\[\\begin{align*} \\color{red}{\\mathrm{Var}\\left[\\underline{\\hat{\\beta}}\\right]} &amp;\\color{red}{= \\mathrm{Var}\\left[\\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}\\mathrm{X}^T\\underline{Y}\\right]} \\\\ &amp;\\color{red}{= \\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}\\mathrm{X}^T \\mathrm{Var}\\left[\\underline{Y}\\right] \\left\\{\\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}\\mathrm{X}^T\\right\\}^T} \\\\ &amp;\\color{red}{= \\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}\\mathrm{X}^T \\mathrm{I}\\sigma_{\\epsilon}^2 \\mathrm{X}\\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}} \\\\ &amp;\\color{red}{= \\sigma_{\\epsilon}^2 \\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}\\mathrm{X}^T \\mathrm{X}\\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}} \\\\ &amp;\\color{red}{= \\sigma_{\\epsilon}^2 \\left(\\mathrm{X}^T\\mathrm{X}\\right)^{-1}} \\end{align*}\\] 1.4.3 Inference for \\(\\underline{\\hat{\\beta}}\\) Since \\(\\underline{\\hat{\\beta}}\\) consists of linear combinations of the \\(Y_i\\)’s, which are independent and normally distributed, it has a multivariate normal distribution, namely \\(\\underline{\\hat{\\beta}} \\sim N_{p+1}\\left(\\underline{\\beta}, \\sigma_{\\epsilon}^2 (\\mathrm{X}^T\\mathrm{X})^{-1}\\right)\\) and each of the individual parameter estimates are univariate normal (due to properties of the multivariate normal distribution). Their (individual) significance can be asssessed via the test statistic \\[ \\color{red}{\\hat{\\beta_j}\\bigg/\\sqrt{v_{jj}s^2} \\sim t_{n-p-1}} \\] where \\(v_{jj}\\) is the \\((j+1)^{th}\\) diagonal element of \\(\\mathrm{V} = (\\mathrm{X}^T\\mathrm{X})^{-1}\\), and \\(s^2\\) is our (unbiased) estimate of \\(\\sigma_{\\epsilon}^2\\) from equation (1.2). Note the use of the \\(t\\)-distribution since we must also estimate \\(s^2\\). 1.4.4 Expectation and variance of the fitted values The fitted values are calculated as \\[ \\underline{\\hat{Y}} = \\mathrm{X}\\underline{\\hat{\\beta}} \\] or equivalently as \\[ \\underline{\\hat{Y}} = \\mathrm{H}\\underline{Y}. \\] Their expectation is \\[\\begin{align*} \\color{red}{\\mathrm{E}\\left[\\underline{\\hat{Y}}\\right]} &amp;\\color{red}{= \\mathrm{E}\\left[\\mathrm{X}\\underline{\\hat{\\beta}}\\right]} \\\\ &amp;\\color{red}{=\\mathrm{X} \\mathrm{E}[\\underline{\\hat{\\beta}}]} \\\\ &amp;\\color{red}{=\\mathrm{X}\\underline{\\beta}}, \\end{align*}\\] with variance given by \\[\\begin{align*} \\color{red}{\\mathrm{Var}\\left[\\underline{\\hat{Y}}\\right]} &amp;\\color{red}{= \\mathrm{Var}\\left[\\mathrm{H}\\underline{Y}\\right]} \\\\ &amp;\\color{red}{= \\mathrm{H} \\mathrm{Var}\\left[\\underline{Y}\\right] \\mathrm{H}^T} \\\\ &amp;\\color{red}{= \\mathrm{H} \\mathrm{I}\\sigma_{\\epsilon}^2 \\mathrm{H}^T} \\\\ &amp;\\color{red}{= \\mathrm{H}\\mathrm{H}\\sigma_{\\epsilon}^2} \\\\ &amp;\\color{red}{= \\mathrm{H} \\sigma_{\\epsilon}^2} \\end{align*}\\] Hence, the variability of the fitted values depends on the hat matrix, \\(\\mathrm{H}\\). We will discuss this further in chapter 2. 1.4.5 Expectation and variance of the residuals Recall that the residuals are found as \\[ \\color{red}{\\underline{\\hat{\\epsilon}} = \\underline{Y} - \\underline{\\hat{Y}}.} \\] or alternatively as \\[ \\color{red}{\\underline{\\hat{\\epsilon}} = (\\mathrm{I} - \\mathrm{H})\\underline{Y}.} \\] We can find the expectation and variance as \\[\\begin{align*} \\color{red}{\\mathrm{E}[\\underline{\\hat{\\epsilon}}]} &amp; \\color{red}{= \\mathrm{E}[\\underline{Y} - \\underline{\\hat{Y}}]} \\\\ &amp;\\color{red}{= \\mathrm{E}[\\mathrm{X}\\underline{\\beta}] - \\mathrm{E}\\left[\\mathrm{X}\\underline{\\hat{\\beta}}\\right]} \\\\ &amp;\\color{red}{= \\mathrm{X}\\underline{\\beta} - \\mathrm{X}\\underline{\\beta}} \\\\ &amp;\\color{red}{= \\underline{0},} \\end{align*}\\] and \\[\\begin{align*} \\color{red}{\\mathrm{Var}[\\underline{\\hat{\\epsilon}}]} &amp;\\color{red}{= \\mathrm{Var}[(\\mathrm{I} - \\mathrm{H})\\underline{Y}]} \\\\ &amp;\\color{red}{= (\\mathrm{I} - \\mathrm{H})\\mathrm{Var}[\\underline{Y}](\\mathrm{I} - \\mathrm{H})^T} \\\\ &amp;\\color{red}{= (\\mathrm{I} - \\mathrm{H})\\sigma_{\\epsilon}^2\\mathrm{I}(\\mathrm{I} - \\mathrm{H})} \\\\ &amp;\\color{red}{= \\sigma_{\\epsilon}^2(\\mathrm{I} - \\mathrm{H} - \\mathrm{H} + \\mathrm{H}\\mathrm{H})} \\\\ &amp;\\color{red}{= \\sigma_{\\epsilon}^2(\\mathrm{I} - \\mathrm{H}).} \\end{align*}\\] Note that this implies that, unless all the diagonal values of \\(\\mathrm{H}\\) are equal then the errors have different variances, and that these variances are smaller for larger values of \\(h_{ii}\\), i.e. higher leverages (see chapter 2). 1.5 Multiple linear regression in R Once we start to think about large datasets and a large number of parameters, finding the parameter estimates by hand becomes laborious, not to mention the possibility of both data entry and/or numerical errors occurring increases greatly. Happily, we can use R to conduct the analyses instead. 1.5.1 Using data in R There are various ways of using data with R. Data can be read in manually, i.e. ## Type the data in the console bodyweight = c(60.4, 81.1, 94.9, 86.4, 90.3, 60.4, 77.8, 85.0, 71.6, 64.6, 75.1, 89.6, 84.4, 93.0, 61.3, 74.9, 94.3, 61.8, 78.1, 74.8, 59.0, 69.2, 67.1, 82.4) The majority of the time, in this module and the wider world, the (external) data in the file ExternalData.RData will be read/loaded directly into R, e.g. ## Load in an external dataset load(&quot;ExternalData.RData&quot;) Alternatively, we may sometimes make use of datasets that are internal to R in that they are part of an R package, i.e. for the dataset InternalRDataset: ## Load in an internal dataset data(InternalRDataset) To view the available datasets in R we can type data() at the console, or, for datasets attached to a particular package we can use data(library = \"Rpackage\"). Example: Analysis of bodyweight data using R To use R for the plots and analysis seen earlier: ## Load the data load(&quot;bodyweight.RData&quot;) ## Plots # Weight versus consumption plot(Weight ~ Consumption, data = bodyweight, pch = 16) # Weight versus exercise plot(Weight ~ Exercise, data = bodyweight, pch = 16) ## Analysis # Simple linear regression on consumption fit1 &lt;- lm(Weight ~ Consumption, data = bodyweight) # Multiple linear regression on consumption &amp; exercise fit2 &lt;- lm(Weight ~ Consumption + Exercise, data = bodyweight) We can inspect a model fit using various commands The summary() command gives an overview of the fit summary(fit2) ## ## Call: ## lm(formula = Weight ~ Consumption + Exercise, data = bodyweight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.5751 -2.5704 -0.7894 2.4049 10.9266 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.104925 7.017861 -0.300 0.76717 ## Consumption 0.027254 0.002286 11.921 8.23e-11 *** ## Exercise -3.278296 0.916795 -3.576 0.00178 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.044 on 21 degrees of freedom ## Multiple R-squared: 0.8916, Adjusted R-squared: 0.8813 ## F-statistic: 86.4 on 2 and 21 DF, p-value: 7.346e-11 We will consider output of this nature in detail later in the module. The fitted values and residuals can also be extracted (output - to three decimal places - is suppressed here) round(fitted.values(fit2), 3) round(residuals(fit2), 3) The variance-covariance matrix for \\(\\underline{\\hat{\\beta}}\\) is also contained within the fit. For the second model fit we get vcov(fit2) ## (Intercept) Consumption Exercise ## (Intercept) 49.2503765 -1.584890e-02 -1.6584367474 ## Consumption -0.0158489 5.227021e-06 0.0003330453 ## Exercise -1.6584367 3.330453e-04 0.8405137595 The hat-values that make up the diagonal of the \\(\\mathrm{H}\\) matrix can also be found - again we round to three decimal places: round(hatvalues(fit2), 3) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 ## 0.294 0.075 0.425 0.079 0.101 0.132 0.095 0.086 0.106 0.087 0.080 0.124 0.078 ## 14 15 16 17 18 19 20 21 22 23 24 ## 0.101 0.210 0.073 0.114 0.128 0.074 0.045 0.197 0.117 0.127 0.051 We can also add a fitted regression line to a scatterplot via the abline() command: plot(Weight ~ Consumption, data = bodyweight, pch = 16) abline(fit1, lty = 2) 1.6 The role of the intercept The intercept, via the parameter \\(\\beta_0\\), is included as a matter of course when fitting a regression model (the default behaviour in R is to have an intercept present in a model). Why is this the case? What would happen if we removed the intercept? Suppose we thought that we should fit the model without an intercept, then the multiple linear regression model takes the form \\[ \\color{red}{Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots \\beta_p x_{ip} + \\epsilon_i \\tag{1.3}} \\] for \\(i = 1, \\ldots, n\\). Or, equivalently, in matrix notation \\[\\begin{equation} \\color{red}{\\underline{Y} = \\mathrm{\\tilde{X}}\\underline{\\beta} + \\underline{\\epsilon}.} \\end{equation}\\] where \\(\\mathrm{\\tilde{X}}\\) represents the design matrix that does not now have a first column of ’1’s. Example: Analysis of bodyweight data without an intercept term Returning to our example on pre-diabetes we would have \\[\\begin{align*} \\color{red}{\\underline{Y} = \\begin{pmatrix} 60.4 \\\\ 81.1 \\\\ \\vdots \\\\ 82.4 \\\\ \\end{pmatrix}, \\mathrm{\\tilde{X}} = \\begin{pmatrix} 2680&amp; 3 \\\\ 3280&amp; 1 \\\\ \\vdots&amp; \\vdots \\\\ 2820&amp; 1 \\\\ \\end{pmatrix}, \\underline{\\beta} = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\end{pmatrix}, \\underline{\\epsilon} = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{24} \\\\ \\end{pmatrix}.} \\end{align*}\\] Note that \\(\\underline{Y}\\) and \\(\\underline{\\epsilon}\\) are unchanged, whereas both the design matrix and \\(\\underline{\\beta}\\) are affected by the removal of the intercept term. Upon fitting we would obtain the fitted model \\[ \\textrm{Weight} = \\hat{\\beta}_1\\times \\textrm{Consumption} + \\hat{\\beta}_2\\times\\textrm{Exercise}. \\] This model - and the equivalent model with an intercept term - assumes the relationship between weight and consumption remains the same for all values of calorific consumption and exercise. Moreover, the model without the intercept further assumes that zero calorie intake and zero exercise gives zero body weight! This may well not be true (or possible), not just here but for many datasets. Forcing a zero intercept can give nonsensical values for the predicted response and it can also severely affect the fit of the regression line, particularly if our estimate of the intercept is significantly different from zero. In the absence of an intercept term, the line of best fit is forced to go through the origin. We will now investigate further with another example. Example: Analysis of men’s Premier League football data - the role of the intercept The data in the following example comes from the 2012-13 men’s English Premier League final football table (on Canvas in the file prem.RData). For each team the number of points they achieved (the response - why?), goals they scored, conceded, and their goal difference (scored - conceded) are recorded, alongside how many times they did not concede a goal (a ‘clean sheet’), which will be our primary focus for now. A snapshot of the data are given below: load(&quot;prem.RData&quot;) kable(head(prem, 5)) Position Team Scored Conceded GoalDifference Points CleanSheets 1 Manchester United 86 43 43 89 13 2 Manchester City 66 34 32 78 18 3 Chelsea 75 39 36 75 14 4 Arsenal 72 37 35 73 14 5 Tottenham Hotspur 66 46 20 72 9 Below is a scatterplot of points against clean sheets: Figure 1.2: Scatterplot of points against clean sheets for the Premier League 2012/13 data. Fit a simple linear regression model with clean sheets as the sole covariate. Overlay the regression line on the scatterplot of the raw data and comment. fitprem1 &lt;- lm(Points ~ CleanSheets, data = prem) plot(Points ~ CleanSheets, data = prem, pch = 16, xlim = c(0, 25), ylim = c(0, 100)) abline(fitprem1, lty = 2, lwd = 1.5, col = &quot;red&quot;) Figure 1.3: Scatterplot of points against clean sheets for the Premier League 2012/13 data with overlaid model fit. Fit a second model, this time without an intercept and overlay this regression line. What do you observe? We can fit the second model and overlay the line using: fitprem2 &lt;- lm(Points ~ CleanSheets - 1, data = prem) abline(fitprem2, lty = 3, lwd = 1.5) Figure 1.4: Scatterplot of points against clean sheets for the Premier League 2012/13 data with two overlaid model fits. We can see that the model without the intercept has a different slope since \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are correlated (see MAS2902). Note also that the line of best fit for the model without the intercept is forced to go through the origin. Both models seem to do a reasonable job - assessing by eye - of capturing the relationship between points and clean sheets. This is not always the case though, as we will see in practical 1. 1.6.1 Interpretability of the intercept and extrapolation Having established that including an intercept is a sensible thing to do, we now move on to the question of its interpretation. Note that we did not formally interpret the intercept in our previous analysis of the bodyweight data, and this is common practice. However, if we did wish to say something meaningful about the intercept how would we go about it? We first inspect the fit for our first model from the previous example: summary(fitprem1) ## ## Call: ## lm(formula = Points ~ CleanSheets, data = prem) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.6526 -7.9816 -0.7842 7.0974 26.8211 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.3368 8.0303 2.034 0.056915 . ## CleanSheets 3.5263 0.7544 4.674 0.000189 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.3 on 18 degrees of freedom ## Multiple R-squared: 0.5483, Adjusted R-squared: 0.5232 ## F-statistic: 21.85 on 1 and 18 DF, p-value: 0.0001888 Figure 1.5: Scatterplot of points against clean sheets for the Premier League 2012/13 data with overlaid model fit. We see that the estimate of the intercept, \\(\\hat{\\beta}_0\\), is 16.34. This tells us that when clean sheets takes the value zero, then we would expect a team to obtain around 16 points (as points is an integer we round). Here this makes some sense, since no clean sheets would mean a team concedes at least one goal in every match they play. Note, however, that the smallest observed value for this variable is five, so by using the value of zero we are extrapolating beyond the observed range of our data and this can be problematic. 1.6.2 Mean-centering of covariates Without any data manipulation prior to model fitting we have seen that the estimate for the intercept is interpreted as the value for the response when all of the covariates take the value zero. This, however, might be a scenario that is either not likely (i.e. a weight of zero kg for an adult), or not permissible (amount of a drug administered as part of a treatment) in the context of the data at hand. One solution to this issue is to scale the covariates via mean-centering. \\[ \\color{red}{\\underline{\\tilde{x}}^{(j)} = \\underline{x}^{(j)} - \\bar{x}^{(j)}} \\] where \\(j = 1, \\ldots, p\\), \\(\\underline{x}^{(j)} = (x_{1j}, x_{2j}, \\ldots, x_{nj})\\) is the vector of values for the \\(j^{th}\\) covariate and \\(\\bar{x}^{(j)}\\) is the sample mean for the \\(j^{th}\\) covariate, for example the mean of the exercise values in the bodyweight data. Note the distinction between \\(\\underline{x}^{(j)}\\) and \\(\\underline{x}_i\\) introduced earlier, which is the vector of values for each individual (or subject). The intercept has the same interpretation as above, namely the value of the response when the covariates are all simultaneously set to zero, i.e. \\(\\underline{x}_i = \\underline{0}\\). However, zero is now the mean value for each covariate, after mean-centering, so the intercept can also now be interpreted as the value of the response when each covariate is at its (own) average value. Furthermore, the value of the intercept turns out to be \\(\\bar{y}\\), the sample mean of the response vector. Recall that in simple linear regression \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\times \\bar{x} \\] and this will clearly reduce to \\(\\hat{\\beta}_0 = \\bar{y}\\) when \\(\\bar{x} = 0\\). This result generalises to the multiple linear regression case. This tends to give a more intuitive interpretation generally. Mean-centering also removes the correlation between \\(\\beta_0\\) and \\(\\beta_1, \\ldots \\beta_p\\). We will now see the effect of mean-centering in an example. Example: Mean-centering (men’s Premier League football data) Returning to the data from the men’s football Premier League. Below is a scatterplot of points against the raw (solid circles) and mean-centered (triangles) versions of our clean sheets covariate. Figure 1.6: Scatterplot of points against the observed clean sheets (solid circles) and their mean-centered counterpart (triangles). Fit a model using a mean-centered version of clean sheets. [Hint: use the scale command in R to perform the mean-centering]. We fit - and inspect - the model using the R commands CleanSheetsScaled &lt;- scale(prem$CleanSheets, scale = FALSE) fit_mean_centre &lt;- lm(Points ~ CleanSheetsScaled, data = prem) summary(fit_mean_centre) ## ## Call: ## lm(formula = Points ~ CleanSheetsScaled, data = prem) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.6526 -7.9816 -0.7842 7.0974 26.8211 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51.6000 2.7513 18.755 2.91e-13 *** ## CleanSheetsScaled 3.5263 0.7544 4.674 0.000189 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.3 on 18 degrees of freedom ## Multiple R-squared: 0.5483, Adjusted R-squared: 0.5232 ## F-statistic: 21.85 on 1 and 18 DF, p-value: 0.0001888 Overlay the lines of best fit for the models using the raw and mean-centered covariates. What do you notice? Figure 1.7: Scatterplot of points against the observed clean sheets (solid circles) and their mean-centered counterpart (triangles) with overlaid lines of best fit. From the summary, we see that the estimate of the slope is exactly the same as before, i.e. \\(\\hat{\\beta}_1 = 3.53\\) so the line has the same gradient, but the intercept is different. The intercept estimate is \\(\\hat{\\beta}_0 = 51.60\\) (recall, it was around 16 earlier) which suggests that a team with the number of clean sheets will obtain around 52 points (nearest integer, as before). This interpretation is cleaner than our earlier interpretation using the raw rather than mean-centered covariate. Although we have illustrated the role of the intercept using simple linear regression, the same ideas hold in the multiple linear regression model. We now return to the issue of multicollinearity. 1.7 Properties of \\((\\mathrm{X}^T\\mathrm{X})^{-1}\\): multicollinearity We saw earlier that both the estimator of \\(\\underline{\\beta}\\) and its variance both depend on the quantity \\(\\mathrm{X}^T\\mathrm{X}^{-1}\\). As such, this quantity plays a critical part in fitting a regression model and in determining the significance (or otherwise) of estimated parameters. We will now consider a situation known as multicollinearity that leads to problems with taking the inverse of \\(\\mathrm{X}^T\\mathrm{X}\\). Example: Multicollinearity in men’s Premier League football data Returning to the Premier League football data, a sports data analyst sets out to fit the following model: \\[ \\textrm{Points}_i = \\beta_0 + \\beta_1 \\textrm{Goal difference}_i + \\beta_2 \\textrm{Scored}_i + \\beta_3 \\textrm{Conceded}_i + \\epsilon_i \\] Construct the design matrix \\(\\mathrm{X}\\), and hence calculate \\(\\mathrm{X}^T\\mathrm{X}\\) and \\((\\mathrm{X}^T\\mathrm{X})^{-1}\\). X &lt;- cbind(1, prem$GoalDifference, prem$Scored, prem$Conceded) XTX &lt;- crossprod(X) solve(XTX) The last line fails, we cannot invert the matrix as it is singular. Can you spot an obvious problem with this model? The problem here is that one of the variables is a linear combination of the others, namely goal difference which is defined as ‘scored’ - ‘conceded’. This means they are collinear and that we will have problems inverting \\(\\mathrm{X}^T\\mathrm{X}\\). Fit the model in R and inspect the fit - what do you notice? Implementing the model we get summary(lm(Points ~ GoalDifference + Scored + Conceded, data = prem)) ## ## Call: ## lm(formula = Points ~ GoalDifference + Scored + Conceded, data = prem) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.0387 -2.7741 0.2508 3.7633 5.7760 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 38.3511 9.6368 3.980 0.000969 *** ## GoalDifference 0.5710 0.1097 5.206 7.13e-05 *** ## Scored 0.2493 0.1803 1.382 0.184780 ## Conceded NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.471 on 17 degrees of freedom ## Multiple R-squared: 0.9437, Adjusted R-squared: 0.937 ## F-statistic: 142.4 on 2 and 17 DF, p-value: 2.404e-11 There is no parameter estimate or standard error for goals conceded, which seems to have been removed from the model. Here it was clear what was driving the multicollinearity, and the issue could be easily spotted, and resolved. Sometimes, however, the problem is more subtle and we will consider this scenario further in practical 1. In the next chapter we will investigate whether our model conforms to assumptions and/or has any unusual observations that warrant further investigation. "],["regression-diagnostics.html", "2 Regression diagnostics 2.1 Standardised residuals 2.2 Regression diagnostics", " 2 Regression diagnostics Chapter 1 introduced the multiple linear regression model, how to estimate the parameters and some properties of the estimators, amongst other things. We now consider the situation where, having fitted a model (or several candidate models), we wish to assess the validity of the model(s). Recall that when defining the multiple linear regression model we made a series of assumptions, which, in turn, allowed us to use maximum likelihood to estimate the parameters. In this chapter we describe a suite of methods that can be used to check if our model conforms to the assumptions. Namely, we can consider whether the relationship between \\(Y\\) and the \\(x\\)-variables is linear; the errors are normally distributed; the errors are uncorrelated; the error term, \\(\\epsilon_i\\), has constant variance \\(\\sigma_{\\epsilon}^2\\); Additionally, we will investigate whether any points are unusual (outliers) have a large effect on the regression coefficients (via their leverage). are unduly influential (via their Cook’s distance); You may have encountered some of these before. In the previous chapter, we informally assessed the assumption of a linear relationship through a scatterplot, or a series of pairwise scatterplots when we have more than one covariate. However, when there are many covariates (and particularly if some of these are categorical or ordinal) this can become an unwieldy approach. Moreover, the underlying linear (or otherwise) relationship can be masked by the relationship of the response with the other variables; the residuals, however, are adjusted for this. As such, it is recommended to use residual checks to verify the functional form of the model, i.e. are the relationships linear? Happily, the residuals also allow us to check the remaining assumptions too, since they are our estimate of the true (but unknown) errors. 2.1 Standardised residuals Recall, from section 1.3.3, that the residual is the observed value minus the fitted value. Its definition and variance are given by \\[\\begin{align*} \\color{red}{\\underline{\\hat{\\epsilon}}} &amp;\\color{red}{= \\underline{y} - \\underline{\\hat{y}}} \\\\ &amp;\\color{red}{= (\\mathrm{I} - \\mathrm{H})\\underline{y}} \\end{align*}\\] and \\[ \\color{red}{\\mathrm{Var}\\left(\\underline{\\hat{\\epsilon}}\\right) = (\\mathrm{I} - \\mathrm{H})\\sigma_{\\epsilon}^2.} \\] We might also like to think about the residuals and fitted values at the observation level, particularly if we are looking at observations that are unusual. Based on the above we have \\[ \\color{red}{\\hat{\\epsilon}_i = y_i - \\hat{y}_i} \\] and \\[ \\color{red}{\\mathrm{Var}(\\hat{\\epsilon}_i) = (1 - h_{ii})\\sigma_{\\epsilon}^2} \\] respectively, for \\(i = 1, \\ldots, n\\). Note that this informs us that the variance is not the same for each residual, since the \\(h_{ii}\\) values will (typically) be different for each \\(i\\). Furthermore, the raw residuals are not scale invariant, so changing the unit of measurement would drastically affect the residuals, and the threshold for a large residual depends on the context at hand. For instance, on data with populations measured in millions we would expect large residuals even in a well-behaved model, whereas in water treatment, or air quality, measurements are often made which amount to microscopic quantities (typically measured in parts per million) that would lead to miniscule residuals, even if the underlying model is poor. To get around these issues, we work with the standardised residuals \\[ \\color{red}{\\hat{e}_i = \\frac{\\hat{\\epsilon}_i}{\\sqrt{(1 - h_{ii})s^2}}} \\] where \\(s^2\\) is our estimate of \\(\\sigma_{\\epsilon}^2\\) (see Chapter 1). The standardised residuals have a mean of zero (as do the raw residuals) since their sum is constrained to be zero - which induces dependence - and a variance of (approximately) one. Placing the residuals on a common scale also allows us to look for outliers or unusual observations more easily (see later). We can calculate the standardised residuals in R using the following command, where fit2 is our second fitted model from chapter 1: rstandard(fit2) ## 1 2 3 4 5 6 ## -0.20643768 -0.74827738 -0.80142866 0.54358256 0.85837659 -0.98421544 ## 7 8 9 10 11 12 ## 1.14666224 -0.09647632 -0.04022545 -0.22714152 -0.33189908 -0.46936793 ## 13 14 15 16 17 18 ## 0.09836234 1.13448521 1.56936911 -1.43201467 1.05443091 -0.82758724 ## 19 20 21 22 23 24 ## -1.03068909 -0.53747090 -1.26308890 0.19268170 -0.58264176 2.77358349 We use the terminology of standardised residuals in this module since this is what is adopted in R. You may sometimes see the phrase ‘(internally) studentised’ residuals elsewhere (in MAS2902 for instance) for this same concept. There is also, as you might expect, an externally studentised residual, but this is not explored further here. 2.1.1 Residual plots We typically use visual inspection (i.e. plots) to check the model assumptions since the raw values themselves are hard to interpret. We primarily plot \\(\\hat{e}_i\\) against the fitted values \\(\\hat{y_i}\\), and they can also be plotted against each of the explanatory variables in order to verify the functional form. If the model assumptions are satisfied, then, for each plot, the standardised residuals should be randomly scattered within a horizontal band: Otherwise, we could get signs of non-constant variance, i.e. increasing, double bow, decreasing, or signs of non-linearity (see practical 2). In practice, nonlinearity is hard to distinguish from correlation in the standardised residuals. Some examples of residuals not conforming to model assumptions are given above. Correlation between residuals and fitted values The reason we plot residuals against fitted values, rather than the observations themselves is because the residuals and fitted values are uncorrelated. This allows us to detect any unusual observations without worrying that these may be due to an underlying dependence between the quantities being plotted. Proof \\[\\begin{align*} \\color{red}{\\mathrm{Cov}(\\underline{e}, \\underline{\\hat{Y}})} &amp;\\color{red}{= \\mathrm{Cov}(\\underline{Y} - \\underline{\\hat{Y}}, \\underline{\\hat{Y}})} \\\\ &amp;\\color{red}{= \\mathrm{Cov}(\\left\\{\\mathrm{I} - \\mathrm{H}\\right\\}\\underline{Y}, \\underline{\\hat{Y}})} \\\\ &amp;\\color{red}{= \\mathrm{Cov}(\\left\\{\\mathrm{I} - \\mathrm{H}\\right\\}\\underline{Y}, \\mathrm{H}\\underline{Y})} \\\\ &amp;\\color{red}{= (\\mathrm{I} - \\mathrm{H})\\mathrm{Cov}(\\underline{Y}, \\underline{Y})\\mathrm{H}^T} \\\\ &amp;\\color{red}{= (\\mathrm{I} - \\mathrm{H})\\mathrm{Var}(\\underline{Y})\\mathrm{H}^T} \\\\ &amp;\\color{red}{= (\\mathrm{I} - \\mathrm{H})\\sigma_{\\epsilon}^2\\mathrm{I}\\mathrm{H}^T} \\\\ &amp;\\color{red}{= \\sigma_{\\epsilon}^2 (\\mathrm{I} - \\mathrm{H}) \\mathrm{H}^T} \\\\ &amp;\\color{red}{= \\sigma_{\\epsilon}^2 (\\mathrm{I} - \\mathrm{H})\\mathrm{H}} \\\\ &amp;\\color{red}{= \\sigma_{\\epsilon}^2(\\mathrm{H} - \\mathrm{H}^2)} \\\\ &amp;\\color{red}{= \\sigma_{\\epsilon}^2 (\\mathrm{H} - \\mathrm{H})} \\\\ &amp;\\color{red}{= \\underline{0}} \\end{align*}\\] This result also hold for the standardised residuals and the fitted values, but the proof is more convoluted; the residuals and the observed values, however, are not independent. 2.1.2 Outliers Outliers are points which appear separated in some way from the remainder of the data. Part of the purpose of our residual plots is to check for unusual observations. The standardised residuals give a good initial indicator of outliers. If we assume the (raw) residuals are normally distributed (we will check this assumption shortly), then the standardised residuals are also normal, albeit with a different variance, which is close to one (see above). Hence, the standardised residuals are assumed to be approximately standard normal, and this suggests that values outside the range \\(\\pm 2\\) are indicative of outliers. However, by normal distribution theory, we expect around \\(5\\%\\) of observations to be outside this range by chance - recall that \\(\\pm 1.96\\) cuts off \\(2.5\\%\\) of a standard normal distribution, i.e. \\(Pr(Z &gt; 1.96) = 0.025\\), where \\(Z \\sim N(0, 1)\\). This value is often rounded to \\(\\pm 2\\) to act as a simple rule-of-thumb for residual plots. Clearly, the larger the absolute value of the standardised residual the more likely it is to be an outlier. Note that it is not reasonable to remove outliers just because we don’t like the look of them! It is, of course, worth checking the original source of the data - if the outliers are a result of recording error, then this should be corrected. If the outliers appear genuine, we should see how discrepant they are. We can carry out an analysis with and without the suspect observations to investigate the sensitivity of the results. Example: Residual analysis for pre-diabetes data Residual plots for the pre-diabetes data can be generated in R using the following commands: Standardised residuals against covariates plot(bodyweight$Consumption, rstandard(fit2), xlab = &quot;Consumption&quot;, ylab =&quot;Standardised residuals&quot;) abline(h = c(-2, 0, 2), lty = 2) Figure 2.1: Scatterplot of standardised residuals against (average) food consumption for the pre-diabetes data. plot(bodyweight$Exercise, rstandard(fit2), xlab = &quot;Exercise&quot;, ylab =&quot;Standardised residuals&quot;) abline(h = c(-2, 0, 2), lty = 2) Figure 2.2: Scatterplot of standardised residuals against (average) food consumption for the pre-diabetes data. Standardised residuals against fitted values plot(fitted.values(fit2), rstandard(fit2), xlab = &quot;Fitted values&quot;, ylab =&quot;Standardised residuals&quot;) abline(h = c(-2, 0, 2), lty = 2) Figure 2.3: Scatterplot of standardised residuals against fitted values for the multiple linear regession model with consumption and exercise. In each of the above plots, we have added a dashed horizontal line at zero to help look for patterns, we would expect around half of the standardised residuals to lie both above and below the line in each case. Further dashed lines at \\(\\pm 2\\) allow us to identify how many points lie outside these bounds (recall, we expect \\(\\approx 5\\%\\) in a well-behaved model). To summarise these plots we can say: - there is no clear pattern in the residuals although there is a (mild) suggestion of curvature in the plot against exercise. - the variance does not appear to change in a systematic way. - there is one point with a standardised residual greater than 2 in modulus. Possible outlier? We expect about \\(5\\%\\) to be in this range, i.e. 1 in a sample size of 24. Thus there is no clear evidence of a departure from our assumptions. Note that when solely discrete (or ordinal) variable(s) are included in the model, the residual plot appears as strips of points at the observed values for the covariate. This is to be expected! We can still assess variability by checking whether the length of the strip is roughly the same and that it is centred around zero. 2.1.3 Normality of the residuals As mentioned earlier, analysis of the multiple linear regression model hinges on assumptions of normality (and independence) of the errors; you will consider models that deviate from this assumption in semester two. Hence, we need to check this assumption after fitting a model as the consequences of non-normality of the residuals are: the least squares estimates, \\(\\underline{\\hat{\\beta}}\\), may not be optimal; the associated tests and confidence intervals are inaccurate. However, it has been shown that only really long-tailed distributions cause a major problem and that mild non-normality can safely be ignored. Furthermore, the larger the sample size the more the the non-normality is the more the consequences are mitigated. To get the normality plot for the pre-diabetes data, we use the following commands: qqnorm(rstandard(fit2), ylab = &quot;Standardised residuals&quot;) abline(a = 0, b = 1, lty = 2) Figure 2.4: Normality plot of standardised residuals. As before, for the R code to work we must have previously defined the object fit2. Comment: The standardised residuals fit fairly well to a straight line but are being partially distorted by the potential outlier. Successive observations in the plot are not independent and so `ripples’ often occur by chance. 2.1.4 Anderson-Darling test Visual inspection of residual plots is a useful way to see where any departures from our assumptions may lie. However, it may also be good to have a summary measure via a formal statistical test to assess where there is a significant departure from normality. The most commonly used statistic is the Anderson-Darling (AD) statistic - R outputs the test statistic and a p-value when carrying out an AD test. The null hypotheses is that the residuals can reasonably be assumed to come from a normal distribution, with the alternative hypothesis stating the converse. As such, large p-values imply the normality assumption is fine, small p-values imply a significant departure from normality. Comment: Note that there are some issues with the AD test for large sample sizes as it is very sensitive to outliers. From above, we know that our model is reasonably robust to outliers so we should be careful not to over-interpret an AD test if the plots pass a visual check. Anderson-Darling test in R To carry out an AD test in R we first need to load the library nortest. For the pre-diabetes example: library(nortest) ad.test(rstandard(fit2)) ## ## Anderson-Darling normality test ## ## data: rstandard(fit2) ## A = 0.53928, p-value = 0.1494 As the p-value is fairly large in this case, we do not have a significant departure from normality and conclude that the assumption is not disputed for this model. A cautionary note One of the assumptions of the model is that the true (but unknown) errors, \\(\\underline{\\epsilon}\\), follow a normal distribution. We assess this using the estimated errors, i.e. the residuals. The assumed normality of the errors also induces normality on the response, but this is conditional on the values of the covariates, namely \\(Y_i \\mid \\underline{x}_i, \\underline{\\beta}, \\sigma_{\\epsilon}^2 \\sim N(\\underline{x}_i^T \\underline{\\beta}, \\sigma_{\\epsilon}^2)\\). As such we cannot judge the normality assumption on plots of the response variable alone - even though it may be tempting or even feel intuitive to do so - without taking into account the values of the covariates. This can be awkward to construct whereas a residual check is straightforward (and will show the same thing). Consider a multiple linear regression model with one continuous covariate (\\(x_1\\)) and one binary covariate (\\(x_2\\)), such as a treatment arm in a trial. If there are considerable differences between the two treatment groups then this will induce a bimodal distribution on the (unconditional) response variable, \\(\\underline{Y}\\): However, if we then fit a multiple linear regression model and look at the (standardised) residuals we see an approximate normal distribution Similarly, we can inspect quantile-quantile plots of both the response (below left) and the (standardised) residuals (below right). We observe that the upper plot does not conform to a straight line relationship whereas the right-hand one does. Note in passing that an AD test gives \\(p &lt; 0.001\\) (reject \\(H_0\\)) for the response variable and \\(p &gt; 0.10\\) (retain \\(H_0\\)) for the residuals. 2.2 Regression diagnostics As well as a raft of residual checks, two other metrics are commonly used to check the fit of a regression model - leverage and influence. We first consider leverage. 2.2.1 Leverage values An observation with an extreme value in the \\(x\\)-space (explanatory variables) is called a point with high leverage. Leverage is a measure of how far the explanatory variables deviate from their mean. Recall that the hat matrix, \\(\\mathrm{H}\\) is defined as \\[ \\color{red}{\\mathrm{H} = \\mathrm{X}(\\mathrm{X}^T\\mathrm{X})^{-1}\\mathrm{X}^T} \\] High leverage points can have an unusually large effect on the estimates of regression coefficients. To detect high leverage points we look for large values of \\(h_{ii}\\), the diagonal elements of \\(\\mathrm{H}\\), which are known as the leverages. Note that the leverage values depend on the covariates alone, and not on the response variable. If the value of \\(h_{ii}\\) is large, then \\(\\mathrm{Var}[\\hat{\\epsilon}_i]\\) will be small, i.e. the fit will be close to \\(Y_i\\), since \\(\\mathrm{Var}[\\hat{\\epsilon}_i] = (1 - h_{ii})\\sigma_{\\epsilon}^2\\) and this will approach zero as the hat values get close to unity. This means that the regression line is `forced’ to fit well to points with a large leverage value so these points have the potential to severely alter the gradient of the regression line. A consequence of this is that the variance of \\(\\hat{Y_i}\\), which is given by \\(h_{ii}\\sigma_{\\epsilon}^2\\), will be at its largest for points of high leverage. The sketches below demonstrate this. Properties of the leverage values The leverages are bounded between 0 and 1, i.e. \\(0 \\leq h_{ii} \\leq 1\\) The sum of the leverages is equal to the number of parameters in the model (including the intercept), i.e. \\(\\Sigma_{i=1}^n h_{ii} = p+1\\) Derivation We will focus on the second property only. Recall that the trace of a (symmetric) matrix is the sum of the diagonal elements. Now, \\[\\begin{align*} \\color{red}{\\Sigma_{i=1}^n h_{ii}} &amp; \\color{red}{= \\mathrm{tr}(\\mathrm{H})} \\\\ &amp;\\color{red}{= \\mathrm{tr}\\left(\\mathrm{X}(\\mathrm{X^T X})^{-1}\\mathrm{X}^T\\right)} \\\\ &amp;\\color{red}{= \\mathrm{tr}\\left(\\mathrm{X}^T\\mathrm{X}(\\mathrm{X^T X})^{-1}\\right)} \\\\ &amp;\\color{red}{= \\mathrm{tr}(\\mathrm{I}_{p+1})} \\\\ &amp;\\color{red}{= p + 1.} \\end{align*}\\] Usually, a value of \\(h_{ii} &gt; 2(p + 1)/n\\) is regarded as indicating a point of high leverage, where \\(p\\) is the number of explanatory variables in the model. Note that the leverage values depend on the \\(x\\)-variables alone, and not on the response variable. Example: Leverage values for the pre-diabetes data As for the residual checks, we typically plot the leverage values to look for large values. This can be achieved in R via the following commands: plot(hatvalues(fit2), ylab =&quot;Leverages&quot;, pch = 16) abline(h = 2*3/24 , col = 2, lty = 2) Figure 2.5: Leverage values from mutliple linear regression model for pre-diabetes data. Note that \\(p = 2\\) here (and \\(n = 24\\)) since we have two explanatory variables: consumption and exercise. From the plot, we can identify that there are two points of high leverage here. For large datasets, it may be more prudent to use R to find out how many points exceed the threshold table(hatvalues(fit2) &gt; 2*3/24) ## ## FALSE TRUE ## 22 2 If any points are flagged we can identify them using levs &lt;- hatvalues(fit2) levs[levs &gt; 2*3/24] ## 1 3 ## 0.2942944 0.4251701 This tells us that it is points 1 and 3 in this case. 2.2.2 Influential observations Some points have more influence on the regression analysis than others. These are not necessarily clear outliers or points of high leverage, but are typically fairly large in both regards. One approach for detecting such observations is using Cook’s distance, which is defined by: \\[ \\color{red}{D_i = \\frac{1}{p+1}(\\hat{e}_i)^2 \\frac{h_{ii}}{1 - h_{ii}}} \\] for \\(i = 1, \\ldots, n\\), and \\(\\hat{e}_i\\) and \\(h_{ii}\\) are the standardised residual and leverage values respectively defined earlier. Although not clear from the formula, Cook’s distance is a measure of how much the fitted values in the model would change if the \\(i^{th}\\) data point was deleted. Large values of \\(D_i\\) indicate that the point has a large influence on the model. We can calculate and plot the Cook’s distances in R using: plot(cooks.distance(fit2), ylab = &quot;Cooks distance&quot;, pch = 16) Figure 2.6: Cook distances from mutliple linear regression model for pre-diabetes data. There is no agreed threshold to identify influential observations, unlike for points of high leverage. However, a value of 1 has been suggested but this is typically conservative. A pragmatic approach is to investigate any (groups of) observations that appear to have larger values than the others. In our example, taking a threshold of 0.10 appears sensible. cooks &lt;- cooks.distance(fit2) cooks[cooks &gt; 0.10] ## 3 15 21 24 ## 0.1583550 0.2177828 0.1302879 0.1374646 We see that point 15 has the largest Cook’s distance, then point 3, and then point 24. Point 24 was the point with the largest absolute value of the standardised residuals; 3 was the point with the highest leverage. Observation 15 has the second largest absolute value of the standardised residuals, and the third highest leverage. 2.2.3 Dealing with unusual observations What should we do with the influential and/or high leverage points? As a first pass, we should check that they have been entered correctly, either by ourselves or by a data clerk or external source, but this may not be possible. Visually, we can plot the data with them highlighted and check whether they stand out. Can they be explained? The following R code produces such a plot for our example, where we have highlighted points 3 (highest leverage), 15 (largest Cook’s distance) and 24 (large outlier) as X, Y and Z respectively. We also colour-code points by exercise (1 is black, 2 is red, 3 is green): (#fig:label_inf_plot1)Scatterplot for pre-diabetes data with unusual points identified. # Or in one command points(bodyweight[c(3, 15, 24), 2:1], pch = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;)) We see that the point with the highest leverage (point 3, X) has an unusually large value for consumption. Point 24 (Z) - the possible outlier - does not look that unusual, being central in the consumption range, albeit with a higher value for weight than we might expect. Point 15 (Y), with the largest Cook’s distance, has the lowest consumption and a high weight value for that group (the green points). We could remove the most influential point and re-fit the model, but it does not change the results much here (this is left as an exercise). Example: Model checking for the Premier League data We now return to Example 1.10 from chapter 1 and carry out the full suite of checks (i.e. outliers, points of high leverage/influence and a normality test) on the model for points scored using clean sheets as the single (mean-centered) covariate. To recap, the call to fit the model (after mean-centering the variable) was CleanSheetsScaled &lt;- scale(prem$CleanSheets, scale = FALSE) fit_mean_centre &lt;- lm(Points ~ CleanSheetsScaled, data = prem) A scatterplot of the original data, with the line of best fit superimposed, is included below, with each point labelled by final league position, from 1 (top) to 20 (bottom): Interpret the plot of the (standardised) residuals. Figure 2.7: Plot of standardised residuals against fitted values. - All but one of the residuals lies in \\((-2, 2)\\), and we would expect one by chance in this dataset of twenty observations. - There is no discernible pattern to the (standardised) residuals with random scatter within our horizontal band, with no evidence of nonlinearity or non constant variance. Assess the normality assumption of the residuals and interpret the output of the Anderson-Darling test. Figure 2.8: Normality plot of standardised residuals. ad.test(rstandard(fit_mean_centre)) # See Chapter 1 ## ## Anderson-Darling normality test ## ## data: rstandard(fit_mean_centre) ## A = 0.33253, p-value = 0.4807 - From the plot, the points lie close to the nominal 45 degree line. - Due to the correlation between (standardised) residuals we expect to see bumps' orripples’ and this does not invalidate the normality assumption. - For the AD test, the p-value is not significant at any conventional level so we retain the null hypothesis that the (standardised) residuals can be assumed to be normally distributed. Use regression diagnostics to identify any points of high leverage, or of high influence. Figure 2.9: Plot of leverages (left) and Cook distances (right) indexed by final league position. The leverage values are plotted on the left, and the Cook’s distances on the right. We can see one point above the threshold (dashed line) for the leverages (which is \\(2\\times 2/20 = 0.20\\) here). This corresponds to observation 2. On inspection of the original data, we see that this is the team (Manchester City) with the most clean sheets, so is large in \\(x\\) space, thus having the capacity to alter the regression line. For the influential points, we see that the largest is for observation 1. This point lies a considerable distance from the regression line, our model would expect fewer points based on this covariate value. Other covariates may be needed in the model. The next largest Cook’s distances (points 5 and 7) are not that much larger than the body of the points so we do not consider them further, apart from observing that they deviate the most from the fitted line (for point 5 it is above the line of best fit, for point 7 it is below). "],["inference-for-the-multiple-linear-regression-model.html", "3 Inference for the multiple linear regression model 3.1 Assessing the fit 3.2 The basic anova table Example: Cheddar cheese study 3.3 The extra sum of squares method Example: Extra sum of squares for cheese data Example: Warfarin study 3.4 The general extra sum of squares method Example: Extended extra sum of squares for cheese data Example: Anova for crime data based on summary information Solution 3.5 Inference on individual parameters Example: Inference on individual parameters - warfarin example 3.6 Confidence and prediction intervals for the fitted values Example: Confidence and prediction intervals for the cheese data 3.7 Polynomial models Example: Polynomial model", " 3 Inference for the multiple linear regression model 3.1 Assessing the fit We can now formulate and fit a multiple linear regression model, and carry out a residual check to verify assumptions. However, a model can pass these checks and not be a good fit in terms of how much uncertainty in the response is accounted for by the explanatory variables, and/or the model itself may need to be simplified. Ideally, we would like a simple measure of how well our model fits the data. In a simple linear regression model we could do this by eye, i.e. a scatterplot of the data with the line of best fit overlaid, or by looking at \\(R^2\\). If the points lie close to the line of best fit, and \\(R^2\\) is large, then we have some assurance about the model. However, this is less straightforward for multiple linear regression models, which may contain a mixture of continuous and categorical covariates, as we would have to look at multiple plots (and non-continuous covariates are less suited to this graphical approach), which might be misleading when we are primarily interested in the overall fit, i.e. the combined effect of all of the explanatory variables. Furthermore, in a simple linear regression model there is a direct equivalence between \\(R^2\\) and the sample correlation \\(r\\); this relationship does not extend to the multiple linear regression model. One alternative approach is to use techniques from analysis of variance (anova) - which we will see again in a different context in chapter 4. We begin by considering the basic regression line concept \\[ \\color{red}{\\text{(observed) data} = \\text{fit} + \\text{residual}} \\] For a fitted multiple linear regression model this amounts to \\[\\begin{align*} \\color{red}{Y_i} &amp; \\color{red}{ = \\hat{Y_i} + \\hat{\\epsilon_i}} \\\\ &amp;\\color{red}{= \\hat{Y_i} + (Y_i - \\hat{Y_i})} \\end{align*}\\] for \\(i = 1, \\ldots, n\\). To map this on to an anova-type problem, we can take the - seemingly artificial - step of subtracting the observed data mean, \\(\\bar{Y}\\), from both sides to obtain \\[ \\color{red}{Y_i - \\bar{Y} = \\hat{Y_i} + (Y_i - \\hat{Y_i}) - \\bar{Y}} \\] which can be rearranged as \\[ \\color{red}{(Y_i - \\bar{Y}) = (\\hat{Y_i} - \\bar{Y}) + (Y_i - \\hat{Y_i}).} \\] The first term captures the difference between an observed data point and the mean response, the second term captures the difference between a fitted data point and the mean response and the third term is the usual (unaltered) definition of the \\(i^{th}\\) (raw) residual. By squaring each observation and taking the sum across each of the \\(n\\) observations we obtain \\[ \\color{red}{\\sum_{i=1}^n(Y_i - \\bar{Y})^2 = \\sum_{i=1}^n\\left\\{(\\hat{Y_i} - \\bar{Y}) + (Y_i - \\hat{Y_i}) \\right\\}^2} \\] This remarkably leads to the following result \\[\\begin{equation} \\color{red}{\\sum_{i=1}^n(Y_i - \\bar{Y})^2} \\color{red}{= \\sum_{i=1}^n(\\hat{Y}_i - \\bar{Y})^2 + \\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2} \\tag{3.1} \\end{equation}\\] since \\[ \\color{red}{\\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})(Y_i - \\hat{Y}_i) = 0.} \\] 3.2 The basic anova table Equation (3.1) can also be stated as \\[ \\color{red}{\\text{Total SS (TSS)} = \\text{Regression SS} + \\text{Residual SS}} \\] where \\(\\text{SS}\\) refers to the sum of squares. Under an anova approach, results are typically presented and summarised in table form: Source Degrees of freedom (df) Sum of squares (SS) Mean square (MS) Mean square ratio (MSR) Regression \\(\\color{red}{p}\\) Reg SS \\(\\color{red}{\\text{Reg MS}\\, = \\frac{\\text{Reg SS}}{p}}\\) \\(\\color{red}{F = \\frac{\\text{Reg MS}}{\\text{RMS}}}\\) Residual \\(\\color{red}{n - p - 1}\\) RSS \\(\\color{red}{\\text{RMS} = \\frac{\\text{RSS}}{n - p - 1}}\\) Total \\(\\color{red}{n - 1}\\) TSS So the regression model can be reduced to a single summary measure, \\(F\\), that can be used to test the overall significance of the model. Recall, from chapter 1, that our (unbiased) estimate of the error variance was found as \\[\\begin{equation*} s^2 = \\frac{\\sum (y_i - \\hat{y_i})^2}{n - p - 1} \\end{equation*}\\] and this is exactly equivalent to the \\(\\text{RMS}\\) in the anova table. Recall also (further back ) from MAS2902 that \\[ \\frac{(N - 1)s^2}{\\sigma^2} \\sim \\chi^2_{N - 1} \\] is the sampling distribution for the sample variance. Hence, letting \\(N = n - p\\) we get \\[ \\frac{(n - p - 1)s^2}{\\sigma^2} \\sim \\chi^2_{n - p - 1} \\] By recognising that we can rearrange the residual sum of squares term as \\(\\text{RSS} = (n - p - 1)\\times\\text{RMS}\\) (see the anova table) we then have \\[ \\frac{\\text{RSS}}{\\sigma^2} \\sim \\chi^2_{n - p - 1} \\] To find the sampling distribution of \\(F\\) (which we need in order to assess the significance of our result) we will make use of Cochran’s theorem. Cochran’s Theorem Suppose we have \\(n\\) observations, \\(Y_i\\), where \\(i = 1, \\ldots, n\\) from the same normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) and the total sum of squares is decomposed into \\(k\\) sums of squares, \\(\\text{SS}_{\\ell}\\), with associated degrees of freedom \\(\\nu_{\\ell}\\) for \\(j = 1, \\ldots, k\\). Then, each of the sums of squares, scaled by the population variance \\[ \\text{SS}_{\\ell}/\\sigma^2 \\] is an independent \\(\\chi^2\\) variable with \\(\\nu_{\\ell}\\) degrees of freedom if \\[ \\sum_{j = 1}^k \\nu_{\\ell} = n - 1. \\] In our case, we have broken the total sum of squares for our multiple linear regression model into two components, Reg SS and RSS. The observations will have the same mean when all the \\(\\beta\\) parameters are simultaneously zero, which we can use as a null hypothesis (see later). Each observation has the same variance, \\(\\sigma_\\epsilon^2\\) in our case (this is one of our key assumptions), and the respective degrees of freedom are \\(p\\) (for Reg SS) and \\(n - p - 1\\) (for RSS) and these sum to \\(n - 1\\). Thus, applying Cochran’s theorem, we must have that \\[ \\color{red}{\\text{Reg SS}/\\sigma_{\\epsilon}^2} \\] is an independent \\(\\chi^2\\) variable, with \\(p\\) degrees of freedom, since \\[ \\color{red}{\\text{RSS}/\\sigma_{\\epsilon}^2} \\] is also an independent \\(\\chi^2\\) variable, with \\(n - p - 1\\) degrees of freedom (we already demonstrated this). When we form our \\(F\\) statistic we divide Reg MS by RMS, i.e. we form the mean square ratio (MSR) which we denote by \\(F\\): \\[ \\color{red}{F = \\frac{\\text{Reg MS}}{\\text{RMS}}} \\] Both of these quantities are a sum of squares divided by its associated degrees of freedom, hence \\[ \\color{red}{F = \\frac{\\left(\\frac{\\text{Reg SS}}{p}\\right)}{\\left(\\frac{\\text{RSS}}{n - p - 1}\\right)}} \\] We can divide the top and bottom by \\(\\sigma_{\\epsilon}^2\\) to ensure the numerator and denominator are both \\(\\chi^2\\) quantities \\[ \\color{red}{F = \\frac{\\left(\\frac{\\text{Reg SS}/\\sigma_{\\epsilon}^2}{p}\\right)}{\\left(\\frac{\\text{RSS}/\\sigma_{\\epsilon}^2}{n - p - 1}\\right)} \\sim F_{p, n - p - 1}} \\] Why do we do this? The F-distribution arises as a ratio of scaled, independent \\(\\chi^2\\) variables, and this is exactly what we now have above. Hence \\(F\\) in our anova table provides us with a means of testing \\[ \\color{red}{H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_p = 0} \\] versus \\[ \\color{red}{H_1: \\text{at least one}\\; \\beta_j \\neq 0, j = 1, \\ldots, p.} \\] This is an example of an omnibus test and we use the test statistic above, \\(F = \\text{Reg MS}/\\text{RMS}\\). When the values of \\(F\\) are large (why?) we reject the null hypothesis, \\(H_0\\), in favour of the alternative hypothesis, \\(H_1\\) and we judge this using \\(p\\)-values as before. Note that both the total sum of squares (TSS) and the total degrees of freedom are unaffected by model choice since these are fixed quantities from the data. The coefficient of determination A widely-used summary measure of the performance of the regression is the coefficient of determination (or correlation), \\(R^2\\), where \\[ \\color{red}{R^2 = \\frac{\\text{Reg SS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}.} \\] This can be interpreted as the proportion of the corrected sum of squares that is explained by the candidate model, i.e. the proportion of the variability in the response that is explained by the explanatory variables. The range of \\(R^2\\) is zero to one but it is often expressed as a percentage to aid interpretation. Example: Cheddar cheese study As cheddar cheese matures, a variety of chemical processes take place. The taste of matured cheese is related to the concentration of several chemicals in the final product. In a study of cheddar cheese from several regions in the UK, 30 samples of cheese were analyzed for their chemical composition and were subjected to taste tests. Data from the study are available in the file cheese.RData with the following variables: Taste: a combined score obtained from several tasters with higher scores indicating tastier cheese (the variable); H2S: the natural log of the concentration of hydrogen sulfide; Lactic: the concentration of lactic acid. load(&quot;cheese.RData&quot;) kable(head(cheese, 5)) Taste H2S Lactic 57.2 7.908 1.90 56.7 10.199 2.01 54.9 6.752 1.52 47.9 7.496 1.81 40.9 9.588 1.74 For these data we can fit a multiple linear regression model \\[ \\color{red}{\\text{Taste}_i = \\beta_0 + \\beta_1\\text{H2S}_i + \\beta_2\\text{Lactic}_i + \\epsilon_i} \\] for \\(i = 1, \\ldots, 30\\), and with the usual assumptions about \\(\\epsilon_i\\) of normality, independence and a common variance. Construct an anova table and test the overall significance of the model. Solution We have \\(n = 30\\) here, and can calculate the total sum of squares from the raw data. We then use the `lm’ command to fit a model in the usual way before extracting the residual sum of squares, the regression sum of squares are then obtained by subtraction; note that both of these quantities depend on the fitted model. Now \\[\\begin{align*} \\color{red}{\\textrm{TSS}} &amp;\\color{red}{= \\sum_{i=1}^{30} (y_i - \\bar{y})^2} \\\\ &amp;\\color{red}{= \\sum_{i=1}^{30} y_i^2 - 30\\bar{y}^2} \\\\ &amp;\\color{red}{= (57.2^2 + 56.7^2 + \\ldots ... + 0.7^2) - 30 \\times 24.53^2} \\\\ &amp;\\color{red}{= 7662.887} \\end{align*}\\] We can then fit a regression model in the usual way, before extracting the residual sum of squares as follows: fit_cheese = lm(Taste ~ H2S + Lactic, data = cheese) (rss = sum(fit_cheese$residuals^2)) ## [1] 2668.965 # Brackets output answer to console We can now produce the anova table for this model: Source df SS MS MSR \\(\\color{red}{\\text{Regression}}\\) \\(\\color{red}{2}\\) \\(\\color{red}{4993.921}\\) \\(\\color{red}{2496.961}\\) \\(\\color{red}{25.260}\\) \\(\\color{red}{\\text{Residual}}\\) \\(\\color{red}{27}\\) \\(\\color{red}{2668.965}\\) \\(\\color{red}{98.851}\\) \\(\\color{red}{\\text{Total}}\\) \\(\\color{red}{29}\\) \\(\\color{red}{7662.887}\\) Clearly, we have a large F-statistic. Comparing with \\(F_{2, 27}\\) (why?) in tables we see that \\(p &lt; 0.01\\) and we clearly reject the null hypothesis that \\(\\beta_1 = \\beta_2 = 0\\). We can also calculate \\[ \\color{red}{R^2 = \\frac{4993.921}{7662.887} = 0.6517} \\] Thus, around two-thirds of the variation has been explained, but this does not necessarily mean this is the right model. The (omnibus) \\(F\\)-test considers all the coefficients together and a significant result indicates that at least one of them is non-zero, but which one, or is it both? In order to answer this question we also need a procedure to consider the coefficients separately. We first, however, take a detour and demonstrate how to fully construct an anova table using R, although it is a slightly clunky procedure. Anova in R We can fit the model in R in the usual way and then use the anova() command to get the sum of squares breakdown via the following commands: load(&quot;cheese.RData&quot;) fit1 = lm(Taste ~ H2S + Lactic, data = cheese) anova(fit1) ## Analysis of Variance Table ## ## Response: Taste ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## H2S 1 4376.7 4376.7 44.2764 3.851e-07 *** ## Lactic 1 617.2 617.2 6.2435 0.01885 * ## Residuals 27 2669.0 98.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We get the (rounded) residual sum of squares directly (\\(RSS = 2669\\)), but a breakdown of the regression sum of squares rather than the total value, although we can easily get the total by addition, i.e. \\(\\text{Reg SS} = 4376.7 + 617.2 = 4993.9\\). N.B. What we actually get is the Type I sums of squares, where the order variables are entered matters, as opposed to Type II sums of squares which condition on the other variables and are thus independent of order. The F-statistic can be found in R using the last line of the output from summary(fit1) ## ## Call: ## lm(formula = Taste ~ H2S + Lactic, data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.343 -6.530 -1.164 4.844 25.618 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -27.592 8.982 -3.072 0.00481 ** ## H2S 3.946 1.136 3.475 0.00174 ** ## Lactic 19.887 7.959 2.499 0.01885 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.942 on 27 degrees of freedom ## Multiple R-squared: 0.6517, Adjusted R-squared: 0.6259 ## F-statistic: 25.26 on 2 and 27 DF, p-value: 6.551e-07 Hence \\(F = 25.26\\) in this case - as in our anova table, with an exact p-value of \\(6.551 \\times 10^{-7}\\). We can also obtain \\(R^2\\) from the penultimate line of the output, or directly using summary(fit1)$r.squared ## [1] 0.6517024 which again matches what we found earlier. Hence, we can use R to circumvent the need for a formal anova table since both \\(F\\) and \\(R^2\\) can be easily extracted. 3.3 The extra sum of squares method Suppose we fit a multiple linear regression model with two continuous covariates: \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i. \\] How should we decide if either of the explanatory variables adds anything to the model? The easiest way is to fit the model with and without the variable and observe the change in the model sum of squares. We can test this change using the `extra sum of squares’ principle. Helpfully, we can use an anova table - similar to those seen earlier - by including an additional row to the basic table. 3.3.1 The extended anova table The regression sum of squares is broken down in to its contribution from each covariate. A key feature of this method is that the order that the covariates are entered into the model matters, as we shall see in the following examples. The general table structure is given below: Note that \\(\\text{Reg SS}_{x_1 + x_2}\\) is the regression sum of squares from the full model. Source df SS MS MSR Regression on \\(x_1\\) 1 \\(\\text{Reg SS}_{x_1}\\) \\(\\text{Reg MS}_{x_1}=\\text{Reg SS}_{x_1}\\) \\(F_1 = \\frac{\\text{Reg MS}_{x_1}}{\\text{RMS}}\\) Regression on \\(x_2\\) having fitted \\(x_1\\) 1 \\(\\text{Reg SS}_{x_2 \\mid x_1} =\\) \\(\\text{Reg SS}_{x_1 + x_2} - \\text{Reg SS}_{x_1}\\) \\(\\text{Reg MS}_{x_2 \\mid x_1}=\\text{Reg MS}_{x_2 \\mid x_1}\\) \\(F_2= \\frac{\\text{Reg MS}_{x_2 \\mid x_1}}{\\text{RMS}}\\) Residual \\(n - 3\\) RSS \\(\\text{RMS} = \\frac{\\text{RSS}}{n - 3}\\) Total \\(n - 1\\) TSS Note that \\(\\text{Reg SS}_{x_1 + x_2}\\) is the regression sum of squares from the full model. Example: Extra sum of squares for cheese data Returning to the cheese data, we can fit the model with either hydrogen sulfide or lactic acid first, i.e. there are two possible orderings. As order matters in the extra sum of squares method we will consider both possibilities in turn to investigate any possible differences. To produce anova tables for the extra sum of squares approach we can use the generic R command anova() as a basis, albeit with some additional work involved to construct the anova table. Fitting hydrogen sulfide first, we get: fit1 = lm(Taste ~ H2S + Lactic, data = cheese) anova(fit1) ## Analysis of Variance Table ## ## Response: Taste ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## H2S 1 4376.7 4376.7 44.2764 3.851e-07 *** ## Lactic 1 617.2 617.2 6.2435 0.01885 * ## Residuals 27 2669.0 98.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can thus populate the anova table: Source df SS MS MSR Regression on H2S 1 4376.746 4376.746 44.276 Regression on lactic acid having fitted H2S 1 617.176 617.176 6.244 Residual 27 2668.965 98.851 Total 29 7662.887 Since \\(F_{1, 27}(5\\%) = 4.21\\) we can see that H2S is needed in the model since \\(F_1 = 44.276\\) hugely exceeds this critical value (the exact p-value from R is \\(3.851 \\times 10^{-7}\\)). We also see that \\(F_2 = 6.244\\) exceeds the critical value, so lactic acid is needed in the model, after H2S has already been included (from R, \\(p = 0.01885\\)). The model where we switch the order of the variables can be fitted in R using fit1A = lm(Taste ~ Lactic + H2S, data = cheese) anova(fit1A) ## Analysis of Variance Table ## ## Response: Taste ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Lactic 1 3800.4 3800.4 38.446 1.25e-06 *** ## H2S 1 1193.5 1193.5 12.074 0.001743 ** ## Residuals 27 2669.0 98.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This leads to the anova table below: Source df SS MS MSR Regression on lactic acid 1 3800.398 3800.398 38.446 Regression on H2S having fitted lactic acid 1 1193.523 1193.523 12.074 Residual 27 2668.965 98.851 Total 29 7662.887 We can see that both \\(F_1\\) and \\(F_2\\) are comfortably bigger than our critical value of 4.21 (this is still the same value) so conclude that lactic acid is needed in the model, and that hydrogen sulfide is needed in the model after lactic acid has been included. The conclusion is unambiguous here: both variables are needed in the model, irrespective of ordering. Note that the residual and total SS are unaffected by the ordering (this is also true for the critical value). We now consider another example where the conclusion is not so clear cut. Henceforth, we will use R directly without formally constructing anova tables. Example: Warfarin study Children with heart problems take warfarin to avoid getting strokes. In a study of 120 children, the warfarin dose (mg), age (months) and height (cm) were measured. A scatterplot of the dose against age produced the following plot Figure 3.1: Scatterplot of dose against age for the warfarin study. The average dose increase with age but so does the variability (spread of observations). The latter breaks one of our assumptions! We need to transform the dose to stabilise the variance (we will return to transformations formally later in the semester). Here we take the square root transformation (other transformations may also work as well, or better) which has the effect of reducing the larger values (more than the smaller values due to the nature of the square root operator). The original variable is in the R dataframe warfarinStudy, so we can perform the transformation using # Add the new variable to the data frame warfarinStudy$root_dose = sqrt(warfarinStudy$warfarin_dose) Plotting the square root of dose against age we see that it increases on average with increasing age and that the variability is now approximately constant. Figure 3.2: Scatterplot of the square root of dose against age for the warfarin study. A similar picture is obtained if we plot the square root of dose against height (the other possible covariate here). We shall now regress the square root of dose against height and age, looking at both possible orderings. The R command anova() breaks down the regression sum of squares into its individual contribution from each covariate: m1 = lm(root_dose ~ height + age, data = warfarinStudy) anova(m1) ## Analysis of Variance Table ## ## Response: root_dose ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## height 1 13.046 13.0462 58.4899 6.289e-12 *** ## age 1 1.152 1.1520 5.1647 0.02488 * ## Residuals 117 26.097 0.2231 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Height on its own produces a miniscule p-value (\\(6.289 \\times 10^{-12}\\)) and thus we clearly need to include height. Fitting age height also gives a small p-value (\\(0.025 &lt; 0.05\\)) which implies that we also need age, once height has been fitted. In a similar way we can again use the command anova() for a model that includes age first, followed by height. m2 = lm(root_dose ~ age + height, data = warfarinStudy) anova(m2) ## Analysis of Variance Table ## ## Response: root_dose ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 14.1975 14.1975 63.652 1.124e-12 *** ## height 1 0.0007 0.0007 0.003 0.9564 ## Residuals 117 26.0969 0.2231 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Age on its own produces a very small p-value (\\(1.124 \\times 10^{-12}\\)) and thus we clearly need to include age. Fitting height after age gives a non-significant p-value (\\(0.9564 &gt; 0.05\\)), however, which implies that we do not need height once age has been fitted. Thus we have conflicting conclusions (for height) - model one suggests we do need height, model two indicates that we do not; both models unanimously agree that age is needed. We want as simple model as possible and so we should just include age and then do model checking as described in chapter 2. Why has this happened? The problem has arisen because age and height are highly correlated. We shall come back to this problem later. The correlation is in fact 0.95 (see plot). Figure 3.3: Scatterplot of height against age for the warfarin study. 3.4 The general extra sum of squares method The extra sum of squares method is most useful if we wish to test whether a subset of the explanatory variables have no effect on the response variable. This can help simplify the model by removing several covariates at once - it is also useful for dealing with factors that have several levels, such as eye colour or favoured mode of transport. In order to use the extra sum of squares method in this scenario we order the \\(x\\) variables so that the subset we are going to test takes the last \\(q\\) places of the parameter vector \\(\\underline{\\beta}\\). Notationally, we partition the parameter vector as \\[\\begin{align*} \\color{red}{\\underline{\\beta} = \\begin{pmatrix} \\underline{\\beta}_1 \\\\ \\underline{\\beta}_2 \\\\ \\end{pmatrix}} \\end{align*}\\] where \\(\\underline{\\beta}_2\\) is the \\(q-\\)vector of parameters that we wish to consider for removal from the model, and \\(\\underline{\\beta}_1\\) is the \\((p-q)\\)-vector of parameters which we wish to keep in the model. To carry out the procedure, we firstcalculate (usually using R) the regression sum of squares for fitting all \\(p\\) candidate parameters, i.e. fit the full (additive) model. We then calculate (again using R, typically) the regression sum of squares for fitting the \\(p - q\\) parameters which we want to include, i.e. fit the subset model. We can then form an extended general anova table: Source df SS MS MSR Regression on \\(x_1, \\ldots x_{p-q}\\) \\(p - q\\) \\(\\text{Reg SS}_{\\underline{\\beta}_1}\\) \\(\\text{Reg MS}_{\\underline{\\beta}_1}=\\frac{\\text{Reg SS}_{\\underline{\\beta}_1}}{p-q}\\) \\(F_1 = \\frac{\\text{Reg MS}_{\\underline{\\beta}_1}}{\\text{RMS}}\\) Regression on \\(x_{p-q+1}, \\ldots x_p\\) having fitted \\(x_1, \\ldots x_{p-q}\\) \\(q\\) \\(\\text{Reg SS}_{\\underline{\\beta}_2 \\mid \\underline{\\beta}_1} =\\) \\(\\text{Reg SS}_{\\underline{\\beta}} - \\text{Reg SS}_{\\underline{\\beta}_1}\\) \\(\\text{Reg MS}_{\\underline{\\beta}_2 \\mid \\underline{\\beta}_1}=\\frac{\\text{Reg MS}_{\\underline{\\beta}_2 \\mid \\underline{\\beta}_1}}{q}\\) \\(F_2= \\frac{\\text{Reg MS}_{\\underline{\\beta}_2 \\mid \\underline{\\beta}_1}}{\\text{RMS}}\\) Residual \\(n - p - 1\\) RSS \\(\\text{RMS} = \\frac{\\text{RSS}}{n - p - 1}\\) Total \\(n - 1\\) TSS We can now test \\(H_0: \\beta_{p - q + 1} = \\beta_{p - q + 2} = \\ldots = \\beta_p = 0\\) given that \\(\\beta_1, \\ldots, \\beta_{p - q}\\) have been included in the model using \\(F_2\\). This is tested against a general alternative \\(H_1: \\text{at least one}\\;\\; \\beta_j \\neq 0\\) for \\(j = p - q + 1, \\ldots, p\\). This is another example of an test as we are testing for several things simultaneously. Large values of \\(F_2\\) lead to rejection of the null hypothesis, \\(H_0\\). This process allows us to remove a block of parameters from a model, rather than one-by-one, and is particularly useful when adding parameters to a model where previous research or expert knowledge suggests some covariates must be included in the model. In essence, we are then testing whether the extra covariates can add anything to the established model. Example: Extended extra sum of squares for cheese data Returning to the cheese data, three additional variables are also available in the dataset cheese2.RData, namely Acetic: the natural log of the concentration of acetic acid (in micromoles per liter, \\(\\mu\\)mol/L); Phosphoric: the amount of phosphoric acid (in mg); Citric: the amount of citric acid (in ml). We can use the extra sum of squares method to investigate whether these additional variables should be added to the original model containing H2S and lactic acid. We do so by fitting the full model and the reduced model with the first \\(p\\) parameters included (we do not need to fit a model with just the remaining parameters). As usual, we can make repeated use of lm() to do this: load(&quot;cheese2.RData&quot;) fit1 = lm(Taste ~ H2S + Lactic, data = cheese2) fit2 = lm(Taste ~ ., data = cheese2) # Using . fits every variable in the dataset as a predictor We can then use the anova command with two arguments (which must be nested models) to carry out the extra sum of squares method and test whether any of acetic, phosphoric or citric acid are needed in the model. The R command and output is shown below: anova(fit1, fit2) ## Analysis of Variance Table ## ## Model 1: Taste ~ H2S + Lactic ## Model 2: Taste ~ H2S + Lactic + Acetic + Phosphoric + Citric ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 27 2669.0 ## 2 24 2602.5 3 66.484 0.2044 0.8923 The syntax is such that we should always have the reduced model first, i.e. anova(reduced_model, full_model) We see that \\(p = 0.8923 &gt; 0.05\\) so we can retain the null hypothesis and our original model containing both H2S and lactic acid is our chosen model; acetic, phosphoric and citric acid are not needed in the model, in the presence of H2S and lactic acid. Summary: extra sum of squares method Removing several parameters at once can lead to the removal of potentially important predictors. This can happen if one variable is borderline significant and the others are not. Hence, we should proceed cautiously. The method must be used for factors with more than two levels since we need a way of assessing the overall significance of the variable which cannot be done using parameter-specific \\(p\\)-values - see later chapters. For a solitary continuous (or ordinal) covariate (or a factor with 2 levels) - i.e. anything with a solitary degree of freedom - we can still apply the method, but it is slightly inefficient, as we shall see in section 3.5. Example: Anova for crime data based on summary information A dataset consisting of crime rates from 47 US states along with 13 continuous explanatory variables was analysed in R using an additive multiple linear regression model. An incomplete analysis of variance using all thirteen predictors produced the following table: Source df SS MS MSR Regression on all predictors 52931 Residual 15879 Total 46 68810 Complete the anova table and test for significance for all 13 predictors simultaneously. A criminologist postulates that there are five key drivers of state crime and conducts a multiple linear regression model using these five covariates. The parameter estimates (and their standard errors) are given below. Estimate Standard error 1.02 0.35 2.03 0.47 1.23 0.14 0.91 0.43 0.63 0.14 Which variable appears to be the most important? Which seems to be the least important? The data analyst further reports that the residual sum of squares is \\(18604\\) for the reduced model. Use this information to test whether the criminologist’s model is sufficient for these data. [You are not required to produce the formal anova table.] Compare the values of \\(R^2\\) for the full model and that put forward by the criminologist. Solution Since we have 13 continuous predictors we must have 13 degrees of freedom for the regression. Then, by subtraction, we can get the error degrees of freedom as \\(46 - 13 = 33\\). Once we have the sums of squares and their associated degrees of freedom it is easy to calculate the mean square terms working within rows. For the regression we have \\(52931/13 = 4071.615\\) and for the error \\(15879/33 = 481.82\\). Finally, we calculate the test statistics as the ratio of the regression mean square to the error mean square, i.e. \\(4071.615/481.182 = 8.462\\). This completes the anova table. Source df SS MS MSR Regression on all predictors 13 52931 4071.615 8.462 Residual 33 15879 481.82 Total 46 68810 We can look up the \\(5\\%, 1\\%\\) and \\(0.1\\%\\) critical values in R using qf(c(0.95, 0.99, 0.999), 13, 33) to get 2.030, 2.723 and 3.773 respectively. Since \\(8.462 &gt; 3.773\\) we conclude that at least one of the predictors is important and there is very strong evidence of some relationship between crime rate and the explanatory variables. To answer this question we must take the ratios to get the respective \\(t\\)-statistics. This gives Estimate Standard error t 1.02 0.35 2.914 2.03 0.47 4.319 1.23 0.14 8.786 0.91 0.43 2.116 0.63 0.14 4.5 Hence, we can now say that \\(x_3\\) is the most important predictor, followed by \\(x_5\\) (which had the lowest estimate). The least important predictor is \\(x_4\\), although it still has a reasonably large \\(t\\)-value Using the relation that the total SS is the sum of the regression and residual SS terms we can get the regression SS for the five-predictor model as \\(68810 - 18604 = 50206\\). To test the reduced model we need its (conditional) regression sum of squares, found by subtraction as \\(52931 - 50206 = 2725\\) (note that \\(52931\\) is the regression sum of squares from the full model in (a)). This has 8 degrees of freedom associated so the regression MS for the additional 8 variables is \\(2725/8 = 340.625\\) which we compare to the full model error MS to get \\(F = 340.625/481.182 = 0.71\\). From R, \\(F_{8, 33}(5\\%) = 2.235\\) so we retain the null hypothesis - the extra 8 predictors are not needed if the five variables nominated by the criminologist are already in the model. A more direct solution is to note that the difference in the residual sums of squares will also give us the reduced model SS, namely \\(18604 - 15879 = 2725\\). We then proceed as above. To calculate \\(R^2\\) we divide the regression total sum of squares by the total sum of squares to identify how much of the total is explained by the regressors. In this case \\(R^2 = 52931/68810 = 0.769\\), so all of the predictors combined explain around \\(77\\%\\) of the variability in crime rates. For the pathologist’s model \\(R^2 = 50206/68810 = 0.730\\), i.e. around \\(73\\%\\) of the variability. This is not much different for the loss of eight predictors and suggests there is some validity to the claim. 3.5 Inference on individual parameters The extra sum of squares method allows us to remove multiple parameters or on a one-by-one basis. In the latter case, however, there is a simpler approach than forming anova tables each time you wish to remove a parameter. The model fit given by the summary() command considers the joint distribution of the parameter vector, \\(\\underline{\\hat{\\beta}}\\). Hence we can use the output directly to test hypotheses and make inferences about individual parameters, without having to be concerned about the order in which the variables entered the model. To test the hypothesis \\(H_0: \\beta_j = b_j\\) for a chosen \\(j \\in 1, \\ldots, p\\), given that the other parameters are fitted, we use \\[ \\color{red}{t = \\frac{\\mid\\hat{\\beta}_j - b_j\\mid}{s.e.\\left(\\hat{\\beta_j}\\right)}} \\] and this is compared to the \\(t\\)-distribution on \\(n - p - 1\\) degrees of freedom. Typically, we test whether the parameter has no effect on the regression line, i.e. \\(\\hat{\\beta}_j = 0\\), whereby \\(b_j = 0\\). As in chapter 1, subsection 1.4.3, let \\(v_{jj}\\) be the \\((j+1)^{th}\\) diagonal element of \\((\\mathrm{X}^T\\mathrm{X})^{-1}\\), for \\(j = 0, \\ldots, p\\). The variance of \\(\\hat{\\beta}_j\\) is then estimated as \\(v_{jj}s^2\\) since our \\(\\beta\\) parameters are indexed starting at \\(0\\) for the intercept. We can then construct a \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\beta_j\\) as \\[\\begin{align*} \\color{red}{\\hat{\\beta}_j} &amp;\\color{red}{\\pm t_{n - p - 1; \\alpha/2} \\times \\sqrt{v_{jj}s^2}} \\\\ &amp;\\color{red}{\\pm t_{n - p - 1; \\alpha/2} \\times s.e.(\\hat{\\beta}_j)} \\end{align*}\\] Note that these hypothesis tests and confidence intervals are only a guide. The \\(t\\)-test outlined above, which are given in the model output via R, are exactly equivalent to an \\(F\\)-test on this parameter having been fitted last. Since the \\(\\hat{\\beta}_j\\) are correlated, statements about single parameters are not independent from statements about the remaining parameters. In practice, we often remove the variable with the largest \\(p\\)-value (assuming \\(p &gt; 0.05\\), say) and refit the model, continuing until all the remaining variables have small \\(p\\)-values (\\(&lt;0.05\\), say), unless we have specific reasons or guidance that certain parameters should be retained in any final model. Example: Inference on individual parameters - warfarin example Returning to the warfarin example and inspecting the summary of the fitted model with both height and age included we obtain summary(m1) ## ## Call: ## lm(formula = root_dose ~ height + age, data = warfarinStudy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.95647 -0.30922 -0.07802 0.33932 1.09938 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0529539 0.3002567 3.507 0.000644 *** ## height 0.0002249 0.0041024 0.055 0.956367 ## age 0.0058346 0.0025674 2.273 0.024876 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4723 on 117 degrees of freedom ## Multiple R-squared: 0.3524, Adjusted R-squared: 0.3413 ## F-statistic: 31.83 on 2 and 117 DF, p-value: 9.187e-12 The \\(p\\)-values given above for the \\(t\\)-tests are exactly the same as those for the respective \\(F\\)-tests - see the earler example. Thus the recommended course of action (as before) is to remove height and fit a model with height alone. The \\(p\\)-value for age then reduces to \\(1.124 \\times 10^{-12}\\)! We can easily produce confidence intervals for the fitted parameters in R using the confint() function: confint(m1) ## 2.5 % 97.5 % ## (Intercept) 0.4583113390 1.647596552 ## height -0.0078996982 0.008349579 ## age 0.0007500711 0.010919132 We see that the confidence interval for height \\((-0.008, 0.008)\\) contains zero implying that parameter should be removed. Note that this relationship between a \\(p\\)-value at the \\(\\alpha\\%\\) level and an associated confidence interval at the equivalent level, i.e. \\(100(1 - \\alpha)\\%\\), always holds. Thus we should remove height and recalculate the confidence interval for height. Note that we would achieve the same results under this approach if we use the fitted model m2 (see earlier), which reverses the order of age and height. Hence, order matters for anova and contributions to the regression sum of squares, but not for inference on individual parameters. Fitting the model without height and recalculating the confidence interval for age we get mfinal = lm(root_dose ~ age, data = warfarinStudy) summary(mfinal) ## ## Call: ## lm(formula = root_dose ~ age, data = warfarinStudy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.9546 -0.3119 -0.0802 0.3416 1.1020 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.068346 0.106103 10.069 &lt; 2e-16 *** ## age 0.005969 0.000745 8.012 9.01e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4703 on 118 degrees of freedom ## Multiple R-squared: 0.3523, Adjusted R-squared: 0.3469 ## F-statistic: 64.19 on 1 and 118 DF, p-value: 9.01e-13 The standard error of the age parameter has reduced considerably from 0.0026 to 0.0007. The \\(95\\%\\) confidence interval for age is thus \\(0.005969 \\pm 1.98 \\times 0.00745 = (0.0045,0.0074)\\). This can be equivalently obtained from R using confint(mfinal). confint(mfinal) ## 2.5 % 97.5 % ## (Intercept) 0.858232773 1.278458803 ## age 0.004493902 0.007444623 Comparing with the confidence interval from the full model we see that it is now much narrower. 3.6 Confidence and prediction intervals for the fitted values Recall that the fitted values in a mutliple linear regression model are given by \\[\\begin{align*} \\underline{\\hat{Y}} &amp;= \\mathrm{X}\\underline{\\hat{\\beta}} \\\\ &amp;= \\mathrm{X}(\\mathrm{X}^T\\mathrm{X})^{-1}\\mathrm{X}^T\\underline{Y} \\\\ &amp;= \\mathrm{H}\\underline{Y} \\end{align*}\\] Making use of these alternative forms we found the expectation and the variance of the fitted values back in subsection 1.4.4 of the notes, namely \\[ \\mathrm{E}[\\underline{\\hat{Y}}] = \\mathrm{X}\\underline{\\beta} \\] and \\[\\begin{equation*} \\mathrm{Var}[\\underline{\\hat{Y}}] = \\mathrm{H}\\sigma_{\\epsilon}^2. \\end{equation*}\\] Hence, at the individual observation level, we have \\(\\mathrm{E}\\left[\\hat{Y}_i\\right] =\\underline{x}_i^T \\underline{\\beta}\\). Also, for the variance we have \\(\\mathrm{Var}\\left[\\hat{Y}_i\\right] = h_{ii}\\sigma_{\\epsilon}^2\\) for \\(i = 1, \\ldots, n\\), where the \\(h_{ii}\\) are the diagonal elements (the leverages, recall) of the hat matrix \\(\\mathrm{H}\\). By noting that the fitted values are a linear combination of random variables, \\(\\underline{\\hat{\\beta}}\\), we can see that they are also normally distributed. This means we can find a \\(100(1 - \\alpha)\\%\\) confidence interval for the fitted value in the usual way as \\[ \\hat{Y}_i \\pm t_{\\nu, 1 - \\alpha/2} \\times s \\sqrt{h_{ii}} \\] where \\(\\nu = n - p - 1\\) is the residual degrees of freedom and \\(s\\) is the square root of RMS (see earlier). It is instructive at this point to consider how the individual \\(h_{ii}\\) terms are calculated. Clearly, from a fitted model object we can just extract them using the R command hatvalues(), but what about for a new observation? Consider the variance of the fitted value, \\(\\hat{Y}_p\\), for this new observation, with covariate vector \\(\\underline{x}_p\\): \\[\\begin{align*} \\color{red}{\\mathrm{Var}\\left[\\hat{Y}_p\\right]} &amp;\\color{red}{= \\mathrm{Var}\\left[\\underline{x}_p^T \\underline{\\hat{\\beta}}\\right]} \\\\ &amp;\\color{red}{= \\underline{x}_p^T \\mathrm{Var}\\left[\\underline{\\hat{\\beta}}\\right] \\underline{x}_p} \\\\ &amp;\\color{red}{= \\underline{x}_p^T (\\mathrm{X}^T\\mathrm{X})^{-1} \\underline{x}_p \\sigma_{\\epsilon}^2} \\end{align*}\\] Hence, for any (observed or typically new) covariate pattern \\(\\underline{x}_p\\) we can calculate the confidence interval for the fitted response as \\[ \\color{red}{\\hat{Y}_p \\pm t_{\\nu, 1 - \\alpha/2} \\times s \\sqrt{h_{pp}}} \\] where \\(h_{pp} = \\underline{x}_p^T (\\mathrm{X}^T\\mathrm{X})^{-1} \\underline{x}_p\\). Note that this interval is for the average or mean response, i.e. for observations that lie perfectly on the line of best fit and have no error attached to them. To account for error, we can also calculate a prediction interval for an individual observation, based on a vector of covariates, again either observed or, more likely, new. We now have \\[\\begin{align*} \\color{red}{\\hat{Y}_p^*} &amp;\\color{red}{= \\underline{x}_i^T \\underline{\\hat{\\beta}} + \\epsilon_p} \\\\ &amp;\\color{red}{= \\hat{Y}_p + \\epsilon_p} \\end{align*}\\] where \\(\\epsilon_p\\) captures the unknown error associated with the prediction, and is assumed to be normally distributed, and independent of \\(\\hat{Y}_p\\). Hence, \\[\\begin{align*} \\color{red}{\\mathrm{Var}\\left[\\hat{Y}_p^* \\right]} &amp;\\color{red}{= \\mathrm{Var}\\left[\\underline{x}_p^T \\underline{\\hat{\\beta}}\\right] + \\mathrm{Var}[\\epsilon_p]} \\\\ &amp;\\color{red}{= \\underline{x}_p^T (\\mathrm{X}^T\\mathrm{X})^{-1} \\underline{x}_p\\sigma_{\\epsilon}^2 + \\sigma_{\\epsilon}^2} \\\\ &amp;\\color{red}{= \\left(\\underline{x}_p^T (\\mathrm{X}^T\\mathrm{X})^{-1} \\underline{x}_p + 1\\right) \\sigma_{\\epsilon}^2}, \\end{align*}\\] and a prediction interval for a new observation is found as \\[ \\color{red}{\\hat{Y}_p^* \\pm t_{\\nu, 1 - \\alpha/2} \\times s \\sqrt{h_{pp} + 1}.} \\] Note the distinction between the two intervals - the prediction interval is always wider. The prediction interval is the interval for an actual observation, whereas the confidence interval is for the average observation, both based on the same set of covariate values. In each case, the width of the interval increases with the leverage - high leverage points are predicted less accurately. Example: Confidence and prediction intervals for the cheese data Confidence and prediction intervals for the fitted values can be easily calculated using R. We need a fitted model (obviously!) and a dataframe containing the covariate values for which we want to construct our confidence and prediction intervals - this is best seen by example. Thus, for the original cheese example with just two covariates: fit1 = lm(Taste ~ H2S + Lactic, data = cheese) newdat = data.frame(H2S = 6, Lactic = 1.5) predict(fit1, newdat, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 25.9166 22.09274 29.74045 predict(fit1, newdat, interval = &quot;prediction&quot;) ## fit lwr upr ## 1 25.9166 5.161269 46.67192 The prediction interval is much wider, by a factor of about 5 for these values of the explanatory variables. 3.7 Polynomial models Sometimes the response might have a curvilinear relationship with one or more of the explanatory variables and we may think about fitting a polynomial model - remember that this still falls under our multiple linear regression framework as long as the postulated model is linear in the parameters, \\(\\underline{\\beta}\\). For instance, if there is just one explanatory variable we might consider fitting \\[ \\color{red}{Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\epsilon_i} \\] for \\(i = 1, \\ldots, n\\). Defining \\[ \\color{red}{x_{i1}^* = x_i; x_{i2}^* = x_i^2; x_{i3}^* = x_i^3}, \\] the model then becomes \\[ \\color{red}{Y_i = \\beta_0 + \\beta_1 x_{i1}^* + \\beta_2 x_{i2}^* + \\beta_3 x_{i3}^* + \\epsilon_i} \\] which is of exactly the form of a multiple linear regression model. Other covariates can also be included in the usual way, i.e. \\[ \\color{red}{Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i1}^2 + \\beta_4 x_{i1}x_{i2} + \\beta_5 x_{i2}^2 + \\epsilon_i} \\] Hence, we can fit polynomial models in the same way to models seen so far during the module. However, when fitting polynomials, there can be high correlations between powers of the covariates, and hence near-multicollinearity problems, known as polynomial multicollinearity. For example, consider a(n equi-spaced) covariate \\(\\underline{x}^T = (1, 2, \\ldots, 10)\\) x = 1:10 cor(x, x^2) ## [1] 0.9745586 cor(x^2, x^3) ## [1] 0.9871797 From practical 1, we saw that this can inflate standard errors, thereby diluting tests on individual parameters and this often conflicts with a large \\(F\\) value (i.e. highly significant) for the overall model. These high correlations can be reduced by mean-centering - we will see this in a practical session. Suppose our covariate is \\(\\underline{x}_1\\) then we introduce \\(z_{i1} = x_{i1} - \\bar{x_1}\\;\\; (i = 1, \\ldots n)\\) and use this in the model. Recall from chapter 1 (and practical 1) that this sort of scaling does not affect the fit of the regression model. Example: Polynomial model An experiment was carried out to determine the frothiness of three types of beer from the time of pouring. Measurements of wet foam height at various time points for the three brands of beer were measured. The results for one particular brand can be found on Canvas in the file beer1.RData. A plot of the foam height against time is given below: Figure 3.4: Scatterplot of foam height against time for the beer data. The plot is strongly suggestive of a curvilinear (possibly quadratic?) relationship. We can fit the model with a quadratic term for time \\[ \\color{red}{\\text{Height}_i = \\beta_0 + \\beta_1 \\text{Time}_i + \\beta_2 \\text{Time}_i^2 + \\epsilon_i} \\] for \\(i = 1, \\ldots, 15\\). This can be done in several ways in R. We will use the built-in function poly() to fit the model - note that this function automatically uses orthogonal polynomials which remove the correlations between the powers of the covariate completely, at the cost of interpretation but the gain of model selection. Fitting the model in R, we use the following commands: fit1 = lm(Height ~ poly(Time, 2), data = beer1) We can summarise the model in the usual way (as it is still a multiple linear regression model): summary(fit1) ## ## Call: ## lm(formula = Height ~ poly(Time, 2), data = beer1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.36021 -0.22760 -0.06058 0.25347 0.41401 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.16000 0.07612 146.62 &lt; 2e-16 *** ## poly(Time, 2)1 -12.85980 0.29480 -43.62 1.37e-14 *** ## poly(Time, 2)2 2.96964 0.29480 10.07 3.31e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2948 on 12 degrees of freedom ## Multiple R-squared: 0.994, Adjusted R-squared: 0.9931 ## F-statistic: 1002 on 2 and 12 DF, p-value: 4.442e-14 We see that both the linear and quadratic terms in time are highly significant beyond the \\(0.1\\%\\) level. The \\(R^2\\) value is also very high, suggesting most of the variability in foam height is explained by the quadratic model in time. There seems little value in considering higher order terms in this case, but how do we choose in cases that are less clear-cut? 3.7.1 Choosing the order of a polynomial model When dealing with polynomial regression models we now have an additional modelling question to consider - should we fit a quadratic, cubic, quartic or higher-order polynomial? As usual, we try to choose the simplest model which gives a reasonable fit. In practice, polynomials higher than a cubic are rarely used - practitioners would usually favour splines or a nonlinear model over a high order polynomial. We start with a low order model and successively fit higher order terms until no significant improvement is obtained. Improvement is a subjective term, so this could be measured in terms of \\(R^2\\) (although other criteria may be better); in the special case of orthogonal polynomials this is made easier as each term in the model is independent. As soon as the highest order term becomes non-significant, then that term is unnecessary and model selection can finish. Once the order is selected, model adequacy should be checked in the usual way, i.e. residual plots, regression diagnostics. Hierarchical model building Consider the model \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\] for \\(i = 1, \\ldots, n\\). Suppose we fit this model and find that the regression summary shows that the linear term is not significant but the quadratic term is. If we then removed the linear term, our reduced model would then become \\[ \\color{red}{Y_i = \\beta_0 + \\beta_2 x_i^2 + \\epsilon_i.} \\] Suppose, however, we then made a scale change whereby \\(x_i \\rightarrow x_i + z\\) for \\(i = 1, \\ldots, n\\). The model above then becomes \\[\\begin{align*} \\color{red}{Y_i} &amp;\\color{red}{= \\beta_0 + \\beta_2 (x_i + z)^2 + \\epsilon_i} \\\\ &amp;\\color{red}{= \\beta_0 + \\beta_2 z^2 + 2\\beta_2 x_i z + \\beta_2 x_i^2 + \\epsilon_i} \\end{align*}\\] The linear term has now reappeared and so our model has effectively changed. Scale changes (such as our mean-centering seen in chapter 1) should not make any important changes to the model, but in this case an additional term has been added, which is highly undesirable. We want our models to be scale invariant. This illustrates why we should not remove lower order terms in the presence of higher order terms. Model building should be hierarchical and we do not want the interpretation of the model to depend on the choice of scale. Removal of the first order term here corresponds to the hypothesis that the predicted response is symmetric about \\(x_i = 0\\), which is not usually tenable and the same argument can be made about taking out the intercept term when it is not significant. Thus it also should be retained. "],["analysis-of-designed-experiments.html", "4 Analysis of designed experiments 4.1 Completely randomised design Example: One-way anova Example: Multiple comparisons 4.2 Randomised block design 4.3 Factorial experiments", " 4 Analysis of designed experiments So far in the module we have focused on continuous covariates. Although very common, a real-world dataset - and thus the statistical model - will typically contain a mixture of continuous and categorical covariates.In this chapter we will consider how to handle categorical variables, in the context of designed experiments. 4.1 Completely randomised design A completely randomised design occurs when all experimental units are drawn from the same ‘population’ of interest. In this scenario, there are no obvious ‘groupings’ where members of different groups might be expected to respond differently. ‘Treatments’ are then applied to the experimental units - this is an example of a factor which can take \\(k\\) values, one for each of \\(k\\) potential treatments. A sensible initial model might be \\[ \\color{red}{Y_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}} \\] for \\(i = 1, \\ldots, k\\) and \\(j = 1, \\ldots n_i\\) where \\(k\\) is the number of treatments or factor values, \\(n_i\\) is the number of observations on the \\(i^{th}\\) treatment. We make the usual assumptions about the error terms, \\(\\epsilon_{ij}\\). In this model: \\(\\mu\\) denotes an overall mean \\(\\alpha_i\\) is the additional effect of the \\(i^{th}\\) treatment. However, this is an over-parameterised model since we have \\(k + 1\\) parameters and essentially only \\(k\\) pieces of information, the sample means for each treatment group, to estimate them from. Thus we must make some (further) assumptions concerning the \\(\\alpha_i\\) parameters. The most commonly adopted assumption is to take \\(\\alpha_1\\) to be zero. Thus, under this assumption, the mean of the first group is given by \\(\\mu\\) the mean of the \\(i^{th}\\) group by \\(\\mu + \\alpha_i\\) for \\(i = 2, 3, \\ldots, k\\) We can then set this model up as a linear model and estimate the parameters using our general theory established in chapter 1. Example: One-way anova We shall demonstrate this design with a simple example. An experiment was carried out to determine the effects of four treatments \\(A, B, C\\) and \\(D\\) on a certain crop. A field was divided into \\(12\\) plots and \\(3\\) replicates of each of the \\(4\\) treatments were randomly assigned to the plots. The following data were recorded: Treatment Yield Total Mean A 33.63 37.80 36.58 108.01 36.00 B 35.83 38.18 37.89 111.90 37.30 C 42.92 40.43 41.46 124.81 41.60 D 38.02 39.62 35.99 113.63 37.88 Comment: There is clearly some variability between our treatments. What we want to know is whether this is just chance or is there evidence of a genuine difference between treatments? By inspection, \\(C\\) seems to have a higher mean than the other 3 treatments. We can set this up as a linear model \\[ \\underline{Y} = \\mathrm{X}\\underline{\\beta} + \\underline{\\epsilon} \\] where, from the data above, we have: \\[\\begin{align*} \\underline{y} = \\begin{pmatrix} 33.63 \\\\ 37.80 \\\\ 36.58 \\\\ 35.83 \\\\ 38.18 \\\\ 37.89 \\\\ 42.92 \\\\ 40.43 \\\\ 41.46 \\\\ 38.02 \\\\ 39.62 \\\\ 35.99 \\\\ \\end{pmatrix}, \\mathrm{X} = \\begin{pmatrix} 1&amp; 0&amp; 0&amp; 0 \\\\ 1&amp; 0&amp; 0&amp; 0 \\\\ 1&amp; 0&amp; 0&amp; 0 \\\\ 1&amp; 1&amp; 0&amp; 0 \\\\ 1&amp; 1&amp; 0&amp; 0 \\\\ 1&amp; 1&amp; 0&amp; 0 \\\\ 1&amp; 0&amp; 1&amp; 0 \\\\ 1&amp; 0&amp; 1&amp; 0 \\\\ 1&amp; 0&amp; 1&amp; 0 \\\\ 1&amp; 0&amp; 0&amp; 1 \\\\ 1&amp; 0&amp; 0&amp; 1 \\\\ 1&amp; 0&amp; 0&amp; 1 \\\\ \\end{pmatrix}, \\underline{\\beta} = \\begin{pmatrix} \\mu \\\\ \\alpha_2 \\\\ \\alpha_3 \\\\ \\alpha_4 \\\\ \\end{pmatrix}, \\underline{\\epsilon} = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5 \\\\ \\epsilon_6 \\\\ \\epsilon_7 \\\\ \\epsilon_8 \\\\ \\epsilon_9 \\\\ \\epsilon_{10} \\\\ \\epsilon_{11} \\\\ \\epsilon_{12} \\end{pmatrix} , \\end{align*}\\] Then we have seen from the general theory, given in chapter 1, that the parameter estimates are \\[ \\color{red}{\\underline{\\hat{\\beta}} = (\\mathrm{X}^T\\mathrm{X})^{-1}\\mathrm{X}^T\\underline{Y}} \\] where \\[\\begin{align*} \\color{red}{\\mathrm{X}^T\\mathrm{X} = \\begin{pmatrix} 12&amp; 3&amp; 3&amp; 3 \\\\ 3&amp; 3&amp; 0&amp; 0 \\\\ 3&amp; 0&amp; 3&amp; 0 \\\\ 3&amp; 0&amp; 0&amp; 3 \\\\ \\end{pmatrix}, \\mathrm{X}^T\\underline{y} = \\begin{bmatrix} \\sum_{i,j} y_{ij}\\\\ \\sum_{j=1}^3 y_{2j} \\\\ \\sum_{j=1}^3 y_{3j} \\\\ \\sum_{j=1}^3 y_{4j} \\\\ \\end{bmatrix} = \\begin{bmatrix} 458.35 \\\\ 111.90 \\\\ 124.81 \\\\ 113.63 \\\\ \\end{bmatrix}} \\end{align*}\\] and \\[\\begin{align*} (\\mathrm{X}^T\\mathrm{X})^{-1} = \\begin{pmatrix} 1/3&amp; -1/3&amp; -1/3&amp; -1/3 \\\\ -1/3&amp; 2/3&amp; 1/3&amp; 1/3 \\\\ -1/3&amp; 1/3&amp; 2/3&amp; 1/3 \\\\ -1/3&amp; 1/3&amp; 1/3&amp; 2/3 \\\\ \\end{pmatrix} \\end{align*}\\] This leads to \\(\\underline{\\hat{\\beta}} = (\\bar{y}_1, \\bar{y}_2 - \\bar{y}_1, \\bar{y}_3 - \\bar{y}_1, \\bar{y}_4 - \\bar{y}_1)^T\\) where \\(\\bar{y_i}\\) is the mean of the \\(i^{th}\\) group. Note that since, as in chapter one, \\(\\mathrm{Var}\\left({\\underline{\\hat{\\beta}}}\\right) = (\\mathrm{X}^T\\mathrm{X})^{-1}\\sigma_{\\epsilon}^2\\), all the parameter estimates are correlated. We can then use anova to test \\(H_0: \\alpha_2 = \\alpha_3 = \\alpha_4 = 0\\) versus \\(H_1\\): at least one of the \\(\\alpha_i\\) is non-zero The anova table used to test these hypotheses is of the general form: Source Degrees of freedom (df) Sum of squares (SS) Mean square (MS) MSR Treatment k-1 Trt SS \\(\\text{Trt MS}\\, = \\frac{\\text{Trt SS}}{k-1}\\) \\(F = \\frac{\\text{Trt MS}}{\\text{RMS}}\\) Residual n-k RSS \\(\\text{RMS} = \\frac{\\text{RSS}}{n - k}\\) Total n - 1 TSS where \\(n\\) is the sample size and \\(k\\) is the number of ‘treatments’. Similarly to the last chapter, we thus have a breakdown of the total variability as \\[ \\color{red}{\\text{Total SS (TSS)} = \\text{Treatment SS} + \\text{Residual SS}}. \\] We then compare the \\(F\\)-statistic with \\(F\\)-tables on \\(k-1\\) (from the ‘treatment’ row) and \\(n - k\\) (from the residual row) degrees of freedom, rejecting the null hypothesis if the \\(F\\)-statistic is large relative to the tables, and calculating a \\(p\\)-value in the usual way. 4.1.1 Analysis of completely randomised design data in R We enter the data as yield &lt;- c(33.63, 37.80, 36.58, 35.83, 38.18, 37.89, 42.92, 40.43, 41.46, 38.02, 39.62, 35.99) treat &lt;- c(&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;D&quot;, &quot;D&quot;, &quot;D&quot;) treatf &lt;- as.factor(treat) # Or alternatively (more directly) treatf &lt;- gl(4, 3, 12, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)) The analysis then proceeds as fitaov &lt;- aov(yield ~ treatf) summary(fitaov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatf 3 51.97 17.322 6.235 0.0173 * ## Residuals 8 22.23 2.778 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Comment: As the \\(F\\)-value is large and the \\(p\\)-value small (\\(&lt;0.05\\)), we reject the null hypothesis and say there is a difference in mean yields amongst the treatment groups. Where might these differences lie? 4.1.2 Interpretation of results: multiple comparisons If the overall \\(F\\)-value is significant (\\(p &lt; 0.05\\)), we want to investigate further to see which treatment groups differ significantly, i.e. where do the differences lie? We can do this using Tukey’s honest significant differences which takes account of the multiple testing problem (by adjusting the \\(p\\)-values). Tukey’s test compares the mean of each group to the mean of every other group, i.e. it considers every possible pair \\(\\mu_A - \\mu_B\\) for \\(A, B = 1, \\ldots, k, A \\neq B\\). The test statistic is formulated as \\[ q = \\frac{\\mu_A - \\mu_B}{s\\sqrt{0.5 \\left(\\frac{1}{n_A} + \\frac{1}{n_B}\\right)}} \\] where \\(s\\) is the square root of the residual mean square and \\(n_A\\) and \\(n_B\\) are the sample sizes for treatment \\(A\\) and \\(B\\) respectively. These \\(q\\) values (one for each pairwise comparison) are then compared to a critical value, \\(q_{t, \\nu}(\\alpha)\\), from the studentised range distribution (available in R via qtukey()), where \\(t\\) is the number of treatments, \\(\\nu\\) is the residual degrees of freedom and \\(\\alpha\\) is the overall significance level for all of the comparisons, not each individual comparison. With \\(k\\) groups we have, in principle, \\(k(k-1)/2\\) possible \\(t\\)-tests that can be done. As \\(k\\) increases we would be doing many tests and some will be significant by chance. The Tukey’s comparisons show us which group means differ significantly, whilst allowing for this multiple testing. Example: Multiple comparisons We can calculate Tukey’s honest significant differences (HSD) in R as follows: TukeyHSD(fitaov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = yield ~ treatf) ## ## $treatf ## diff lwr upr p adj ## B-A 1.2966667 -3.06163127 5.6549646 0.7786392 ## C-A 5.6000000 1.24170206 9.9582979 0.0142896 ## D-A 1.8733333 -2.48496461 6.2316313 0.5456469 ## C-B 4.3033333 -0.05496461 8.6616313 0.0529252 ## D-B 0.5766667 -3.78163127 4.9349646 0.9727893 ## D-C -3.7266667 -8.08496461 0.6316313 0.0962564 Comments: This tells us that \\(A\\) and \\(C\\) differ significantly in mean and that \\(B\\) and \\(C\\) are approaching significance, and \\(D\\) and \\(C\\) are weakly significant. There is no evidence of any difference in mean between \\(A, B\\) and \\(D\\). \\(C\\) however seems to be different. This is what was suggested by inspection of the treatment means earlier. 4.1.3 Model checking It is much less common to have highly influential points in an anova context as several observations typically come from each possible treatment/factor level, which provides a measure of robustness. However, it is useful to conduct a conventional residual check by plotting the standardised residuals against the fitted values to check for constant variance and outliers, and to produce a quantile-quantile plot to check for normality, alongside a formal test (via Anderson-Darling). This is done as before, i.e. plot(fitted.values(fitaov), rstandard(fitaov), xlab=&quot;Fitted values&quot;, ylab=&quot;Standardised residuals&quot;, pch = 16) abline(h = c(-2, 0, 2), lty = 2) qqnorm(rstandard(fitaov), ylab=&quot;Standardised residuals&quot;, pch = 16) abline(0, 1, lty = 2) Figure 4.1: Residual plots for the crop yield data. library(nortest) ad.test(rstandard(fitaov)) ## ## Anderson-Darling normality test ## ## data: rstandard(fitaov) ## A = 0.32721, p-value = 0.466 From the Anderson-Darling test we obtain a \\(p\\)-value of 0.466, suggesting that normality of the residuals can be safely assumed. Comments - We see an approximately random scatter in the first plot, implying constant variance. All the standardised residuals lie in \\([-2,2]\\) and so no outliers. - The second plot fits quite well to a straight line implying approximate normality of the residuals and the large \\(p\\)-value implies that there is no significant departure from normality. 4.1.4 Completely randomised design: dealing with quantitative variables Sometimes the treatments are quantitative levels of an additive, or particular levels of an underlying continuous variable such as temperature, or pressure. For example: 0, 5, 10, 20, 30 grams of supplement, such as creatine, in the diet with three individuals allocated to each treatment. This should be analysed as a regression problem with multiple observations at each \\(x\\)-value. When handling quantitative variables we must also consider the functional form of the covariate, i.e. is the effect linear, or quadratic? Example: Randomised design with a quantitative variable The data in this example are the amount of grain obtained at various plant densities (per square metre), available in the file grain.RData. An exploratory plot allows us to see the structure of the data, and look for patterns which may suggest candidate models. Figure 4.2: Exploratory plot for the grain and plant density data. We observe clear non-linearity in the relationship between grain and plant density. This suggests that a polynomial (i.e. a quadratic in this case) model may be appropriate, or we can treat \\(x\\) as a factor. In cases such as this, we have to choose whether to treat \\(x\\) as a continuous covariate, or to convert it to a factor - there are pros and cons to either approach. A continuous variable + may offer a simpler interpretation (and allow predictions for intermediate values) - may overlook more complex relationships between the response and the covariate unless we resort to high-order polynomials. On the other hand, a factor will use \\(k-1\\) degrees of freedom (for a variable with \\(k\\) levels) but will estimate each level separately. In this case, we see that yield increases with plant density up to a maximum and then declines again. This is possibly indicative of a quadratic model, which we will now fit to the data, along with the factor model. Quadratic model We can fit a quadratic model in various ways - see subsection 3.7. Here we will use the raw polynomial version: fit_poly &lt;- lm(y ~ poly(x, 2, raw = TRUE), data = grain) anova(fit_poly) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## poly(x, 2, raw = TRUE) 2 85.20 42.600 51.741 1.259e-06 *** ## Residuals 12 9.88 0.823 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Comment: We see from above that the polynomial model is highly significant, with a very small \\(p\\)-value. This does not, however, tell us about the relative contributions of the linear and quadratic terms (the above is another example of an omnibus test). Factor model Alternatively, we can recast \\(x\\) as a factor. This, in fact, leads to a one-way analysis of variance model. As such, we can analyse it using the aov() command (or lm() as usual). (grain$x.factor &lt;- factor(grain$x/10)) ## [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 ## Levels: 1 2 3 4 5 fit_factor &lt;- aov(y ~ x.factor, data = grain) anova(fit_factor) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x.factor 4 87.60 21.900 29.278 1.69e-05 *** ## Residuals 10 7.48 0.748 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Comment: The \\(F\\)-test shows that there is a significant difference between the 5 treatment levels, which was obvious from our plot. Choosing between non-nested models We can assess each fit by eye by plotting the fitted values (at each distinct \\(x\\) value) and overlaying the lines of best fit. Note that the quadratic model produces a continuous curve for the mean response, whereas the factor model is piecewise linear. The fits are very close though and there is no obvious preference based on the figure alone. Figure 4.3: Fitted models for the grain and plant density data: blue (polynomial), red(factor). Our two candidate models are non-nested, i.e. one is not a subset of another. The respective \\(R^2\\) values for the quadratic and factor models are \\(89.6\\%\\) and \\(92.1\\%\\). We may be guilty of overfitting in the factor model - ideally we would like to validate this result in another dataset. Often we choose a simpler model, even if it has lower values of \\(R^2\\) and higher RMS (or MSE). Note in passing that the adjusted \\(R^2\\) values are \\(87.9\\%\\) and \\(89.0\\%\\). Choosing a model is subjective and you must use your judgement. Here, we may opt for the simpler quadratic model as there is little difference between the models and this is better for predictive purposes. We also check the residuals - if these flag up any concerns we may revert to an alternative model: Figure 4.4: Residual plots for the polynomial model fitted to the grain data. In this case the residuals checks are fine and the model also has standardised residuals that yield a p-value \\(&gt;0.1\\) for the Anderson-Darling test: ad.test(rstandard(fit_poly)) ## ## Anderson-Darling normality test ## ## data: rstandard(fit_poly) ## A = 0.21793, p-value = 0.8041 4.2 Randomised block design A randomised block design is the most commonly used design in practice. Background: often the experimental units are grouped such that those in a group (year, farm, day, laboratory) are expected to respond similarly to a treatment, but different groups are expected to respond differently. The groups are called blocks. Essentially the blocking factor is a `nuisance factor’; we know it probably affects the response but we are not that interested in it per se. However we need to correct for it to make our treatment comparisons more exact, i.e. by reducing the residual mean square (i.e. the variance). Randomised block designs are analysed with a two-way analysis of variance. 4.2.1 Two-way analysis of variance In a one-way anova we look for the effect of one factor, in a two-way anova we look for the effects of two factors. The second factor may also be of genuine interest, or it may be a blocking or nuisance factor (c.f. the paired sample t-test). A further complication arises because of the possibility of interactions between the factors, namely do the treatments work differently in the different groups? We will consider interactions later. The different values of a factor are called ‘levels’. A factorial experiment is one where all of the explanatory variables are qualitative factors rather than quantitative variables. We will later consider cases where we have both factors and quantitative explanatory variables. Typically, we are interested in a randomised block design when we have models of the form: \\[ \\color{red}{Y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}} \\] where \\(i = 1, 2, \\ldots, k_1\\) refer to the levels of factor 1 and \\(j = 1, 2, \\ldots, k_2\\) refer to the levels of factor 2. We make the usual assumption of normally distributed errors with constant variance. As with single factor experiments, we must make assumptions in order to estimate these parameters, since this model is again over-parameterised otherwise. We now (further) assume that \\(\\alpha_1 = \\beta_1 = 0\\). The additional source of variation leads to an extra row in the anova table. We now have a further breakdown of the total variability as \\[ \\color{red}{\\text{Total SS (TSS)} = \\text{Treatment SS} + \\text{Block SS} + \\text{Residual SS}}. \\] Without the blocking factor, this source of variation is absorbed into the residual sum of squares, which affects the significance of the treatments. 4.2.2 Orthogonality and testing of blocks in a two-way analysis of variance model If each treatment occurs an equal number of times in each block, then the estimates of the treatment effects are uncorrelated with those of the blocks. The two factors are then said to be orthogonal; if there is not equal replication then the sums of squares depends on the order of fitting and, to avoid bias, we fit the blocking factor first. Generally, if the \\(F\\)-statistic for blocks exceeds unity (i.e. \\(&gt; 1\\)) then it is advantageous to have used blocks. The treatment comparison will then be more accurate. We will now consider an example. Example: Two-way anova on nitrate data Six nitrogen timing treatments were used in six fields in each of four farms (the blocking factor), giving \\(n = 24\\), and the resulting nitrate in wheat stems was assayed. The data were analysed to produce the anova table below: Source Degrees of freedom Sum of squares Mean square Mean square ratio Farms 3 197.00 65.67 9.12 Timings 5 201.32 40.26 5.59 Residual 15 108.01 7.20 Total 23 506.33 There is a large difference between farms (\\(F &gt;&gt; 1\\)). Thus blocking was worthwhile. There are significant differences between timings - these could be investigated using Tukey’s HSD as before. The adequacy of the model is checked by residual plots as usual. Example: Chicken egg production An experiment was conducted at each of four units to see if extra lighting increased egg production by chickens during the winter months. The treatments were natural daylight; extended day length to 14 hours using artificial lighting; two 20 second light periods during the night. The number of eggs laid by six chickens in a three-month period was recorded: Treatment Unit 1 Unit 2 Unit 1 Unit 4 1 330 288 295 313 2 372 340 343 341 3 359 337 373 302 The data are available on Canvas in the data file ChickenEggs.RData. We can carry out a two-way anova in R as follows: load(&quot;ChickenEggs.RData&quot;) fitaov &lt;- aov(eggs ~ unit + light, data = ChickenEggs) summary(fitaov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## unit 3 2330 776.8 2.008 0.2145 ## light 2 4212 2106.2 5.444 0.0449 * ## Residuals 6 2322 386.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Comments \\(F\\)-ratio for units (blocks) is greater than 1 so the treatment comparison was made more exact by blocking, but the differences between units were not large. The differences between treatment were significant (\\(p=0.045\\)). We can now use Tukey’s HSD (via R) to see where the differences are: TukeyHSD(fitaov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = eggs ~ unit + light, data = ChickenEggs) ## ## $unit ## diff lwr upr p adj ## 2-1 -32.00000 -87.59733 23.59733 0.2874709 ## 3-1 -16.66667 -72.26400 38.93066 0.7356765 ## 4-1 -35.00000 -90.59733 20.59733 0.2308903 ## 3-2 15.33333 -40.26400 70.93066 0.7785977 ## 4-2 -3.00000 -58.59733 52.59733 0.9974306 ## 4-3 -18.33333 -73.93066 37.26400 0.6802599 ## ## $light ## diff lwr upr p adj ## 2-1 42.50 -0.1764142 85.17641 0.0507874 ## 3-1 36.25 -6.4264142 78.92641 0.0891770 ## 3-2 -6.25 -48.9264142 36.42641 0.8965079 We see that there is a borderline significant difference in means between treatments 1 and 2. The difference between 1 and 3 is approaching significance and there is no evidence of a difference between treatments 2 and 3. Thus, extra lighting seems to increase egg production, but there is little difference between the two extra light treatments. As expected from the \\(p\\)-value there is no significant differences between the units themselves. We can check model adequacy in the usual way: ## ## Anderson-Darling normality test ## ## data: rstandard(fitaov) ## A = 0.256, p-value = 0.6577 Figure 4.5: Residual plots for the two-way anova model on the chicken egg data. From the left-hand plot we see there is random scatter with no evidence of curvature or changing variance, and no outliers. There is a good fit to the straight line on the right-hand plot, and a large \\(p\\)-value from the Anderson-Darling test, implying the normality assumption is fine. 4.3 Factorial experiments Sometimes, when there are two factors in an experiment, both are of genuine, and equal, interest and we are interested as to whether they are each important. In addition, we may wish to consider whether they interact, i.e. whether the level of one factor affects the effect of the other factor. We are thus interested in fitting a model of the form: \\[ \\color{red}{Y_{ij} = \\mu + \\alpha_i + \\beta_j + \\gamma_{ij} + \\epsilon_{ij}} \\] where we make similar assumptions to the randomised block structure, but we additionally have the term, \\(\\gamma_{ij}\\), which denotes the interaction between the two factors. Now, any term with \\(i\\) or \\(j\\) equal to 1 is set to zero to avoid over-parameterisation. Interaction terms can only be fitted if there is replication for at least some of the combinations of the two factors. Thus we could not have fitted an interaction term in the previous examples. Example: Two-way anova with interactions for the yeast data As part of an investigation of toxic agents, 48 yeast strains were allocated to three poisons (I, II, III) and four treatments (\\(A, B, C, D\\)). Each poison-treatment combination occurred four times and the response was survival time (measured in tens of hours). A snapshot of the data are given below: Time Poison Treatment 0.31 I A 0.82 I B 0.43 I C 0.45 I D 0.45 I A ⋮ ⋮ ⋮ 0.29 II A 0.61 II B ⋮ ⋮ ⋮ 0.33 III D The data are available in the file yeast.RData. The syntax : is used in R to represent an interaction. Thus, we can fit the interaction model as follows load(&quot;yeast.RData&quot;) aov1 &lt;- aov(time ~ poison + treatment + poison:treatment, data = yeast) summary(aov1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## poison 2 1.0330 0.5165 23.222 3.33e-07 *** ## treatment 3 0.9212 0.3071 13.806 3.78e-06 *** ## poison:treatment 6 0.2501 0.0417 1.874 0.112 ## Residuals 36 0.8007 0.0222 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From an Anderson-Darling test of the standardised residuals we obtain a test statistic of \\(1.59\\) and \\(p &lt; 0.001\\) (we will investigate this shortly …): ad.test(rstandard(aov1)) ## ## Anderson-Darling normality test ## ## data: rstandard(aov1) ## A = 1.5923, p-value = 0.0003712 Comments The interaction term is not significant, implying that the effects of the poisons are approximately the same for each treatment and vice versa. However, poison and treatment are both highly significant. 4.3.1 Exploratory plots for interactions Potential presence or absence of an interaction can be explored using interaction plots. In such plots, non-parallel (or crossing) lines are indicative of the presence of an interaction since this suggests that the response differs for certain combinations of the factors. We can produce an interaction plot in R via the below commands: Figure 4.6: Interaction plots for the yeast data. We can see that the plots are approximately parallel, implying no significant interaction, although we see that treatment \\(D\\) responds somewhat differently to the others for poisons \\(I\\) and \\(II\\). For each treatment (left-hand plot), poisons are generally \\(I &gt; II&gt; III\\) whereas for each poison (right-hand plot), treatments are \\(B &gt; D &gt; C &gt; A\\). Model checking As usual, we can look at a plot of standardised residuals versus the fitted values as well as a plot of the standardised residuals and a formal test of normality: ## ## Anderson-Darling normality test ## ## data: rstandard(aov1) ## A = 1.5923, p-value = 0.0003712 Figure 4.7: Residual plots for the yeast data. We see from the first plot that there is clearly an increase in variance with two large positive residuals. There is also a poor fit to normality in the quantile-quantile plot with the large positive standardised residuals distorting the plot. This leads to the very small \\(p\\)-value from the Anderson-Darling statistic. In cases of increasing variance as the mean increases (a funnel shape), a log transformation usually works well. Thus, we fit a new model with \\(\\log(\\text{time})\\) as the response variable. Example: Transforming the response We can fit a transformed model directly in R via the following commands: aov2 &lt;- aov(log(time) ~ treatment + poison + treatment*poison, data = yeast) anova(aov2) ## Analysis of Variance Table ## ## Response: log(time) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 3 3.5572 1.18572 21.9295 2.987e-08 *** ## poison 2 5.2375 2.61874 48.4324 6.195e-11 *** ## treatment:poison 6 0.3957 0.06596 1.2199 0.3189 ## Residuals 36 1.9465 0.05407 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see that the \\(p\\)-values for the factors have decreased and are even more significant but the interaction is still not significant. The \\(R^2\\) (not shown) due to the two factors has increased from \\(65\\%\\) to \\(79\\%\\), showing a better fit to the data. Model checking for the log-response model We can again perform our residual checks: ## ## Anderson-Darling normality test ## ## data: rstandard(aov2) ## A = 0.35688, p-value = 0.442 Figure 4.8: Residual plots for the transformed yeast data. The Anderson-Darling test now produces a \\(p\\)-value of \\(0.442\\). We see now that there is still evidence of an increasing variance for the standardised residuals, but this has been mitigated through the log-transform. There are still 4 points outside (-2, 2) which is more than we would expect by chance (\\(5\\%\\; \\text{of}\\; 48 = 2.4\\)), but they are all only just outside the range. The points fit well to a straight line in the quantile-quantile plot, and the \\(p\\)-value for the AD test is large, implying that the normality assumption is now fine. Multiple comparisions for the log-response model In order to see how the different levels of the factor affect the response, we need to fit the factors without an interaction and then look at which factor levels differ in mean value: aov3 &lt;- aov(log(time) ~ poison + treatment, data = yeast) TukeyHSD(aov3) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = log(time) ~ poison + treatment, data = yeast) ## ## $poison ## diff lwr upr p adj ## II-I -0.1866630 -0.3895078 0.01618181 0.0767735 ## III-I -0.7751531 -0.9779980 -0.57230832 0.0000000 ## III-II -0.5884901 -0.7913350 -0.38564531 0.0000000 ## ## $treatment ## diff lwr upr p adj ## B-A 0.7046545 0.44676456 0.96254438 0.0000000 ## C-A 0.1967076 -0.06118234 0.45459748 0.1897915 ## D-A 0.5070686 0.24917864 0.76495846 0.0000264 ## C-B -0.5079469 -0.76583681 -0.25005699 0.0000257 ## D-B -0.1975859 -0.45547583 0.06030399 0.1866278 ## D-C 0.3103610 0.05247107 0.56825089 0.0127525 Comment For the poisons, \\(III\\) differs highly significantly from both \\(I\\) and \\(II\\) but \\(I\\) and \\(II\\) do not differ significantly from each other (but the \\(p\\)-value is approaching significance. In this model we are also interested in the treatment differences themselves. For the treatments: we see that \\(A\\) differs significantly from \\(B\\) and \\(D\\), and that \\(C\\) differs significantly from \\(B\\) and \\(D\\). Neither of the other two differences are significant giving \\(B,D &gt; C,A\\) as a general interpretation. "],["general-linear-models.html", "5 General linear models 5.1 Indicator and dummy variables Example: Gasoline data", " 5 General linear models In the models considered thus far, we have either had exclusively continuous covariates (multiple linear regression models) or solely factors (anova models). In reality, we are often faced with variables of both types within the same dataset. Both of these models actually fall under the umbrella term of a general linear model, as do many more. We will now consider a model of this type. 5.1 Indicator and dummy variables Let us consider an example where there appeared to be a linear relationship between weight and length in both male and female lobsters. However we may want to consider: Is there a difference in the intercepts between males and females? Is there a difference in the slopes? We can thus define an indicator variable which indicates the sex of the lobster. Let \\(x_1 = 0\\) for a male and \\(x_1 = 1\\) for a female, i.e. \\[ \\color{red}{x_{1} = \\begin{cases} 1&amp; \\text{if ``Female&#39;&#39;} \\\\ 0&amp; \\text{if ``Male&#39;&#39;} \\end{cases}} \\] This can also be written as \\(I(x_1 = \\text{``Female&#39;&#39;})\\), where \\(I()\\) is an indicator variable, taking the value 1 if the condition i parentheses is true, and zero otherwise. An indicator always takes the values 0 or 1, to indicate the absence or presence of a particular characteristic (here the characteristic is `female’). If all the regressor variables are indicator variables, then we are actually dealing with anova models (as we did in Chapter 4); if some of the regressor variables are indicator variables, then we have analysis of covariance (ancova) models. Our first example of using indicator variables will be to fit two simple linear regression equations simultaneously. Consider the model: \\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\] where \\(x_1\\) is an indicator variable and \\(x_2\\) is a quantitative variable. Then \\(\\mathrm{E}(Y \\mid x_1 = 0) = \\beta_0 + \\beta_2 x_2\\), and \\(\\mathrm{E}(Y \\mid x_1 = 1) = (\\beta_0 + \\beta_1) + \\beta_2 x_2\\) So what we are really doing is fitting two regression lines with a common slope, i.e. two parallel lines. The lines are separated in the vertical plane by the value of \\(\\beta_1\\). Hence, if this value is close to zero then one line will suffice. Formally, we can test for a common intercept by testing \\[ \\color{red}{H_0: \\beta_1 = 0 \\;\\;\\text{versus}\\;\\; H_1: \\beta_1 \\neq 0} \\] in the usual way (i.e. we are testing for a common line, which corresponds to no difference between the two categories). To fit two lines with different slopes, we fit: \\[ \\color{red}{Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon} \\] Under this formulation \\(\\mathrm{E}(Y \\mid x_1 = 0) = \\beta_0 + \\beta_2 x_2\\) \\(\\mathrm{E}(Y \\mid x_1 = 1) = (\\beta_0 + \\beta_1) + (\\beta_2 + \\beta_3) x_2\\) This is an interaction model! We can test for a common slope via \\[ \\color{red}{H_0: \\beta_3 = 0 \\;\\;\\text{versus}\\;\\; H_1: \\beta_3 \\neq 0.} \\] Example: Gasoline data The data in the file gasoline.RData gives the gasoline mileage (\\(y\\)), the engine displacement (\\(x_1\\)) and the type of transmission (\\(x_2\\)) for a sample of cars, where \\(x_2\\) is coded as 0 for automatic transmission and 1 for manual transmission. We begin by plotting the data. We can use the command xyplot() to produce an appropriate plot (although plot() can also be used with different symbols for each factor level). Figure 5.1: Plot of mileage against engine displacement by transmission type. There is a clear difference in the intercepts with most automatic transmission points above manual transmission points. Mileage declines with engine displacement, and it looks like the rate of decline may be different for the two transmission types. In particular, there looks to be a steeper decline for cars with automatic transmission. Let us see what happens if we fit a common line, i.e. ignore transmission type. m1 = lm(y ~ x1, data = gasoline) summary(m1) ## ## Call: ## lm(formula = y ~ x1, data = gasoline) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9498 -1.8377 -0.0842 1.8158 6.6023 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.026933 1.674994 20.315 2.40e-14 *** ## x1 -0.048408 0.006168 -7.848 2.22e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.324 on 19 degrees of freedom ## Multiple R-squared: 0.7643, Adjusted R-squared: 0.7519 ## F-statistic: 61.6 on 1 and 19 DF, p-value: 2.224e-07 Comments: - We can see that engine displacement gives a very small \\(p\\)-value , suggesting that this is very important. - However, we have ignored the crucial information on transmission type, and this can give misleading conclusions. We can add the fitted line to the raw data using abline(m1): Figure 5.2: Plot of mileage against engine displacement by transmission type with line of best fit overlaid. We can see that the line captures that there is a decline with engine displacement. However, it does not fit the data well, particularly for automatic transmission cars, because we have ignored transmission type. We will now add in the transmission type, allowing a test of \\[ \\color{red}{H_0: \\beta_2 = 0 \\;\\;\\text{versus}\\;\\; H_1: \\beta_2 \\neq 0} \\] We use the R commands m2 = lm(y ~ x1 + x2, data = gasoline) summary(m2) ## ## Call: ## lm(formula = y ~ x1 + x2, data = gasoline) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.880 -1.970 -0.104 1.796 6.605 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.12798 1.89989 17.963 6.1e-13 *** ## x1 -0.04963 0.01162 -4.271 0.00046 *** ## x21 0.34592 2.76144 0.125 0.90170 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.414 on 18 degrees of freedom ## Multiple R-squared: 0.7645, Adjusted R-squared: 0.7383 ## F-statistic: 29.21 on 2 and 18 DF, p-value: 2.231e-06 We observe that the addition of the transmission type indicator variable is not significant, and \\(R^2\\) has not changed much either. Perhaps a common slope is a plausible claim? Figure 5.3: Plot of mileage against engine displacement by transmission type with lines of best fit for each transmission type overlaid (automatic - black, manual - red). Are we overlooking something? What about the gradients? We now fit a model with different intercepts and different slopes \\[ \\color{red}{H_0: \\beta_3 = 0 \\;\\;\\text{versus}\\;\\; H_1: \\beta_3 \\neq 0} \\] m3 = lm(y ~ x1 + x2 + x1:x2, data = gasoline) summary(m3) ## ## Call: ## lm(formula = y ~ x1 + x2 + x1:x2, data = gasoline) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.2712 -1.2042 0.2958 1.4758 3.5412 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.91963 2.78705 15.400 2.04e-11 *** ## x1 -0.11677 0.02022 -5.776 2.24e-05 *** ## x21 -13.77463 4.36449 -3.156 0.00577 ** ## x1:x21 0.08329 0.02252 3.699 0.00178 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.615 on 17 degrees of freedom ## Multiple R-squared: 0.8695, Adjusted R-squared: 0.8465 ## F-statistic: 37.75 on 3 and 17 DF, p-value: 9.809e-08 We see that the \\(p\\)-value for the interaction is significant beyond the \\(1\\%\\) level, suggesting the slopes are different and \\(\\beta_3 \\neq 0\\). Furthermore, we also see that the main effect of the transmission type is also now significant (also at the \\(1\\%\\) level). The conclusion for engine displacement is largely as before, with a significant (and negative) coefficient although the magnitude has now changed. \\(R^2\\) has increased to around \\(87\\%\\), indicating that this model captures more of the uncertainty in mileage. These results highlight the need to always include all lower order terms up-to-and-including the highest order term. Adding the respective lines of best fit to the raw data now gives: Figure 5.4: Plot of mileage against engine displacement by transmission type with lines of best fit for each transmission type overlaid (automatic - black, manual - red) from interaction model. This is clearly a much better fit. Note the crossing lines, which are indicative of an interaction. Now that we are happier with our model we should carry out the usual residual checks (not included here). Model interpretation The final model is: \\[\\begin{align*} \\color{red}{\\text{Mileage} = 42.92} &amp;\\color{red}{- 0.12\\times\\text{Engine displacement}} \\\\ &amp;\\color{red}{- 13.77 \\times I(\\text{Transmission type} = 1)} \\\\ &amp;\\color{red}{+ 0.08\\times \\text{Engine displacement} \\times I(\\text{Transmission type} = 1)} \\end{align*}\\] This can be expressed as two separate models: \\[\\begin{align*} \\color{red}{\\text{Type 0: Mileage}} &amp;\\color{red}{= 42.92 - 0.12\\times \\text{Engine displacement}} \\\\ \\color{red}{\\text{Type 1: Mileage}} &amp;\\color{red}{= 29.15 - 0.04\\times \\text{Engine displacement}} \\end{align*}\\] Overall it can be seen that the mileage decreases by 0.12 for every unit increase in engine displacement when the transmission is automatic, and by 0.04 units when the transmission is manual. The decrease is greater when the transmission is automatic, as we would expect based on our initial plot. The effect of engine displacement differs according to the transmission type and two non-parallel lines must be used to model the data. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
